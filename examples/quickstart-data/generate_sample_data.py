#!/usr/bin/env python3
"""
Quickstart Data Generator for NeuronDB
=======================================
Generates a small sample dataset (100-500 records) for quick testing.
Includes sample documents with pre-computed embeddings.

Usage:
    python generate_sample_data.py [--output-dir DIR] [--count COUNT]

Copyright (c) 2024-2026, neurondb, Inc.
"""

import sys
import os
import argparse
import json
from pathlib import Path

try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
except ImportError:
    print("Error: sentence-transformers and numpy are required")
    print("Install with: pip install sentence-transformers numpy")
    sys.exit(1)


# Sample documents for quickstart
SAMPLE_DOCUMENTS = [
    {
        "title": "Introduction to Vector Databases",
        "content": "Vector databases are specialized databases designed to store and query high-dimensional vectors efficiently. They enable semantic search by comparing vector embeddings that capture the meaning of text, images, or other data.",
        "category": "database",
        "tags": ["vectors", "databases", "search"]
    },
    {
        "title": "PostgreSQL Performance Optimization",
        "content": "PostgreSQL performance can be significantly improved through proper indexing strategies. B-tree indexes work well for most queries, while GiST indexes are useful for full-text search and geometric data.",
        "category": "database",
        "tags": ["postgresql", "performance", "indexing"]
    },
    {
        "title": "Machine Learning Embeddings",
        "content": "Embeddings are dense vector representations that capture semantic meaning. They are generated by neural networks and enable similarity search, clustering, and recommendation systems.",
        "category": "machine_learning",
        "tags": ["embeddings", "neural_networks", "similarity"]
    },
    {
        "title": "Retrieval-Augmented Generation (RAG)",
        "content": "RAG combines large language models with external knowledge retrieval. It involves converting queries to embeddings, retrieving relevant documents using vector similarity, and providing context to the LLM.",
        "category": "ai",
        "tags": ["rag", "llm", "retrieval"]
    },
    {
        "title": "HNSW Index Algorithm",
        "content": "Hierarchical Navigable Small World (HNSW) is a graph-based approximate nearest neighbor search algorithm. It provides fast query times with good recall, making it ideal for vector databases.",
        "category": "algorithms",
        "tags": ["hnsw", "indexing", "algorithms"]
    },
    {
        "title": "Cosine Similarity for Vectors",
        "content": "Cosine similarity measures the cosine of the angle between two vectors. It's widely used in information retrieval and machine learning to compare document embeddings and find similar items.",
        "category": "machine_learning",
        "tags": ["similarity", "vectors", "metrics"]
    },
    {
        "title": "Semantic Search Implementation",
        "content": "Semantic search uses vector embeddings to find documents based on meaning rather than keyword matching. It understands context and synonyms, providing more relevant search results.",
        "category": "search",
        "tags": ["semantic_search", "embeddings", "nlp"]
    },
    {
        "title": "Database Indexing Best Practices",
        "content": "Effective indexing requires understanding query patterns. Create indexes on frequently filtered columns, use composite indexes for multi-column queries, and monitor index usage to avoid unnecessary overhead.",
        "category": "database",
        "tags": ["indexing", "optimization", "best_practices"]
    },
    {
        "title": "Vector Quantization Techniques",
        "content": "Vector quantization reduces storage requirements by compressing high-dimensional vectors. Techniques like product quantization and scalar quantization enable efficient storage of billions of vectors.",
        "category": "algorithms",
        "tags": ["quantization", "compression", "vectors"]
    },
    {
        "title": "Hybrid Search Strategies",
        "content": "Hybrid search combines keyword-based and vector-based search for better results. Reciprocal Rank Fusion (RRF) and weighted scoring are common approaches to merge results from different search methods.",
        "category": "search",
        "tags": ["hybrid_search", "rrf", "fusion"]
    },
    {
        "title": "Neural Network Architectures",
        "content": "Modern neural networks use architectures like transformers, which process sequences efficiently. Attention mechanisms allow models to focus on relevant parts of the input when generating embeddings.",
        "category": "machine_learning",
        "tags": ["neural_networks", "transformers", "attention"]
    },
    {
        "title": "Database Sharding Strategies",
        "content": "Sharding distributes data across multiple database instances to improve scalability. Strategies include range-based, hash-based, and directory-based sharding, each with different trade-offs.",
        "category": "database",
        "tags": ["sharding", "scalability", "distributed"]
    },
    {
        "title": "Full-Text Search in PostgreSQL",
        "content": "PostgreSQL provides robust full-text search capabilities using tsvector and tsquery. GIN indexes accelerate text search queries, making it fast to search large document collections.",
        "category": "database",
        "tags": ["postgresql", "fulltext_search", "gin"]
    },
    {
        "title": "Approximate Nearest Neighbor Search",
        "content": "ANN search algorithms trade exact results for speed, enabling fast similarity search on large vector collections. HNSW, IVF, and LSH are popular ANN algorithms with different performance characteristics.",
        "category": "algorithms",
        "tags": ["ann", "similarity_search", "algorithms"]
    },
    {
        "title": "Document Chunking Strategies",
        "content": "Effective document chunking is crucial for RAG systems. Strategies include fixed-size chunks, sentence-based chunking, and semantic chunking, each balancing context preservation with retrieval precision.",
        "category": "ai",
        "tags": ["chunking", "rag", "preprocessing"]
    }
]


def generate_sample_data(output_dir: Path, count: int = 200, model_name: str = "all-MiniLM-L6-v2"):
    """
    Generate sample data with embeddings.
    
    Args:
        output_dir: Directory to write output files
        count: Number of sample records to generate (by duplicating and varying documents)
        model_name: Embedding model to use
    """
    print(f"Loading embedding model: {model_name}")
    try:
        model = SentenceTransformer(model_name)
    except Exception as e:
        print(f"Error loading model: {e}")
        print("This may take a few minutes on first run to download the model...")
        sys.exit(1)
    
    # Ensure output directory exists
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate documents by duplicating and slightly varying base documents
    documents = []
    for i in range(count):
        base_doc = SAMPLE_DOCUMENTS[i % len(SAMPLE_DOCUMENTS)]
        
        # Create slight variations for duplicate documents
        if i >= len(SAMPLE_DOCUMENTS):
            doc = {
                "title": f"{base_doc['title']} (Variant {i // len(SAMPLE_DOCUMENTS) + 1})",
                "content": base_doc['content'],
                "category": base_doc['category'],
                "tags": base_doc['tags'].copy()
            }
        else:
            doc = base_doc.copy()
        
        documents.append(doc)
    
    print(f"Generating embeddings for {len(documents)} documents...")
    texts = [f"{doc['title']}. {doc['content']}" for doc in documents]
    
    # Generate embeddings in batches
    batch_size = 32
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_embeddings = model.encode(batch, show_progress_bar=False, normalize_embeddings=True)
        embeddings.extend(batch_embeddings.tolist())
        
        if (i // batch_size + 1) % 10 == 0:
            print(f"  Processed {min(i + batch_size, len(texts))}/{len(texts)} documents...")
    
    print(f"✓ Generated {len(embeddings)} embeddings")
    
    # Write SQL file
    sql_file = output_dir / "sample_data.sql"
    print(f"Writing SQL file: {sql_file}")
    
    with open(sql_file, 'w') as f:
        f.write("-- Quickstart Sample Data for NeuronDB\n")
        f.write("-- Generated sample dataset with pre-computed embeddings\n")
        f.write("-- Embedding model: all-MiniLM-L6-v2 (384 dimensions)\n\n")
        
        f.write("-- Create extension if not exists\n")
        f.write("CREATE EXTENSION IF NOT EXISTS neurondb;\n\n")
        
        f.write("-- Create documents table\n")
        f.write("CREATE TABLE IF NOT EXISTS quickstart_documents (\n")
        f.write("    id SERIAL PRIMARY KEY,\n")
        f.write("    title TEXT NOT NULL,\n")
        f.write("    content TEXT NOT NULL,\n")
        f.write("    category TEXT,\n")
        f.write("    tags TEXT[],\n")
        f.write("    embedding vector(384),\n")
        f.write("    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n")
        f.write(");\n\n")
        
        f.write("-- Insert documents with embeddings\n")
        for i, (doc, embedding) in enumerate(zip(documents, embeddings), 1):
            # Format embedding as PostgreSQL vector literal
            vec_str = '[' + ','.join(f"{v:.6f}" for v in embedding) + ']'
            
            # Escape single quotes in text
            title = doc['title'].replace("'", "''")
            content = doc['content'].replace("'", "''")
            tags_array = '{' + ','.join(f'"{tag}"' for tag in doc['tags']) + '}'
            
            f.write(f"INSERT INTO quickstart_documents (title, content, category, tags, embedding)\n")
            f.write(f"VALUES (\n")
            f.write(f"    '{title}',\n")
            f.write(f"    '{content}',\n")
            f.write(f"    '{doc['category']}',\n")
            f.write(f"    ARRAY[{tags_array.replace('"', '')}],\n")
            f.write(f"    '{vec_str}'::vector\n")
            f.write(f");\n\n")
            
            if i % 50 == 0:
                print(f"  Written {i}/{len(documents)} documents...")
        
        f.write("-- Create HNSW index for fast similarity search\n")
        f.write("CREATE INDEX IF NOT EXISTS quickstart_documents_embedding_idx\n")
        f.write("ON quickstart_documents USING hnsw (embedding vector_cosine_ops)\n")
        f.write("WITH (m = 16, ef_construction = 64);\n\n")
        
        f.write("-- Verification queries\n")
        f.write("SELECT COUNT(*) AS total_documents FROM quickstart_documents;\n")
        f.write("SELECT COUNT(*) AS documents_with_embeddings\n")
        f.write("FROM quickstart_documents WHERE embedding IS NOT NULL;\n")
    
    print(f"✓ SQL file written: {sql_file}")
    
    # Write JSON metadata file
    metadata_file = output_dir / "metadata.json"
    with open(metadata_file, 'w') as f:
        json.dump({
            "model": model_name,
            "embedding_dimension": len(embeddings[0]) if embeddings else 384,
            "document_count": len(documents),
            "generated_at": str(Path.cwd()),
        }, f, indent=2)
    
    print(f"✓ Metadata written: {metadata_file}")
    print(f"\n✓ Sample data generation complete!")
    print(f"  - Documents: {len(documents)}")
    print(f"  - Embeddings: {len(embeddings)}")
    print(f"  - SQL file: {sql_file}")
    print(f"  - Metadata: {metadata_file}")


def main():
    parser = argparse.ArgumentParser(
        description="Generate sample data for NeuronDB quickstart",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python generate_sample_data.py
  python generate_sample_data.py --output-dir ./output --count 300
  python generate_sample_data.py --model all-mpnet-base-v2
        """
    )
    
    parser.add_argument(
        '--output-dir',
        type=Path,
        default=Path(__file__).parent / "sample_data",
        help='Output directory for generated files (default: ./sample_data)'
    )
    
    parser.add_argument(
        '--count',
        type=int,
        default=200,
        help='Number of sample documents to generate (default: 200)'
    )
    
    parser.add_argument(
        '--model',
        type=str,
        default='all-MiniLM-L6-v2',
        help='Embedding model to use (default: all-MiniLM-L6-v2)'
    )
    
    args = parser.parse_args()
    
    generate_sample_data(args.output_dir, args.count, args.model)


if __name__ == '__main__':
    main()

