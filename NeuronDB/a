diff --git a/NeuronAgent/cmd/agent-server/main.go b/NeuronAgent/cmd/agent-server/main.go
index a9bc6ac..1902abb 100644
--- a/NeuronAgent/cmd/agent-server/main.go
+++ b/NeuronAgent/cmd/agent-server/main.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * main.go
+ *    Main entry point for NeuronAgent server
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/cmd/agent-server/main.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package main
 
 import (
@@ -23,7 +36,7 @@ import (
 )
 
 func main() {
-	// Load configuration
+  /* Load configuration */
 	cfg := config.DefaultConfig()
 	if configPath := os.Getenv("CONFIG_PATH"); configPath != "" {
 		var err error
@@ -32,14 +45,14 @@ func main() {
 			fmt.Printf("Failed to load config: %v, using defaults\n", err)
 		}
 	} else {
-		// Load from environment variables if no config file
+   /* Load from environment variables if no config file */
 		config.LoadFromEnv(cfg)
 	}
 
-	// Initialize logging
+  /* Initialize logging */
 	metrics.InitLogging(cfg.Logging.Level, cfg.Logging.Format)
 
-	// Connect to database
+  /* Connect to database */
 	connStr := fmt.Sprintf("host=%s port=%d user=%s password=%s dbname=%s sslmode=disable",
 		cfg.Database.Host, cfg.Database.Port, cfg.Database.User, cfg.Database.Password, cfg.Database.Database)
 
@@ -59,7 +72,7 @@ func main() {
 	}
 	defer database.Close()
 
-	// Run migrations
+  /* Run migrations */
 	migrationRunner, err := db.NewMigrationRunner(database.DB, "./migrations")
 	if err == nil {
 		if err := migrationRunner.Run(context.Background()); err != nil {
@@ -67,33 +80,33 @@ func main() {
 		}
 	}
 
-	// Initialize components
+  /* Initialize components */
 	queries := db.NewQueries(database.DB)
 	queries.SetConnInfoFunc(database.GetConnInfoString)
 	embedClient := neurondb.NewEmbeddingClient(database.DB)
 	toolRegistry := tools.NewRegistry(queries, database)
 	runtime := agent.NewRuntime(database, queries, toolRegistry, embedClient)
 
-	// Initialize session management
+  /* Initialize session management */
 	sessionCache := session.NewCache(5 * time.Minute)
-	_ = session.NewManager(queries, sessionCache) // Session manager for future use
+ 	_ = session.NewManager(queries, sessionCache) /* Session manager for future use */
 	sessionCleanup := session.NewCleanupService(queries, 1*time.Hour, 24*time.Hour)
 	sessionCleanup.Start()
 	defer sessionCleanup.Stop()
 
-	// Initialize API
+  /* Initialize API */
 	handlers := api.NewHandlers(queries, runtime)
 	keyManager := auth.NewAPIKeyManager(queries)
 	rateLimiter := auth.NewRateLimiter()
 
-	// Setup router
+  /* Setup router */
 	router := mux.NewRouter()
 	router.Use(api.RequestIDMiddleware)
 	router.Use(api.CORSMiddleware)
 	router.Use(api.LoggingMiddleware)
 	router.Use(api.AuthMiddleware(keyManager, rateLimiter))
 
-	// API routes
+  /* API routes */
 	apiRouter := router.PathPrefix("/api/v1").Subrouter()
 	apiRouter.HandleFunc("/agents", handlers.CreateAgent).Methods("POST")
 	apiRouter.HandleFunc("/agents", handlers.ListAgents).Methods("GET")
@@ -107,7 +120,7 @@ func main() {
 	apiRouter.HandleFunc("/sessions/{session_id}/messages", handlers.GetMessages).Methods("GET")
 	apiRouter.HandleFunc("/ws", api.HandleWebSocket(runtime)).Methods("GET")
 
-	// Health check
+  /* Health check */
 	router.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
 		if err := database.HealthCheck(r.Context()); err != nil {
 			w.WriteHeader(http.StatusServiceUnavailable)
@@ -116,22 +129,22 @@ func main() {
 		w.WriteHeader(http.StatusOK)
 	}).Methods("GET")
 
-	// Metrics endpoint (no auth required)
+  /* Metrics endpoint (no auth required) */
 	router.Handle("/metrics", metrics.Handler()).Methods("GET")
 
-	// Start background workers
+  /* Start background workers */
 	queue := jobs.NewQueue(queries)
 	processor := jobs.NewProcessor(database)
 	worker := jobs.NewWorker(queue, processor, 5)
 	worker.Start()
 	defer worker.Stop()
 
-	// Start job scheduler
+  /* Start job scheduler */
 	scheduler := jobs.NewScheduler(queue)
 	scheduler.Start()
 	defer scheduler.Stop()
 
-	// Start server
+  /* Start server */
 	addr := fmt.Sprintf("%s:%d", cfg.Server.Host, cfg.Server.Port)
 	srv := &http.Server{
 		Addr:         addr,
@@ -140,7 +153,7 @@ func main() {
 		WriteTimeout: cfg.Server.WriteTimeout,
 	}
 
-	// Graceful shutdown
+  /* Graceful shutdown */
 	go func() {
 		fmt.Printf("Server starting on %s\n", addr)
 		if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {
@@ -148,7 +161,7 @@ func main() {
 		}
 	}()
 
-	// Wait for interrupt signal
+  /* Wait for interrupt signal */
 	quit := make(chan os.Signal, 1)
 	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
 	<-quit
diff --git a/NeuronAgent/cmd/generate-key/main.go b/NeuronAgent/cmd/generate-key/main.go
index 5fe2de6..e4a231d 100644
--- a/NeuronAgent/cmd/generate-key/main.go
+++ b/NeuronAgent/cmd/generate-key/main.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * main.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/cmd/generate-key/main.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package main
 
 import (
@@ -26,7 +39,7 @@ func main() {
 	)
 	flag.Parse()
 
-	// Parse roles
+  /* Parse roles */
 	roleList := []string{}
 	if *roles != "" {
 		roleList = strings.Split(*roles, ",")
@@ -35,7 +48,7 @@ func main() {
 		}
 	}
 
-	// Connect to database
+  /* Connect to database */
 	cfg := config.DefaultConfig()
 	cfg.Database.Host = *dbHost
 	cfg.Database.Port = *dbPort
@@ -61,7 +74,7 @@ func main() {
 	queries := db.NewQueries(database.DB)
 	keyManager := auth.NewAPIKeyManager(queries)
 
-	// Generate key
+  /* Generate key */
 	ctx := context.Background()
 	var orgIDPtr, userIDPtr *string
 	if *orgID != "" {
diff --git a/NeuronAgent/examples/MODULAR_GUIDE.md b/NeuronAgent/examples/MODULAR_GUIDE.md
index 62631c9..e14bb2c 100644
--- a/NeuronAgent/examples/MODULAR_GUIDE.md
+++ b/NeuronAgent/examples/MODULAR_GUIDE.md
@@ -550,3 +550,4 @@ For issues or questions:
 
 
 
+
diff --git a/NeuronAgent/examples/QUICKSTART.md b/NeuronAgent/examples/QUICKSTART.md
index 018187c..8e51506 100644
--- a/NeuronAgent/examples/QUICKSTART.md
+++ b/NeuronAgent/examples/QUICKSTART.md
@@ -120,3 +120,4 @@ Happy coding! ðŸš€
 
 
 
+
diff --git a/NeuronAgent/examples/README.md b/NeuronAgent/examples/README.md
index b5a51d1..2c09ce9 100644
--- a/NeuronAgent/examples/README.md
+++ b/NeuronAgent/examples/README.md
@@ -334,3 +334,4 @@ See [LICENSE](../../LICENSE) for license information.
 
 
 
+
diff --git a/NeuronAgent/examples/REQUIREMENTS.md b/NeuronAgent/examples/REQUIREMENTS.md
index 034c8b5..a271dbc 100644
--- a/NeuronAgent/examples/REQUIREMENTS.md
+++ b/NeuronAgent/examples/REQUIREMENTS.md
@@ -445,3 +445,4 @@ Most dependencies use permissive licenses (MIT, Apache 2.0, BSD).
 
 
 
+
diff --git a/NeuronAgent/examples/agent_configs.json b/NeuronAgent/examples/agent_configs.json
index db9120d..d2123c7 100644
--- a/NeuronAgent/examples/agent_configs.json
+++ b/NeuronAgent/examples/agent_configs.json
@@ -84,3 +84,4 @@
 
 
 
+
diff --git a/NeuronAgent/examples/complete_example.py b/NeuronAgent/examples/complete_example.py
index 14a421a..a23c338 100755
--- a/NeuronAgent/examples/complete_example.py
+++ b/NeuronAgent/examples/complete_example.py
@@ -332,3 +332,4 @@ if __name__ == "__main__":
 
 
 
+
diff --git a/NeuronAgent/examples/examples_modular/01_basic_usage.py b/NeuronAgent/examples/examples_modular/01_basic_usage.py
index bc86180..fac063f 100755
--- a/NeuronAgent/examples/examples_modular/01_basic_usage.py
+++ b/NeuronAgent/examples/examples_modular/01_basic_usage.py
@@ -71,3 +71,4 @@ if __name__ == "__main__":
 
 
 
+
diff --git a/NeuronAgent/examples/examples_modular/02_agent_profiles.py b/NeuronAgent/examples/examples_modular/02_agent_profiles.py
index 33db00f..8c014e8 100755
--- a/NeuronAgent/examples/examples_modular/02_agent_profiles.py
+++ b/NeuronAgent/examples/examples_modular/02_agent_profiles.py
@@ -80,3 +80,4 @@ if __name__ == "__main__":
 
 
 
+
diff --git a/NeuronAgent/examples/examples_modular/03_conversation_manager.py b/NeuronAgent/examples/examples_modular/03_conversation_manager.py
index 7d68b31..574ce7e 100755
--- a/NeuronAgent/examples/examples_modular/03_conversation_manager.py
+++ b/NeuronAgent/examples/examples_modular/03_conversation_manager.py
@@ -78,3 +78,4 @@ if __name__ == "__main__":
 
 
 
+
diff --git a/NeuronAgent/examples/examples_modular/04_streaming.py b/NeuronAgent/examples/examples_modular/04_streaming.py
index 6cd8dff..db5f6c7 100755
--- a/NeuronAgent/examples/examples_modular/04_streaming.py
+++ b/NeuronAgent/examples/examples_modular/04_streaming.py
@@ -67,3 +67,4 @@ if __name__ == "__main__":
 
 
 
+
diff --git a/NeuronAgent/examples/examples_modular/05_production_patterns.py b/NeuronAgent/examples/examples_modular/05_production_patterns.py
index dbb1f39..9edac7f 100755
--- a/NeuronAgent/examples/examples_modular/05_production_patterns.py
+++ b/NeuronAgent/examples/examples_modular/05_production_patterns.py
@@ -182,3 +182,4 @@ if __name__ == "__main__":
 
 
 
+
diff --git a/NeuronAgent/examples/examples_modular/06_advanced_agent_management.py b/NeuronAgent/examples/examples_modular/06_advanced_agent_management.py
index 76e5aae..3b3abca 100755
--- a/NeuronAgent/examples/examples_modular/06_advanced_agent_management.py
+++ b/NeuronAgent/examples/examples_modular/06_advanced_agent_management.py
@@ -110,3 +110,4 @@ if __name__ == "__main__":
 
 
 
+
diff --git a/NeuronAgent/examples/examples_modular/README.md b/NeuronAgent/examples/examples_modular/README.md
index dd44be4..cd970d2 100644
--- a/NeuronAgent/examples/examples_modular/README.md
+++ b/NeuronAgent/examples/examples_modular/README.md
@@ -234,3 +234,4 @@ api_key = loader.get_env("NEURONAGENT_API_KEY")
 
 
 
+
diff --git a/NeuronAgent/examples/go_client.go b/NeuronAgent/examples/go_client.go
index 199685b..394170e 100644
--- a/NeuronAgent/examples/go_client.go
+++ b/NeuronAgent/examples/go_client.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * go_client.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/examples/go_client.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package main
 
 import (
@@ -12,14 +25,14 @@ import (
 	"github.com/google/uuid"
 )
 
-// NeuronAgentClient is a Go client for NeuronAgent API
+/* NeuronAgentClient is a Go client for NeuronAgent API */
 type NeuronAgentClient struct {
 	BaseURL string
 	APIKey  string
 	Client  *http.Client
 }
 
-// NewNeuronAgentClient creates a new client instance
+/* NewNeuronAgentClient creates a new client instance */
 func NewNeuronAgentClient(baseURL, apiKey string) *NeuronAgentClient {
 	if baseURL == "" {
 		baseURL = "http://localhost:8080"
@@ -37,7 +50,7 @@ func NewNeuronAgentClient(baseURL, apiKey string) *NeuronAgentClient {
 	}
 }
 
-// Agent represents an agent configuration
+/* Agent represents an agent configuration */
 type Agent struct {
 	ID           uuid.UUID              `json:"id"`
 	Name         string                 `json:"name"`
@@ -50,7 +63,7 @@ type Agent struct {
 	UpdatedAt    time.Time              `json:"updated_at"`
 }
 
-// Session represents a conversation session
+/* Session represents a conversation session */
 type Session struct {
 	ID             uuid.UUID              `json:"id"`
 	AgentID        uuid.UUID              `json:"agent_id"`
@@ -60,7 +73,7 @@ type Session struct {
 	LastActivityAt time.Time              `json:"last_activity_at"`
 }
 
-// Message represents a message in a conversation
+/* Message represents a message in a conversation */
 type Message struct {
 	ID         int64                  `json:"id"`
 	SessionID  uuid.UUID              `json:"session_id"`
@@ -70,7 +83,7 @@ type Message struct {
 	CreatedAt  time.Time              `json:"created_at"`
 }
 
-// CreateAgentRequest is the request to create an agent
+/* CreateAgentRequest is the request to create an agent */
 type CreateAgentRequest struct {
 	Name         string                 `json:"name"`
 	Description  *string                `json:"description,omitempty"`
@@ -80,14 +93,14 @@ type CreateAgentRequest struct {
 	Config       map[string]interface{} `json:"config"`
 }
 
-// CreateSessionRequest is the request to create a session
+/* CreateSessionRequest is the request to create a session */
 type CreateSessionRequest struct {
 	AgentID       uuid.UUID              `json:"agent_id"`
 	ExternalUserID *string                `json:"external_user_id,omitempty"`
 	Metadata      map[string]interface{}  `json:"metadata,omitempty"`
 }
 
-// SendMessageRequest is the request to send a message
+/* SendMessageRequest is the request to send a message */
 type SendMessageRequest struct {
 	Content  string                 `json:"content"`
 	Role     string                 `json:"role"`
@@ -95,7 +108,7 @@ type SendMessageRequest struct {
 	Metadata map[string]interface{} `json:"metadata,omitempty"`
 }
 
-// SendMessageResponse is the response from sending a message
+/* SendMessageResponse is the response from sending a message */
 type SendMessageResponse struct {
 	SessionID   uuid.UUID `json:"session_id"`
 	AgentID     uuid.UUID `json:"agent_id"`
@@ -105,7 +118,7 @@ type SendMessageResponse struct {
 	ToolResults []interface{} `json:"tool_results"`
 }
 
-// makeRequest makes an authenticated HTTP request
+/* makeRequest makes an authenticated HTTP request */
 func (c *NeuronAgentClient) makeRequest(method, path string, body interface{}) (*http.Response, error) {
 	var reqBody io.Reader
 	if body != nil {
@@ -138,7 +151,7 @@ func (c *NeuronAgentClient) makeRequest(method, path string, body interface{}) (
 	return resp, nil
 }
 
-// HealthCheck checks if the server is healthy
+/* HealthCheck checks if the server is healthy */
 func (c *NeuronAgentClient) HealthCheck() error {
 	resp, err := http.Get(c.BaseURL + "/health")
 	if err != nil {
@@ -152,7 +165,7 @@ func (c *NeuronAgentClient) HealthCheck() error {
 	return nil
 }
 
-// CreateAgent creates a new agent
+/* CreateAgent creates a new agent */
 func (c *NeuronAgentClient) CreateAgent(req CreateAgentRequest) (*Agent, error) {
 	resp, err := c.makeRequest("POST", "/api/v1/agents", req)
 	if err != nil {
@@ -168,7 +181,7 @@ func (c *NeuronAgentClient) CreateAgent(req CreateAgentRequest) (*Agent, error)
 	return &agent, nil
 }
 
-// GetAgent retrieves an agent by ID
+/* GetAgent retrieves an agent by ID */
 func (c *NeuronAgentClient) GetAgent(id uuid.UUID) (*Agent, error) {
 	resp, err := c.makeRequest("GET", "/api/v1/agents/"+id.String(), nil)
 	if err != nil {
@@ -184,7 +197,7 @@ func (c *NeuronAgentClient) GetAgent(id uuid.UUID) (*Agent, error) {
 	return &agent, nil
 }
 
-// ListAgents lists all agents
+/* ListAgents lists all agents */
 func (c *NeuronAgentClient) ListAgents() ([]Agent, error) {
 	resp, err := c.makeRequest("GET", "/api/v1/agents", nil)
 	if err != nil {
@@ -200,7 +213,7 @@ func (c *NeuronAgentClient) ListAgents() ([]Agent, error) {
 	return agents, nil
 }
 
-// CreateSession creates a new session
+/* CreateSession creates a new session */
 func (c *NeuronAgentClient) CreateSession(req CreateSessionRequest) (*Session, error) {
 	resp, err := c.makeRequest("POST", "/api/v1/sessions", req)
 	if err != nil {
@@ -216,7 +229,7 @@ func (c *NeuronAgentClient) CreateSession(req CreateSessionRequest) (*Session, e
 	return &session, nil
 }
 
-// SendMessage sends a message to the agent
+/* SendMessage sends a message to the agent */
 func (c *NeuronAgentClient) SendMessage(sessionID uuid.UUID, req SendMessageRequest) (*SendMessageResponse, error) {
 	resp, err := c.makeRequest("POST", "/api/v1/sessions/"+sessionID.String()+"/messages", req)
 	if err != nil {
@@ -232,7 +245,7 @@ func (c *NeuronAgentClient) SendMessage(sessionID uuid.UUID, req SendMessageRequ
 	return &response, nil
 }
 
-// GetMessages retrieves messages from a session
+/* GetMessages retrieves messages from a session */
 func (c *NeuronAgentClient) GetMessages(sessionID uuid.UUID, limit, offset int) ([]Message, error) {
 	path := fmt.Sprintf("/api/v1/sessions/%s/messages?limit=%d&offset=%d", sessionID.String(), limit, offset)
 	resp, err := c.makeRequest("GET", path, nil)
@@ -249,7 +262,7 @@ func (c *NeuronAgentClient) GetMessages(sessionID uuid.UUID, limit, offset int)
 	return messages, nil
 }
 
-// Example usage
+/* Example usage */
 func main() {
 	apiKey := os.Getenv("NEURONAGENT_API_KEY")
 	if apiKey == "" {
@@ -260,7 +273,7 @@ func main() {
 
 	client := NewNeuronAgentClient("http://localhost:8080", apiKey)
 
-	// Health check
+  /* Health check */
 	fmt.Println("Checking server health...")
 	if err := client.HealthCheck(); err != nil {
 		fmt.Printf("âŒ Server health check failed: %v\n", err)
@@ -268,7 +281,7 @@ func main() {
 	}
 	fmt.Println("âœ… Server is healthy")
 
-	// Create an agent
+  /* Create an agent */
 	fmt.Println("\nCreating agent...")
 	agent, err := client.CreateAgent(CreateAgentRequest{
 		Name:         "go-example-agent",
@@ -287,7 +300,7 @@ func main() {
 	}
 	fmt.Printf("âœ… Agent created: %s (%s)\n", agent.Name, agent.ID)
 
-	// Create a session
+  /* Create a session */
 	fmt.Println("\nCreating session...")
 	session, err := client.CreateSession(CreateSessionRequest{
 		AgentID:       agent.ID,
@@ -302,7 +315,7 @@ func main() {
 	}
 	fmt.Printf("âœ… Session created: %s\n", session.ID)
 
-	// Send messages
+  /* Send messages */
 	messages := []string{
 		"Hello! Can you introduce yourself?",
 		"What can you help me with?",
@@ -324,7 +337,7 @@ func main() {
 		time.Sleep(1 * time.Second)
 	}
 
-	// Get conversation history
+  /* Get conversation history */
 	fmt.Println("\nðŸ“œ Retrieving conversation history...")
 	history, err := client.GetMessages(session.ID, 10, 0)
 	if err != nil {
@@ -339,7 +352,7 @@ func main() {
 	fmt.Println("\nâœ… Example completed successfully!")
 }
 
-// Helper functions
+/* Helper functions */
 func stringPtr(s string) *string {
 	return &s
 }
@@ -353,3 +366,4 @@ func truncateString(s string, maxLen int) string {
 
 
 
+
diff --git a/NeuronAgent/examples/neurondb_client/__init__.py b/NeuronAgent/examples/neurondb_client/__init__.py
index c2dd3a3..e63dd58 100644
--- a/NeuronAgent/examples/neurondb_client/__init__.py
+++ b/NeuronAgent/examples/neurondb_client/__init__.py
@@ -46,3 +46,4 @@ __all__ = [
 
 
 
+
diff --git a/NeuronAgent/examples/neurondb_client/agents/manager.py b/NeuronAgent/examples/neurondb_client/agents/manager.py
index a00999a..f6445c1 100644
--- a/NeuronAgent/examples/neurondb_client/agents/manager.py
+++ b/NeuronAgent/examples/neurondb_client/agents/manager.py
@@ -216,3 +216,4 @@ class AgentManager:
 
 
 
+
diff --git a/NeuronAgent/examples/neurondb_client/agents/profile.py b/NeuronAgent/examples/neurondb_client/agents/profile.py
index ea2186a..70bc0d4 100644
--- a/NeuronAgent/examples/neurondb_client/agents/profile.py
+++ b/NeuronAgent/examples/neurondb_client/agents/profile.py
@@ -133,3 +133,4 @@ def get_default_profile(name: str) -> Optional[AgentProfile]:
 
 
 
+
diff --git a/NeuronAgent/examples/neurondb_client/core/exceptions.py b/NeuronAgent/examples/neurondb_client/core/exceptions.py
index b740ddb..81d255d 100644
--- a/NeuronAgent/examples/neurondb_client/core/exceptions.py
+++ b/NeuronAgent/examples/neurondb_client/core/exceptions.py
@@ -54,3 +54,4 @@ class TimeoutError(NeuronAgentError):
 
 
 
+
diff --git a/NeuronAgent/examples/neurondb_client/core/websocket.py b/NeuronAgent/examples/neurondb_client/core/websocket.py
index 52bab38..6b4d9f2 100644
--- a/NeuronAgent/examples/neurondb_client/core/websocket.py
+++ b/NeuronAgent/examples/neurondb_client/core/websocket.py
@@ -109,3 +109,4 @@ class WebSocketClient:
 
 
 
+
diff --git a/NeuronAgent/examples/neurondb_client/sessions/conversation.py b/NeuronAgent/examples/neurondb_client/sessions/conversation.py
index 91fce8d..745a4be 100644
--- a/NeuronAgent/examples/neurondb_client/sessions/conversation.py
+++ b/NeuronAgent/examples/neurondb_client/sessions/conversation.py
@@ -238,3 +238,4 @@ class ConversationManager:
 
 
 
+
diff --git a/NeuronAgent/examples/neurondb_client/sessions/manager.py b/NeuronAgent/examples/neurondb_client/sessions/manager.py
index 2ad1919..8d940d4 100644
--- a/NeuronAgent/examples/neurondb_client/sessions/manager.py
+++ b/NeuronAgent/examples/neurondb_client/sessions/manager.py
@@ -167,3 +167,4 @@ class SessionManager:
 
 
 
+
diff --git a/NeuronAgent/examples/neurondb_client/utils/config.py b/NeuronAgent/examples/neurondb_client/utils/config.py
index 77308f2..4ba06cd 100644
--- a/NeuronAgent/examples/neurondb_client/utils/config.py
+++ b/NeuronAgent/examples/neurondb_client/utils/config.py
@@ -96,3 +96,4 @@ class ConfigLoader:
 
 
 
+
diff --git a/NeuronAgent/examples/neurondb_client/utils/logging.py b/NeuronAgent/examples/neurondb_client/utils/logging.py
index 18225c4..c04ef2a 100644
--- a/NeuronAgent/examples/neurondb_client/utils/logging.py
+++ b/NeuronAgent/examples/neurondb_client/utils/logging.py
@@ -51,3 +51,4 @@ def get_logger(name: str) -> logging.Logger:
 
 
 
+
diff --git a/NeuronAgent/examples/neurondb_client/utils/metrics.py b/NeuronAgent/examples/neurondb_client/utils/metrics.py
index d3c9253..2dd228f 100644
--- a/NeuronAgent/examples/neurondb_client/utils/metrics.py
+++ b/NeuronAgent/examples/neurondb_client/utils/metrics.py
@@ -98,3 +98,4 @@ class MetricsCollector:
 
 
 
+
diff --git a/NeuronAgent/examples/python_client.py b/NeuronAgent/examples/python_client.py
index bd62566..044410d 100755
--- a/NeuronAgent/examples/python_client.py
+++ b/NeuronAgent/examples/python_client.py
@@ -533,3 +533,4 @@ if __name__ == "__main__":
 
 
 
+
diff --git a/NeuronAgent/examples/quick_test.py b/NeuronAgent/examples/quick_test.py
index d5fa93e..cd5d7b7 100755
--- a/NeuronAgent/examples/quick_test.py
+++ b/NeuronAgent/examples/quick_test.py
@@ -60,3 +60,4 @@ print("3. Run examples: python3 examples_modular/01_basic_usage.py")
 
 
 
+
diff --git a/NeuronAgent/examples/requirements-dev.txt b/NeuronAgent/examples/requirements-dev.txt
index 2f18795..beeb4e5 100644
--- a/NeuronAgent/examples/requirements-dev.txt
+++ b/NeuronAgent/examples/requirements-dev.txt
@@ -47,3 +47,4 @@ wheel>=0.41.0,<1.0.0
 
 
 
+
diff --git a/NeuronAgent/examples/requirements-minimal.txt b/NeuronAgent/examples/requirements-minimal.txt
index 856aae4..7fedc75 100644
--- a/NeuronAgent/examples/requirements-minimal.txt
+++ b/NeuronAgent/examples/requirements-minimal.txt
@@ -18,3 +18,4 @@ urllib3>=2.0.0,<3.0.0
 
 
 
+
diff --git a/NeuronAgent/examples/requirements-neurondb.txt b/NeuronAgent/examples/requirements-neurondb.txt
index 9621f52..412430f 100644
--- a/NeuronAgent/examples/requirements-neurondb.txt
+++ b/NeuronAgent/examples/requirements-neurondb.txt
@@ -56,3 +56,4 @@ jsonschema>=4.19.0,<5.0.0
 
 
 
+
diff --git a/NeuronAgent/examples/setup_example.sh b/NeuronAgent/examples/setup_example.sh
index 8f7487e..7bd6aa4 100755
--- a/NeuronAgent/examples/setup_example.sh
+++ b/NeuronAgent/examples/setup_example.sh
@@ -89,3 +89,4 @@ echo ""
 
 
 
+
diff --git a/NeuronAgent/examples/test_agent.sh b/NeuronAgent/examples/test_agent.sh
index dcd10cf..6af56fd 100755
--- a/NeuronAgent/examples/test_agent.sh
+++ b/NeuronAgent/examples/test_agent.sh
@@ -92,3 +92,4 @@ echo "  cd examples && python3 examples_modular/01_basic_usage.py"
 
 
 
+
diff --git a/NeuronAgent/examples/test_with_psql.sql b/NeuronAgent/examples/test_with_psql.sql
index 02b6efe..eb62ebf 100644
--- a/NeuronAgent/examples/test_with_psql.sql
+++ b/NeuronAgent/examples/test_with_psql.sql
@@ -97,3 +97,4 @@ SELECT
 
 
 
+
diff --git a/NeuronAgent/internal/agent/context.go b/NeuronAgent/internal/agent/context.go
index 1bfa794..f827186 100644
--- a/NeuronAgent/internal/agent/context.go
+++ b/NeuronAgent/internal/agent/context.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * context.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/agent/context.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package agent
 
 import (
@@ -28,23 +41,23 @@ func NewContextLoader(queries *db.Queries, memory *MemoryManager, llm *LLMClient
 }
 
 func (l *ContextLoader) Load(ctx context.Context, sessionID uuid.UUID, agentID uuid.UUID, userMessage string, maxMessages int, maxMemoryChunks int) (*Context, error) {
-	// Load recent messages
+  /* Load recent messages */
 	messages, err := l.queries.GetRecentMessages(ctx, sessionID, maxMessages)
 	if err != nil {
 		return nil, fmt.Errorf("context loading failed (load messages): session_id='%s', agent_id='%s', user_message_length=%d, max_messages=%d, error=%w",
 			sessionID.String(), agentID.String(), len(userMessage), maxMessages, err)
 	}
 
-	// Generate embedding for user message to search memory
+  /* Generate embedding for user message to search memory */
 	embeddingModel := "all-MiniLM-L6-v2"
 	embedding, err := l.llm.Embed(ctx, embeddingModel, userMessage)
 	if err != nil {
-		// If embedding fails, continue without memory chunks but log the error
+   /* If embedding fails, continue without memory chunks but log the error */
 		embedding = nil
-		// Note: We continue without memory chunks, but this is logged
+   /* Note: We continue without memory chunks, but this is logged */
 	}
 
-	// Retrieve relevant memory chunks
+  /* Retrieve relevant memory chunks */
 	var memoryChunks []MemoryChunk
 	if embedding != nil {
 		chunks, err := l.memory.Retrieve(ctx, agentID, embedding, maxMemoryChunks)
@@ -61,26 +74,26 @@ func (l *ContextLoader) Load(ctx context.Context, sessionID uuid.UUID, agentID u
 	}, nil
 }
 
-// CompressContext reduces context size by summarizing or removing less important messages
+/* CompressContext reduces context size by summarizing or removing less important messages */
 func CompressContext(ctx *Context, maxTokens int) *Context {
-	// Count tokens in current context
+  /* Count tokens in current context */
 	totalTokens := 0
 	for _, msg := range ctx.Messages {
 		totalTokens += EstimateTokens(msg.Content)
 	}
 	
-	// If within limit, return as is
+  /* If within limit, return as is */
 	if totalTokens <= maxTokens {
 		return ctx
 	}
 	
-	// Strategy: Keep system messages, recent messages, and important memory chunks
+  /* Strategy: Keep system messages, recent messages, and important memory chunks */
 	compressed := &Context{
 		Messages:     []db.Message{},
 		MemoryChunks: []MemoryChunk{},
 	}
 	
-	// Keep all memory chunks (they're already filtered)
+  /* Keep all memory chunks (they're already filtered) */
 	compressed.MemoryChunks = ctx.MemoryChunks
 	memoryTokens := 0
 	for _, chunk := range ctx.MemoryChunks {
@@ -89,11 +102,11 @@ func CompressContext(ctx *Context, maxTokens int) *Context {
 	
 	availableTokens := maxTokens - memoryTokens
 	if availableTokens < 100 {
-		// Not enough space, return minimal context
+   /* Not enough space, return minimal context */
 		return compressed
 	}
 	
-	// Keep messages from most recent, up to token limit
+  /* Keep messages from most recent, up to token limit */
 	tokensUsed := 0
 	for i := len(ctx.Messages) - 1; i >= 0; i-- {
 		msg := ctx.Messages[i]
@@ -103,7 +116,7 @@ func CompressContext(ctx *Context, maxTokens int) *Context {
 			break
 		}
 		
-		// Prepend to maintain order
+   /* Prepend to maintain order */
 		compressed.Messages = append([]db.Message{msg}, compressed.Messages...)
 		tokensUsed += msgTokens
 	}
diff --git a/NeuronAgent/internal/agent/llm.go b/NeuronAgent/internal/agent/llm.go
index 3026b06..1bb5af5 100644
--- a/NeuronAgent/internal/agent/llm.go
+++ b/NeuronAgent/internal/agent/llm.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * llm.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/agent/llm.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package agent
 
 import (
@@ -27,7 +40,7 @@ func (c *LLMClient) Generate(ctx context.Context, modelName string, prompt strin
 		Model: modelName,
 	}
 
-	// Extract config values
+  /* Extract config values */
 	if temp, ok := config["temperature"].(float64); ok {
 		llmConfig.Temperature = &temp
 	}
@@ -41,12 +54,12 @@ func (c *LLMClient) Generate(ctx context.Context, modelName string, prompt strin
 
 	result, err := c.llmClient.Generate(ctx, prompt, llmConfig)
 	
-	// Record metrics
+  /* Record metrics */
 	status := "success"
 	if err != nil {
 		status = "error"
 	}
-	metrics.RecordLLMCall(modelName, status, result.TokensUsed, 0) // Completion tokens not available
+ 	metrics.RecordLLMCall(modelName, status, result.TokensUsed, 0) /* Completion tokens not available */
 	
 	if err != nil {
 		promptTokens := EstimateTokens(prompt)
@@ -66,7 +79,7 @@ func (c *LLMClient) Generate(ctx context.Context, modelName string, prompt strin
 			modelName, len(prompt), promptTokens, temperature, maxTokens, topP, err)
 	}
 
-	// Estimate completion tokens if not provided
+  /* Estimate completion tokens if not provided */
 	completionTokens := EstimateTokens(result.Output)
 	promptTokens := EstimateTokens(prompt)
 	if result.TokensUsed == 0 {
@@ -75,7 +88,7 @@ func (c *LLMClient) Generate(ctx context.Context, modelName string, prompt strin
 
 	return &LLMResponse{
 		Content:   result.Output,
-		ToolCalls: []ToolCall{}, // Will be parsed separately
+  		ToolCalls: []ToolCall{}, /* Will be parsed separately */
 		Usage: TokenUsage{
 			PromptTokens:     promptTokens,
 			CompletionTokens: completionTokens,
@@ -90,7 +103,7 @@ func (c *LLMClient) GenerateStream(ctx context.Context, modelName string, prompt
 		Stream: true,
 	}
 
-	// Extract config values
+  /* Extract config values */
 	if temp, ok := config["temperature"].(float64); ok {
 		llmConfig.Temperature = &temp
 	}
diff --git a/NeuronAgent/internal/agent/memory.go b/NeuronAgent/internal/agent/memory.go
index bc13df7..8ab6403 100644
--- a/NeuronAgent/internal/agent/memory.go
+++ b/NeuronAgent/internal/agent/memory.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * memory.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/agent/memory.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package agent
 
 import (
@@ -34,7 +47,7 @@ func NewMemoryManager(db *db.DB, queries *db.Queries, embedClient *neurondb.Embe
 }
 
 func (m *MemoryManager) Retrieve(ctx context.Context, agentID uuid.UUID, queryEmbedding []float32, topK int) ([]MemoryChunk, error) {
-	// Record metrics
+  /* Record metrics */
 	defer func() {
 		metrics.RecordMemoryRetrieval(agentID.String())
 	}()
@@ -60,24 +73,24 @@ func (m *MemoryManager) Retrieve(ctx context.Context, agentID uuid.UUID, queryEm
 }
 
 func (m *MemoryManager) StoreChunks(ctx context.Context, agentID, sessionID uuid.UUID, content string, toolResults []ToolResult) {
-	// Compute importance score (heuristic: length, user flags, etc.)
+  /* Compute importance score (heuristic: length, user flags, etc.) */
 	importance := m.computeImportance(content, toolResults)
 
-	// Only store if importance > threshold
+  /* Only store if importance > threshold */
 	if importance < 0.3 {
 		return
 	}
 
-	// Compute embedding
+  /* Compute embedding */
 	embeddingModel := "all-MiniLM-L6-v2"
 	embedding, err := m.embed.Embed(ctx, content, embeddingModel)
 	if err != nil {
-		// Log error but don't fail (async operation)
-		// Error is already detailed in embedding client
+   /* Log error but don't fail (async operation) */
+   /* Error is already detailed in embedding client */
 		return
 	}
 
-	// Store chunk
+  /* Store chunk */
 	_, err = m.queries.CreateMemoryChunk(ctx, &db.MemoryChunk{
 		AgentID:         agentID,
 		SessionID:       &sessionID,
@@ -86,31 +99,31 @@ func (m *MemoryManager) StoreChunks(ctx context.Context, agentID, sessionID uuid
 		ImportanceScore: importance,
 	})
 	if err != nil {
-		// Log error but don't fail (async operation)
-		// Error is already detailed in queries.CreateMemoryChunk
+   /* Log error but don't fail (async operation) */
+   /* Error is already detailed in queries.CreateMemoryChunk */
 		return
 	}
 
-	// Record metrics
+  /* Record metrics */
 	metrics.RecordMemoryChunkStored(agentID.String())
 }
 
 func (m *MemoryManager) computeImportance(content string, toolResults []ToolResult) float64 {
-	score := 0.5 // Base score
+ 	score := 0.5 /* Base score */
 
-	// Increase score based on content length (longer = more important)
+  /* Increase score based on content length (longer = more important) */
 	if len(content) > 500 {
 		score += 0.2
 	} else if len(content) > 200 {
 		score += 0.1
 	}
 
-	// Increase score if tool results present (actionable information)
+  /* Increase score if tool results present (actionable information) */
 	if len(toolResults) > 0 {
 		score += 0.2
 	}
 
-	// Increase score if content contains important keywords
+  /* Increase score if content contains important keywords */
 	importantKeywords := []string{"error", "solution", "important", "note", "warning", "summary"}
 	contentLower := strings.ToLower(content)
 	for _, keyword := range importantKeywords {
@@ -120,7 +133,7 @@ func (m *MemoryManager) computeImportance(content string, toolResults []ToolResu
 		}
 	}
 
-	// Cap at 1.0
+  /* Cap at 1.0 */
 	if score > 1.0 {
 		score = 1.0
 	}
diff --git a/NeuronAgent/internal/agent/planner.go b/NeuronAgent/internal/agent/planner.go
index 9a5d7a4..29d88fa 100644
--- a/NeuronAgent/internal/agent/planner.go
+++ b/NeuronAgent/internal/agent/planner.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * planner.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/agent/planner.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package agent
 
 import (
@@ -11,14 +24,14 @@ type Planner struct {
 
 func NewPlanner() *Planner {
 	return &Planner{
-		maxIterations: 10, // Prevent infinite loops
+  		maxIterations: 10, /* Prevent infinite loops */
 	}
 }
 
-// Plan creates a multi-step plan for complex tasks
+/* Plan creates a multi-step plan for complex tasks */
 func (p *Planner) Plan(ctx context.Context, userMessage string, availableTools []string) ([]PlanStep, error) {
-	// Simple implementation: single step plan
-	// In production, this would use an LLM to break down complex tasks
+  /* Simple implementation: single step plan */
+  /* In production, this would use an LLM to break down complex tasks */
 	steps := []PlanStep{
 		{
 			Action:   "execute",
@@ -35,7 +48,7 @@ type PlanStep struct {
 	Payload map[string]interface{}
 }
 
-// ExecutePlan executes a multi-step plan
+/* ExecutePlan executes a multi-step plan */
 func (p *Planner) ExecutePlan(ctx context.Context, steps []PlanStep, executor func(step PlanStep) (interface{}, error)) ([]interface{}, error) {
 	var results []interface{}
 	iterations := 0
diff --git a/NeuronAgent/internal/agent/profiles.go b/NeuronAgent/internal/agent/profiles.go
index 076b190..ba2fbbe 100644
--- a/NeuronAgent/internal/agent/profiles.go
+++ b/NeuronAgent/internal/agent/profiles.go
@@ -1,10 +1,23 @@
+/*-------------------------------------------------------------------------
+ *
+ * profiles.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/agent/profiles.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package agent
 
 import (
 	"github.com/neurondb/NeuronAgent/internal/db"
 )
 
-// Profile represents a predefined agent profile
+/* Profile represents a predefined agent profile */
 type Profile struct {
 	Name         string
 	Description  string
@@ -14,7 +27,7 @@ type Profile struct {
 	EnabledTools []string
 }
 
-// GetDefaultProfiles returns default agent profiles
+/* GetDefaultProfiles returns default agent profiles */
 func GetDefaultProfiles() []Profile {
 	return []Profile{
 		{
@@ -68,7 +81,7 @@ func GetDefaultProfiles() []Profile {
 	}
 }
 
-// CreateAgentFromProfile creates an agent from a profile
+/* CreateAgentFromProfile creates an agent from a profile */
 func CreateAgentFromProfile(profile Profile) *db.Agent {
 	return &db.Agent{
 		Name:         profile.Name,
@@ -80,7 +93,7 @@ func CreateAgentFromProfile(profile Profile) *db.Agent {
 	}
 }
 
-// FindProfile finds a profile by name
+/* FindProfile finds a profile by name */
 func FindProfile(name string) *Profile {
 	profiles := GetDefaultProfiles()
 	for _, p := range profiles {
diff --git a/NeuronAgent/internal/agent/prompt.go b/NeuronAgent/internal/agent/prompt.go
index 9eda511..ed5ec37 100644
--- a/NeuronAgent/internal/agent/prompt.go
+++ b/NeuronAgent/internal/agent/prompt.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * prompt.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/agent/prompt.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package agent
 
 import (
@@ -13,7 +26,7 @@ type PromptBuilder struct {
 
 func NewPromptBuilder() *PromptBuilder {
 	return &PromptBuilder{
-		maxTokens: 4000, // Default max tokens
+  		maxTokens: 4000, /* Default max tokens */
 	}
 }
 
@@ -24,10 +37,10 @@ func (p *PromptBuilder) SetMaxTokens(maxTokens int) {
 func (p *PromptBuilder) Build(agent *db.Agent, context *Context, userMessage string) (string, error) {
 	var parts []string
 
-	// System prompt
+  /* System prompt */
 	parts = append(parts, agent.SystemPrompt)
 
-	// Memory chunks
+  /* Memory chunks */
 	if len(context.MemoryChunks) > 0 {
 		parts = append(parts, "\n\n## Relevant Context:")
 		for i, chunk := range context.MemoryChunks {
@@ -35,7 +48,7 @@ func (p *PromptBuilder) Build(agent *db.Agent, context *Context, userMessage str
 		}
 	}
 
-	// Conversation history
+  /* Conversation history */
 	if len(context.Messages) > 0 {
 		parts = append(parts, "\n\n## Conversation History:")
 		for _, msg := range context.Messages {
@@ -44,7 +57,7 @@ func (p *PromptBuilder) Build(agent *db.Agent, context *Context, userMessage str
 		}
 	}
 
-	// Current user message
+  /* Current user message */
 	parts = append(parts, fmt.Sprintf("\n\n## Current Request:\nUser: %s", userMessage))
 	parts = append(parts, "\n\nAssistant:")
 
@@ -54,10 +67,10 @@ func (p *PromptBuilder) Build(agent *db.Agent, context *Context, userMessage str
 func (p *PromptBuilder) BuildWithToolResults(agent *db.Agent, context *Context, userMessage string, llmResponse *LLMResponse, toolResults []ToolResult) (string, error) {
 	var parts []string
 
-	// System prompt
+  /* System prompt */
 	parts = append(parts, agent.SystemPrompt)
 
-	// Memory chunks
+  /* Memory chunks */
 	if len(context.MemoryChunks) > 0 {
 		parts = append(parts, "\n\n## Relevant Context:")
 		for i, chunk := range context.MemoryChunks {
@@ -65,7 +78,7 @@ func (p *PromptBuilder) BuildWithToolResults(agent *db.Agent, context *Context,
 		}
 	}
 
-	// Conversation history
+  /* Conversation history */
 	if len(context.Messages) > 0 {
 		parts = append(parts, "\n\n## Conversation History:")
 		for _, msg := range context.Messages {
@@ -74,10 +87,10 @@ func (p *PromptBuilder) BuildWithToolResults(agent *db.Agent, context *Context,
 		}
 	}
 
-	// Current user message
+  /* Current user message */
 	parts = append(parts, fmt.Sprintf("\n\n## Current Request:\nUser: %s", userMessage))
 
-	// Tool calls and results
+  /* Tool calls and results */
 	if len(llmResponse.ToolCalls) > 0 {
 		parts = append(parts, "\n\n## Tool Calls:")
 		for _, call := range llmResponse.ToolCalls {
diff --git a/NeuronAgent/internal/agent/runtime.go b/NeuronAgent/internal/agent/runtime.go
index 14dc876..33f5829 100644
--- a/NeuronAgent/internal/agent/runtime.go
+++ b/NeuronAgent/internal/agent/runtime.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * runtime.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/agent/runtime.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package agent
 
 import (
@@ -60,7 +73,7 @@ type TokenUsage struct {
 	TotalTokens      int
 }
 
-// ToolRegistry interface for tool management
+/* ToolRegistry interface for tool management */
 type ToolRegistry interface {
 	Get(name string) (*db.Tool, error)
 	Execute(ctx context.Context, tool *db.Tool, args map[string]interface{}) (string, error)
@@ -85,7 +98,7 @@ func (r *Runtime) Execute(ctx context.Context, sessionID uuid.UUID, userMessage
 		UserMessage: userMessage,
 	}
 
-	// Step 1: Load agent and session
+  /* Step 1: Load agent and session */
 	session, err := r.queries.GetSession(ctx, sessionID)
 	if err != nil {
 		return nil, fmt.Errorf("agent execution failed at step 1 (load session): session_id='%s', user_message_length=%d, error=%w",
@@ -99,7 +112,7 @@ func (r *Runtime) Execute(ctx context.Context, sessionID uuid.UUID, userMessage
 			sessionID.String(), session.AgentID.String(), len(userMessage), err)
 	}
 
-	// Step 2: Load context (recent messages + memory)
+  /* Step 2: Load context (recent messages + memory) */
 	contextLoader := NewContextLoader(r.queries, r.memory, r.llm)
 	agentContext, err := contextLoader.Load(ctx, sessionID, agent.ID, userMessage, 20, 5)
 	if err != nil {
@@ -108,7 +121,7 @@ func (r *Runtime) Execute(ctx context.Context, sessionID uuid.UUID, userMessage
 	}
 	state.Context = agentContext
 
-	// Step 3: Build prompt
+  /* Step 3: Build prompt */
 	prompt, err := r.prompt.Build(agent, agentContext, userMessage)
 	if err != nil {
 		messageCount := len(agentContext.Messages)
@@ -117,7 +130,7 @@ func (r *Runtime) Execute(ctx context.Context, sessionID uuid.UUID, userMessage
 			sessionID.String(), agent.ID.String(), agent.Name, len(userMessage), messageCount, memoryChunkCount, err)
 	}
 
-	// Step 4: Call LLM via NeuronDB
+  /* Step 4: Call LLM via NeuronDB */
 	llmResponse, err := r.llm.Generate(ctx, agent.ModelName, prompt, agent.Config)
 	if err != nil {
 		promptTokens := EstimateTokens(prompt)
@@ -125,26 +138,26 @@ func (r *Runtime) Execute(ctx context.Context, sessionID uuid.UUID, userMessage
 			sessionID.String(), agent.ID.String(), agent.Name, agent.ModelName, len(prompt), promptTokens, len(userMessage), err)
 	}
 	
-	// Update token count in response
+  /* Update token count in response */
 	if llmResponse.Usage.TotalTokens == 0 {
-		// Estimate if not provided
+   /* Estimate if not provided */
 		llmResponse.Usage.PromptTokens = EstimateTokens(prompt)
 		llmResponse.Usage.CompletionTokens = EstimateTokens(llmResponse.Content)
 		llmResponse.Usage.TotalTokens = llmResponse.Usage.PromptTokens + llmResponse.Usage.CompletionTokens
 	}
 
-	// Step 5: Parse tool calls from response
+  /* Step 5: Parse tool calls from response */
 	toolCalls, err := ParseToolCalls(llmResponse.Content)
 	if err == nil && len(toolCalls) > 0 {
 		llmResponse.ToolCalls = toolCalls
 	}
 	state.LLMResponse = llmResponse
 
-	// Step 6: Execute tools if any
+  /* Step 6: Execute tools if any */
 	if len(llmResponse.ToolCalls) > 0 {
 		state.ToolCalls = llmResponse.ToolCalls
 
-		// Execute tools
+   /* Execute tools */
 		toolResults, err := r.executeTools(ctx, agent, llmResponse.ToolCalls)
 		if err != nil {
 			toolNames := make([]string, len(llmResponse.ToolCalls))
@@ -156,7 +169,7 @@ func (r *Runtime) Execute(ctx context.Context, sessionID uuid.UUID, userMessage
 		}
 		state.ToolResults = toolResults
 
-		// Step 7: Call LLM again with tool results
+   /* Step 7: Call LLM again with tool results */
 		finalPrompt, err := r.prompt.BuildWithToolResults(agent, agentContext, userMessage, llmResponse, toolResults)
 		if err != nil {
 			return nil, fmt.Errorf("agent execution failed at step 7 (build final prompt): session_id='%s', agent_id='%s', agent_name='%s', tool_result_count=%d, error=%w",
@@ -170,7 +183,7 @@ func (r *Runtime) Execute(ctx context.Context, sessionID uuid.UUID, userMessage
 				sessionID.String(), agent.ID.String(), agent.Name, agent.ModelName, len(finalPrompt), finalPromptTokens, len(toolResults), err)
 		}
 		
-		// Update token counts
+   /* Update token counts */
 		if finalResponse.Usage.TotalTokens == 0 {
 			finalResponse.Usage.PromptTokens = EstimateTokens(finalPrompt)
 			finalResponse.Usage.CompletionTokens = EstimateTokens(finalResponse.Content)
@@ -183,18 +196,18 @@ func (r *Runtime) Execute(ctx context.Context, sessionID uuid.UUID, userMessage
 		state.FinalAnswer = llmResponse.Content
 		state.TokensUsed = llmResponse.Usage.TotalTokens
 		if state.TokensUsed == 0 {
-			// Estimate if not provided
+    /* Estimate if not provided */
 			state.TokensUsed = EstimateTokens(prompt) + EstimateTokens(state.FinalAnswer)
 		}
 	}
 
-	// Step 8: Store messages with token counts
+  /* Step 8: Store messages with token counts */
 	if err := r.storeMessages(ctx, sessionID, userMessage, state.FinalAnswer, state.ToolCalls, state.ToolResults, state.TokensUsed); err != nil {
 		return nil, fmt.Errorf("agent execution failed at step 8 (store messages): session_id='%s', agent_id='%s', agent_name='%s', user_message_length=%d, final_answer_length=%d, tool_call_count=%d, tool_result_count=%d, total_tokens=%d, error=%w",
 			sessionID.String(), agent.ID.String(), agent.Name, len(userMessage), len(state.FinalAnswer), len(state.ToolCalls), len(state.ToolResults), state.TokensUsed, err)
 	}
 
-	// Step 9: Store memory chunks (async, non-blocking)
+  /* Step 9: Store memory chunks (async, non-blocking) */
 	go func() {
 		bgCtx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
 		defer cancel()
@@ -208,7 +221,7 @@ func (r *Runtime) executeTools(ctx context.Context, agent *db.Agent, toolCalls [
 	results := make([]ToolResult, 0, len(toolCalls))
 
 	for _, call := range toolCalls {
-		// Get tool from registry
+   /* Get tool from registry */
 		tool, err := r.tools.Get(call.Name)
 		if err != nil {
 			argKeys := make([]string, 0, len(call.Arguments))
@@ -223,7 +236,7 @@ func (r *Runtime) executeTools(ctx context.Context, agent *db.Agent, toolCalls [
 			continue
 		}
 
-		// Check if tool is enabled for this agent
+   /* Check if tool is enabled for this agent */
 		if !contains(agent.EnabledTools, call.Name) {
 			results = append(results, ToolResult{
 				ToolCallID: call.ID,
@@ -233,7 +246,7 @@ func (r *Runtime) executeTools(ctx context.Context, agent *db.Agent, toolCalls [
 			continue
 		}
 
-		// Execute tool
+   /* Execute tool */
 		result, err := r.tools.Execute(ctx, tool, call.Arguments)
 		if err != nil {
 			argKeys := make([]string, 0, len(call.Arguments))
@@ -259,7 +272,7 @@ func (r *Runtime) executeTools(ctx context.Context, agent *db.Agent, toolCalls [
 }
 
 func (r *Runtime) storeMessages(ctx context.Context, sessionID uuid.UUID, userMsg, assistantMsg string, toolCalls []ToolCall, toolResults []ToolResult, totalTokens int) error {
-	// Store user message
+  /* Store user message */
 	userTokens := EstimateTokens(userMsg)
 	if _, err := r.queries.CreateMessage(ctx, &db.Message{
 		SessionID:  sessionID,
@@ -271,7 +284,7 @@ func (r *Runtime) storeMessages(ctx context.Context, sessionID uuid.UUID, userMs
 			sessionID.String(), len(userMsg), userTokens, err)
 	}
 
-	// Store tool calls as messages
+  /* Store tool calls as messages */
 	for _, call := range toolCalls {
 		callJSON, _ := json.Marshal(call.Arguments)
 		toolCallID := call.ID
@@ -287,7 +300,7 @@ func (r *Runtime) storeMessages(ctx context.Context, sessionID uuid.UUID, userMs
 		}
 	}
 
-	// Store tool results
+  /* Store tool results */
 	for _, result := range toolResults {
 		toolName := result.ToolCallID
 		toolCallID := result.ToolCallID
@@ -304,7 +317,7 @@ func (r *Runtime) storeMessages(ctx context.Context, sessionID uuid.UUID, userMs
 		}
 	}
 
-	// Store assistant message
+  /* Store assistant message */
 	assistantTokens := EstimateTokens(assistantMsg)
 	if _, err := r.queries.CreateMessage(ctx, &db.Message{
 		SessionID:  sessionID,
@@ -319,7 +332,7 @@ func (r *Runtime) storeMessages(ctx context.Context, sessionID uuid.UUID, userMs
 	return nil
 }
 
-// Helper function to check if a string is in an array
+/* Helper function to check if a string is in an array */
 func contains(arr pq.StringArray, s string) bool {
 	for _, item := range arr {
 		if item == s {
diff --git a/NeuronAgent/internal/agent/token_counter.go b/NeuronAgent/internal/agent/token_counter.go
index 9f137e7..e570398 100644
--- a/NeuronAgent/internal/agent/token_counter.go
+++ b/NeuronAgent/internal/agent/token_counter.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * token_counter.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/agent/token_counter.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package agent
 
 import (
@@ -5,25 +18,25 @@ import (
 	"unicode/utf8"
 )
 
-// EstimateTokens estimates token count for text (rough approximation)
-// For GPT models, ~4 characters = 1 token, but this varies
+/* EstimateTokens estimates token count for text (rough approximation) */
+/* For GPT models, ~4 characters = 1 token, but this varies */
 func EstimateTokens(text string) int {
-	// Simple approximation: count words and add some overhead
+  /* Simple approximation: count words and add some overhead */
 	words := strings.Fields(text)
 	baseTokens := len(words)
 	
-	// Add tokens for punctuation and special characters
+  /* Add tokens for punctuation and special characters */
 	charCount := utf8.RuneCountInString(text)
 	charTokens := charCount / 4
 	
-	// Use the larger estimate
+  /* Use the larger estimate */
 	if charTokens > baseTokens {
 		return charTokens
 	}
 	return baseTokens
 }
 
-// CountTokensInMessages counts total tokens in messages
+/* CountTokensInMessages counts total tokens in messages */
 func CountTokensInMessages(messages []interface{}) int {
 	total := 0
 	for _, msg := range messages {
@@ -38,14 +51,14 @@ func CountTokensInMessages(messages []interface{}) int {
 	return total
 }
 
-// TruncateToMaxTokens truncates text to fit within max tokens
+/* TruncateToMaxTokens truncates text to fit within max tokens */
 func TruncateToMaxTokens(text string, maxTokens int) string {
 	tokens := EstimateTokens(text)
 	if tokens <= maxTokens {
 		return text
 	}
 	
-	// Rough truncation - remove from end
+  /* Rough truncation - remove from end */
 	charsPerToken := len(text) / tokens
 	maxChars := maxTokens * charsPerToken
 	
@@ -53,9 +66,9 @@ func TruncateToMaxTokens(text string, maxTokens int) string {
 		return text
 	}
 	
-	// Truncate and add ellipsis
+  /* Truncate and add ellipsis */
 	truncated := text[:maxChars]
-	// Try to cut at word boundary
+  /* Try to cut at word boundary */
 	if lastSpace := strings.LastIndex(truncated, " "); lastSpace > maxChars*3/4 {
 		truncated = truncated[:lastSpace]
 	}
diff --git a/NeuronAgent/internal/agent/tool_parser.go b/NeuronAgent/internal/agent/tool_parser.go
index 588c017..bad3c0b 100644
--- a/NeuronAgent/internal/agent/tool_parser.go
+++ b/NeuronAgent/internal/agent/tool_parser.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * tool_parser.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/agent/tool_parser.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package agent
 
 import (
@@ -7,23 +20,23 @@ import (
 	"strings"
 )
 
-// ParseToolCalls extracts tool calls from LLM response
-// Supports OpenAI format and custom formats
+/* ParseToolCalls extracts tool calls from LLM response */
+/* Supports OpenAI format and custom formats */
 func ParseToolCalls(response string) ([]ToolCall, error) {
-	// Try OpenAI JSON format first
+  /* Try OpenAI JSON format first */
 	if strings.Contains(response, "tool_calls") || strings.Contains(response, "function") {
 		return parseOpenAIFormat(response)
 	}
 
-	// Try custom format: <tool:name:args>
+  /* Try custom format: <tool:name:args> */
 	return parseCustomFormat(response)
 }
 
 func parseOpenAIFormat(response string) ([]ToolCall, error) {
-	// Look for JSON structure with tool_calls
+  /* Look for JSON structure with tool_calls */
 	var toolCalls []ToolCall
 
-	// Try to find JSON object with tool_calls
+  /* Try to find JSON object with tool_calls */
 	jsonRegex := regexp.MustCompile(`\{[^{}]*"tool_calls"[^{}]*\}`)
 	matches := jsonRegex.FindAllString(response, -1)
 
@@ -57,7 +70,7 @@ func parseOpenAIFormat(response string) ([]ToolCall, error) {
 }
 
 func parseCustomFormat(response string) ([]ToolCall, error) {
-	// Custom format: <tool:name:{"arg":"value"}>
+  /* Custom format: <tool:name:{"arg":"value"}> */
 	pattern := regexp.MustCompile(`<tool:([^:]+):([^>]+)>`)
 	matches := pattern.FindAllStringSubmatch(response, -1)
 
@@ -72,7 +85,7 @@ func parseCustomFormat(response string) ([]ToolCall, error) {
 
 		var args map[string]interface{}
 		if err := json.Unmarshal([]byte(argsStr), &args); err != nil {
-			// If not JSON, create a simple map
+    /* If not JSON, create a simple map */
 			args = map[string]interface{}{
 				"input": argsStr,
 			}
diff --git a/NeuronAgent/internal/api/errors.go b/NeuronAgent/internal/api/errors.go
index 745fd25..d58e866 100644
--- a/NeuronAgent/internal/api/errors.go
+++ b/NeuronAgent/internal/api/errors.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * errors.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/api/errors.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package api
 
 import (
@@ -87,7 +100,7 @@ var (
 	ErrInternal     = NewError(http.StatusInternalServerError, "internal server error", nil)
 )
 
-// WrapError wraps an error with request ID
+/* WrapError wraps an error with request ID */
 func WrapError(err *APIError, requestID string) *APIError {
 	if err == nil {
 		return nil
diff --git a/NeuronAgent/internal/api/handlers.go b/NeuronAgent/internal/api/handlers.go
index 8f6a52d..6f57ca7 100644
--- a/NeuronAgent/internal/api/handlers.go
+++ b/NeuronAgent/internal/api/handlers.go
@@ -1,3 +1,18 @@
+/*-------------------------------------------------------------------------
+ *
+ * handlers.go
+ *    API handlers for NeuronAgent
+ *
+ * Provides HTTP handlers for agents, sessions, messages, and other API endpoints.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/api/handlers.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package api
 
 import (
@@ -27,7 +42,7 @@ func NewHandlers(queries *db.Queries, runtime *agent.Runtime) *Handlers {
 	}
 }
 
-// Agents
+/* Agents */
 
 func (h *Handlers) CreateAgent(w http.ResponseWriter, r *http.Request) {
 	requestID := GetRequestID(r.Context())
@@ -46,7 +61,7 @@ func (h *Handlers) CreateAgent(w http.ResponseWriter, r *http.Request) {
 		return
 	}
 
-	// Validate request
+  /* Validate request */
 	if !ValidateAndRespond(w, func() error { return ValidateCreateAgentRequest(&req) }) {
 		return
 	}
@@ -125,7 +140,7 @@ func (h *Handlers) UpdateAgent(w http.ResponseWriter, r *http.Request) {
 		return
 	}
 
-	// Validate request
+  /* Validate request */
 	if !ValidateAndRespond(w, func() error { return ValidateCreateAgentRequest(&req) }) {
 		return
 	}
@@ -137,7 +152,7 @@ func (h *Handlers) UpdateAgent(w http.ResponseWriter, r *http.Request) {
 		return
 	}
 
-	// Update fields
+  /* Update fields */
 	agent.Name = req.Name
 	agent.Description = req.Description
 	agent.SystemPrompt = req.SystemPrompt
@@ -173,7 +188,7 @@ func (h *Handlers) DeleteAgent(w http.ResponseWriter, r *http.Request) {
 	w.WriteHeader(http.StatusNoContent)
 }
 
-// Sessions
+/* Sessions */
 
 func (h *Handlers) CreateSession(w http.ResponseWriter, r *http.Request) {
 	var req CreateSessionRequest
@@ -183,7 +198,7 @@ func (h *Handlers) CreateSession(w http.ResponseWriter, r *http.Request) {
 		return
 	}
 
-	// Validate request
+  /* Validate request */
 	if !ValidateAndRespond(w, func() error { return ValidateCreateSessionRequest(&req) }) {
 		return
 	}
@@ -237,7 +252,7 @@ func (h *Handlers) ListSessions(w http.ResponseWriter, r *http.Request) {
 
 	limit := 50
 	offset := 0
-	// Parse query parameters for pagination
+  /* Parse query parameters for pagination */
 	if l := r.URL.Query().Get("limit"); l != "" {
 		fmt.Sscanf(l, "%d", &limit)
 	}
@@ -260,7 +275,7 @@ func (h *Handlers) ListSessions(w http.ResponseWriter, r *http.Request) {
 	respondJSON(w, http.StatusOK, responses)
 }
 
-// Messages
+/* Messages */
 
 func (h *Handlers) SendMessage(w http.ResponseWriter, r *http.Request) {
 	start := time.Now()
@@ -279,12 +294,12 @@ func (h *Handlers) SendMessage(w http.ResponseWriter, r *http.Request) {
 		return
 	}
 
-	// Validate request
+  /* Validate request */
 	if !ValidateAndRespond(w, func() error { return ValidateSendMessageRequest(&req) }) {
 		return
 	}
 
-	// Check if streaming is requested
+  /* Check if streaming is requested */
 	if req.Stream {
 		StreamResponse(w, r, h.runtime, sessionID.String(), req.Content)
 		return
@@ -298,7 +313,7 @@ func (h *Handlers) SendMessage(w http.ResponseWriter, r *http.Request) {
 		return
 	}
 
-	// Record metrics
+  /* Record metrics */
 	duration := time.Since(start)
 	metrics.RecordAgentExecution(state.AgentID.String(), "success", duration)
 
@@ -325,7 +340,7 @@ func (h *Handlers) GetMessages(w http.ResponseWriter, r *http.Request) {
 
 	limit := 100
 	offset := 0
-	// Parse query parameters
+  /* Parse query parameters */
 	if l := r.URL.Query().Get("limit"); l != "" {
 		_, _ = fmt.Sscanf(l, "%d", &limit)
 	}
@@ -348,7 +363,7 @@ func (h *Handlers) GetMessages(w http.ResponseWriter, r *http.Request) {
 	respondJSON(w, http.StatusOK, responses)
 }
 
-// Helper functions
+/* Helper functions */
 
 func toAgentResponse(a *db.Agent) AgentResponse {
 	return AgentResponse{
diff --git a/NeuronAgent/internal/api/middleware.go b/NeuronAgent/internal/api/middleware.go
index 245b562..03a31fe 100644
--- a/NeuronAgent/internal/api/middleware.go
+++ b/NeuronAgent/internal/api/middleware.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * middleware.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/api/middleware.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package api
 
 import (
@@ -15,17 +28,17 @@ type contextKey string
 
 const apiKeyContextKey contextKey = "api_key"
 
-// AuthMiddleware authenticates requests using API keys
+/* AuthMiddleware authenticates requests using API keys */
 func AuthMiddleware(keyManager *auth.APIKeyManager, rateLimiter *auth.RateLimiter) func(http.Handler) http.Handler {
 	return func(next http.Handler) http.Handler {
 		return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
-			// Skip auth for health and metrics endpoints
+    /* Skip auth for health and metrics endpoints */
 			if r.URL.Path == "/health" || r.URL.Path == "/metrics" {
 				next.ServeHTTP(w, r)
 				return
 			}
 
-			// Get API key from header
+    /* Get API key from header */
 			authHeader := r.Header.Get("Authorization")
 			if authHeader == "" {
 				requestID := GetRequestID(r.Context())
@@ -33,7 +46,7 @@ func AuthMiddleware(keyManager *auth.APIKeyManager, rateLimiter *auth.RateLimite
 				return
 			}
 
-			// Extract key (format: "Bearer <key>" or "ApiKey <key>")
+    /* Extract key (format: "Bearer <key>" or "ApiKey <key>") */
 			parts := strings.Fields(authHeader)
 			if len(parts) != 2 {
 				requestID := GetRequestID(r.Context())
@@ -48,11 +61,11 @@ func AuthMiddleware(keyManager *auth.APIKeyManager, rateLimiter *auth.RateLimite
 			}
 			fmt.Printf("[MIDDLEWARE] Extracted key: prefix=%s, len=%d\n", keyPrefix, len(key))
 
-			// Validate key
+    /* Validate key */
 			apiKey, err := keyManager.ValidateAPIKey(r.Context(), key)
 			if err != nil {
 				requestID := GetRequestID(r.Context())
-				// Log the actual error for debugging
+     /* Log the actual error for debugging */
 				prefix := key
 				if len(prefix) > 8 {
 					prefix = prefix[:8]
@@ -63,21 +76,21 @@ func AuthMiddleware(keyManager *auth.APIKeyManager, rateLimiter *auth.RateLimite
 			}
 			fmt.Printf("[MIDDLEWARE] Authentication succeeded: prefix=%s\n", apiKey.KeyPrefix)
 
-			// Check rate limit
+    /* Check rate limit */
 			if !rateLimiter.CheckLimit(apiKey.ID.String(), apiKey.RateLimitPerMin) {
 				requestID := GetRequestID(r.Context())
 				respondError(w, WrapError(NewError(http.StatusTooManyRequests, "rate limit exceeded", nil), requestID))
 				return
 			}
 
-			// Add API key to context
+    /* Add API key to context */
 			ctx := context.WithValue(r.Context(), apiKeyContextKey, apiKey)
 			next.ServeHTTP(w, r.WithContext(ctx))
 		})
 	}
 }
 
-// CORSMiddleware adds CORS headers
+/* CORSMiddleware adds CORS headers */
 func CORSMiddleware(next http.Handler) http.Handler {
 	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
 		w.Header().Set("Access-Control-Allow-Origin", "*")
@@ -93,19 +106,19 @@ func CORSMiddleware(next http.Handler) http.Handler {
 	})
 }
 
-// LoggingMiddleware logs requests with structured logging and metrics
+/* LoggingMiddleware logs requests with structured logging and metrics */
 func LoggingMiddleware(next http.Handler) http.Handler {
 	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
 		start := time.Now()
 		
-		// Wrap response writer to capture status code
+   /* Wrap response writer to capture status code */
 		wrapped := &responseWriter{ResponseWriter: w, statusCode: http.StatusOK}
 		
 		next.ServeHTTP(wrapped, r)
 		
 		duration := time.Since(start)
 		
-		// Record metrics
+   /* Record metrics */
 		endpoint := r.URL.Path
 		metrics.RecordHTTPRequest(r.Method, endpoint, wrapped.statusCode, duration)
 	})
diff --git a/NeuronAgent/internal/api/models.go b/NeuronAgent/internal/api/models.go
index 90e1ee8..40881cf 100644
--- a/NeuronAgent/internal/api/models.go
+++ b/NeuronAgent/internal/api/models.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * models.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/api/models.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package api
 
 import (
@@ -6,7 +19,7 @@ import (
 	"github.com/google/uuid"
 )
 
-// Request DTOs
+/* Request DTOs */
 
 type CreateAgentRequest struct {
 	Name         string                 `json:"name"`
@@ -31,7 +44,7 @@ type SendMessageRequest struct {
 	Metadata map[string]interface{} `json:"metadata"`
 }
 
-// Response DTOs
+/* Response DTOs */
 
 type AgentResponse struct {
 	ID           uuid.UUID              `json:"id"`
diff --git a/NeuronAgent/internal/api/request_id.go b/NeuronAgent/internal/api/request_id.go
index 39ad24b..f10402b 100644
--- a/NeuronAgent/internal/api/request_id.go
+++ b/NeuronAgent/internal/api/request_id.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * request_id.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/api/request_id.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package api
 
 import (
@@ -9,7 +22,7 @@ import (
 
 const requestIDKey contextKey = "request_id"
 
-// RequestIDMiddleware adds a unique request ID to each request
+/* RequestIDMiddleware adds a unique request ID to each request */
 func RequestIDMiddleware(next http.Handler) http.Handler {
 	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
 		requestID := r.Header.Get("X-Request-ID")
@@ -17,18 +30,18 @@ func RequestIDMiddleware(next http.Handler) http.Handler {
 			requestID = uuid.New().String()
 		}
 
-		// Add to context
+   /* Add to context */
 		ctx := context.WithValue(r.Context(), requestIDKey, requestID)
 		r = r.WithContext(ctx)
 
-		// Add to response header
+   /* Add to response header */
 		w.Header().Set("X-Request-ID", requestID)
 
 		next.ServeHTTP(w, r)
 	})
 }
 
-// GetRequestID gets the request ID from context
+/* GetRequestID gets the request ID from context */
 func GetRequestID(ctx context.Context) string {
 	if id, ok := ctx.Value(requestIDKey).(string); ok {
 		return id
diff --git a/NeuronAgent/internal/api/streaming.go b/NeuronAgent/internal/api/streaming.go
index 5cf957b..a586cb3 100644
--- a/NeuronAgent/internal/api/streaming.go
+++ b/NeuronAgent/internal/api/streaming.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * streaming.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/api/streaming.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package api
 
 import (
@@ -9,9 +22,9 @@ import (
 	"github.com/neurondb/NeuronAgent/internal/agent"
 )
 
-// StreamResponse streams agent responses chunk by chunk
+/* StreamResponse streams agent responses chunk by chunk */
 func StreamResponse(w http.ResponseWriter, r *http.Request, runtime *agent.Runtime, sessionIDStr string, userMessage string) {
-	// Set headers for streaming
+  /* Set headers for streaming */
 	w.Header().Set("Content-Type", "text/event-stream")
 	w.Header().Set("Cache-Control", "no-cache")
 	w.Header().Set("Connection", "keep-alive")
@@ -23,7 +36,7 @@ func StreamResponse(w http.ResponseWriter, r *http.Request, runtime *agent.Runti
 		return
 	}
 
-	// Parse session ID
+  /* Parse session ID */
 	sessionID, err := uuid.Parse(sessionIDStr)
 	if err != nil {
 		sendSSE(w, flusher, "error", map[string]interface{}{
@@ -32,8 +45,8 @@ func StreamResponse(w http.ResponseWriter, r *http.Request, runtime *agent.Runti
 		return
 	}
 
-	// Execute agent with streaming
-	// Note: This is a simplified version - full implementation would stream LLM output
+  /* Execute agent with streaming */
+  /* Note: This is a simplified version - full implementation would stream LLM output */
 	state, err := runtime.Execute(r.Context(), sessionID, userMessage)
 	if err != nil {
 		sendSSE(w, flusher, "error", map[string]interface{}{
@@ -42,9 +55,9 @@ func StreamResponse(w http.ResponseWriter, r *http.Request, runtime *agent.Runti
 		return
 	}
 
-	// Stream response in chunks
+  /* Stream response in chunks */
 	response := state.FinalAnswer
-	chunkSize := 50 // Characters per chunk
+ 	chunkSize := 50 /* Characters per chunk */
 
 	for i := 0; i < len(response); i += chunkSize {
 		end := i + chunkSize
@@ -57,13 +70,13 @@ func StreamResponse(w http.ResponseWriter, r *http.Request, runtime *agent.Runti
 			"content": chunk,
 		})
 
-		// Check if client disconnected
+   /* Check if client disconnected */
 		if r.Context().Err() != nil {
 			return
 		}
 	}
 
-	// Send completion
+  /* Send completion */
 	sendSSE(w, flusher, "done", map[string]interface{}{
 		"tokens_used":  state.TokensUsed,
 		"tool_calls":   state.ToolCalls,
diff --git a/NeuronAgent/internal/api/validation.go b/NeuronAgent/internal/api/validation.go
index 28cb2a6..da5ede4 100644
--- a/NeuronAgent/internal/api/validation.go
+++ b/NeuronAgent/internal/api/validation.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * validation.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/api/validation.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package api
 
 import (
@@ -7,7 +20,7 @@ import (
 	"github.com/neurondb/NeuronAgent/internal/utils"
 )
 
-// ValidateCreateAgentRequest validates CreateAgentRequest
+/* ValidateCreateAgentRequest validates CreateAgentRequest */
 func ValidateCreateAgentRequest(req *CreateAgentRequest) error {
 	if err := utils.ValidateRequiredWithError(req.Name, "name"); err != nil {
 		return err
@@ -27,13 +40,13 @@ func ValidateCreateAgentRequest(req *CreateAgentRequest) error {
 	return nil
 }
 
-// ValidateCreateSessionRequest validates CreateSessionRequest
+/* ValidateCreateSessionRequest validates CreateSessionRequest */
 func ValidateCreateSessionRequest(req *CreateSessionRequest) error {
-	// AgentID is required (UUID validation happens in handler)
+  /* AgentID is required (UUID validation happens in handler) */
 	return nil
 }
 
-// ValidateSendMessageRequest validates SendMessageRequest
+/* ValidateSendMessageRequest validates SendMessageRequest */
 func ValidateSendMessageRequest(req *SendMessageRequest) error {
 	if err := utils.ValidateRequiredWithError(req.Content, "content"); err != nil {
 		return err
@@ -47,7 +60,7 @@ func ValidateSendMessageRequest(req *SendMessageRequest) error {
 	return nil
 }
 
-// ValidateAndRespond validates a request and responds with error if invalid
+/* ValidateAndRespond validates a request and responds with error if invalid */
 func ValidateAndRespond(w http.ResponseWriter, validator func() error) bool {
 	if err := validator(); err != nil {
 		respondError(w, NewError(http.StatusBadRequest, "validation failed", err))
diff --git a/NeuronAgent/internal/api/websocket.go b/NeuronAgent/internal/api/websocket.go
index 7109378..bde4c77 100644
--- a/NeuronAgent/internal/api/websocket.go
+++ b/NeuronAgent/internal/api/websocket.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * websocket.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/api/websocket.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package api
 
 import (
@@ -10,11 +23,11 @@ import (
 
 var upgrader = websocket.Upgrader{
 	CheckOrigin: func(r *http.Request) bool {
-		return true // Allow all origins in development
+  		return true /* Allow all origins in development */
 	},
 }
 
-// HandleWebSocket handles WebSocket connections for streaming agent responses
+/* HandleWebSocket handles WebSocket connections for streaming agent responses */
 func HandleWebSocket(runtime *agent.Runtime) http.HandlerFunc {
 	return func(w http.ResponseWriter, r *http.Request) {
 		conn, err := upgrader.Upgrade(w, r, nil)
@@ -23,7 +36,7 @@ func HandleWebSocket(runtime *agent.Runtime) http.HandlerFunc {
 		}
 		defer conn.Close()
 
-		// Get session ID from query parameter
+   /* Get session ID from query parameter */
 		sessionIDStr := r.URL.Query().Get("session_id")
 		sessionID, err := uuid.Parse(sessionIDStr)
 		if err != nil {
@@ -31,7 +44,7 @@ func HandleWebSocket(runtime *agent.Runtime) http.HandlerFunc {
 			return
 		}
 
-		// Read messages from client
+   /* Read messages from client */
 		for {
 			var msg map[string]interface{}
 			if err := conn.ReadJSON(&msg); err != nil {
@@ -44,14 +57,14 @@ func HandleWebSocket(runtime *agent.Runtime) http.HandlerFunc {
 				continue
 			}
 
-			// Execute agent
+    /* Execute agent */
 			state, err := runtime.Execute(r.Context(), sessionID, content)
 			if err != nil {
 				conn.WriteJSON(map[string]string{"error": err.Error()})
 				continue
 			}
 
-			// Stream response
+    /* Stream response */
 			response := map[string]interface{}{
 				"type":     "response",
 				"content":  state.FinalAnswer,
diff --git a/NeuronAgent/internal/auth/api_key.go b/NeuronAgent/internal/auth/api_key.go
index 1e882a3..a8633be 100644
--- a/NeuronAgent/internal/auth/api_key.go
+++ b/NeuronAgent/internal/auth/api_key.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * api_key.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/auth/api_key.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package auth
 
 import (
@@ -18,9 +31,9 @@ func NewAPIKeyManager(queries *db.Queries) *APIKeyManager {
 	return &APIKeyManager{queries: queries}
 }
 
-// GenerateAPIKey generates a new API key
+/* GenerateAPIKey generates a new API key */
 func (m *APIKeyManager) GenerateAPIKey(ctx context.Context, organizationID, userID *string, rateLimit int, roles []string) (string, *db.APIKey, error) {
-	// Generate random key (32 bytes = 44 base64 chars)
+  /* Generate random key (32 bytes = 44 base64 chars) */
 	keyBytes := make([]byte, 32)
 	if _, err := rand.Read(keyBytes); err != nil {
 		return "", nil, fmt.Errorf("failed to generate key: %w", err)
@@ -40,7 +53,7 @@ func (m *APIKeyManager) GenerateAPIKey(ctx context.Context, organizationID, user
 		UserID:          userID,
 		RateLimitPerMin: rateLimit,
 		Roles:           roles,
-		Metadata:        make(db.JSONBMap), // Initialize empty metadata
+  		Metadata:        make(db.JSONBMap), /* Initialize empty metadata */
 	}
 
 	if err := m.queries.CreateAPIKey(ctx, apiKey); err != nil {
@@ -50,12 +63,12 @@ func (m *APIKeyManager) GenerateAPIKey(ctx context.Context, organizationID, user
 	return key, apiKey, nil
 }
 
-// ValidateAPIKey validates an API key and returns the key record
+/* ValidateAPIKey validates an API key and returns the key record */
 func (m *APIKeyManager) ValidateAPIKey(ctx context.Context, key string) (*db.APIKey, error) {
 	prefix := GetKeyPrefix(key)
 	fmt.Printf("[AUTH] ValidateAPIKey: prefix=%s, key_len=%d\n", prefix, len(key))
 
-	// Find key by prefix
+  /* Find key by prefix */
 	apiKey, err := m.queries.GetAPIKeyByPrefix(ctx, prefix)
 	if err != nil {
 		fmt.Printf("[AUTH] GetAPIKeyByPrefix failed: prefix=%s, error=%v\n", prefix, err)
@@ -63,20 +76,20 @@ func (m *APIKeyManager) ValidateAPIKey(ctx context.Context, key string) (*db.API
 	}
 	fmt.Printf("[AUTH] GetAPIKeyByPrefix succeeded: prefix=%s, hash=%s\n", apiKey.KeyPrefix, apiKey.KeyHash[:30])
 
-	// Verify key
+  /* Verify key */
 	if !VerifyAPIKey(key, apiKey.KeyHash) {
 		fmt.Printf("[AUTH] Key verification failed: prefix=%s\n", prefix)
 		return nil, fmt.Errorf("invalid API key: key verification failed")
 	}
 	fmt.Printf("[AUTH] Key verification succeeded: prefix=%s\n", prefix)
 
-	// Update last used
+  /* Update last used */
 	_ = m.queries.UpdateAPIKeyLastUsed(ctx, apiKey.ID)
 
 	return apiKey, nil
 }
 
-// DeleteAPIKey deletes an API key
+/* DeleteAPIKey deletes an API key */
 func (m *APIKeyManager) DeleteAPIKey(ctx context.Context, id uuid.UUID) error {
 	return m.queries.DeleteAPIKey(ctx, id)
 }
diff --git a/NeuronAgent/internal/auth/hasher.go b/NeuronAgent/internal/auth/hasher.go
index d183c0e..258f9ed 100644
--- a/NeuronAgent/internal/auth/hasher.go
+++ b/NeuronAgent/internal/auth/hasher.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * hasher.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/auth/hasher.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package auth
 
 import (
@@ -6,7 +19,7 @@ import (
 
 const bcryptCost = 12
 
-// HashAPIKey hashes an API key using bcrypt
+/* HashAPIKey hashes an API key using bcrypt */
 func HashAPIKey(key string) (string, error) {
 	hash, err := bcrypt.GenerateFromPassword([]byte(key), bcryptCost)
 	if err != nil {
@@ -15,13 +28,13 @@ func HashAPIKey(key string) (string, error) {
 	return string(hash), nil
 }
 
-// VerifyAPIKey verifies an API key against its hash
+/* VerifyAPIKey verifies an API key against its hash */
 func VerifyAPIKey(key, hash string) bool {
 	err := bcrypt.CompareHashAndPassword([]byte(hash), []byte(key))
 	return err == nil
 }
 
-// GetKeyPrefix returns the first 8 characters of a key for identification
+/* GetKeyPrefix returns the first 8 characters of a key for identification */
 func GetKeyPrefix(key string) string {
 	if len(key) < 8 {
 		return key
diff --git a/NeuronAgent/internal/auth/roles.go b/NeuronAgent/internal/auth/roles.go
index c36ceda..aaa4082 100644
--- a/NeuronAgent/internal/auth/roles.go
+++ b/NeuronAgent/internal/auth/roles.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * roles.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/auth/roles.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package auth
 
 import (
@@ -12,7 +25,7 @@ const (
 	RoleReadOnly = "read-only"
 )
 
-// HasRole checks if an API key has a specific role
+/* HasRole checks if an API key has a specific role */
 func HasRole(apiKey *db.APIKey, role string) bool {
 	for _, r := range apiKey.Roles {
 		if r == role {
@@ -22,7 +35,7 @@ func HasRole(apiKey *db.APIKey, role string) bool {
 	return false
 }
 
-// RequireRole checks if an API key has required role, returns error if not
+/* RequireRole checks if an API key has required role, returns error if not */
 func RequireRole(apiKey *db.APIKey, role string) error {
 	if !HasRole(apiKey, role) {
 		return fmt.Errorf("insufficient permissions: role %s required", role)
@@ -30,7 +43,7 @@ func RequireRole(apiKey *db.APIKey, role string) error {
 	return nil
 }
 
-// RequireAnyRole checks if an API key has any of the required roles
+/* RequireAnyRole checks if an API key has any of the required roles */
 func RequireAnyRole(apiKey *db.APIKey, roles ...string) error {
 	for _, role := range roles {
 		if HasRole(apiKey, role) {
@@ -40,7 +53,7 @@ func RequireAnyRole(apiKey *db.APIKey, roles ...string) error {
 	return fmt.Errorf("insufficient permissions: one of roles %v required", roles)
 }
 
-// RequireAllRoles checks if an API key has all of the required roles
+/* RequireAllRoles checks if an API key has all of the required roles */
 func RequireAllRoles(apiKey *db.APIKey, roles ...string) error {
 	for _, role := range roles {
 		if !HasRole(apiKey, role) {
@@ -50,22 +63,22 @@ func RequireAllRoles(apiKey *db.APIKey, roles ...string) error {
 	return nil
 }
 
-// CanCreate checks if API key can create resources
+/* CanCreate checks if API key can create resources */
 func CanCreate(apiKey *db.APIKey) bool {
 	return HasRole(apiKey, RoleAdmin) || HasRole(apiKey, RoleUser)
 }
 
-// CanUpdate checks if API key can update resources
+/* CanUpdate checks if API key can update resources */
 func CanUpdate(apiKey *db.APIKey) bool {
 	return HasRole(apiKey, RoleAdmin) || HasRole(apiKey, RoleUser)
 }
 
-// CanDelete checks if API key can delete resources
+/* CanDelete checks if API key can delete resources */
 func CanDelete(apiKey *db.APIKey) bool {
 	return HasRole(apiKey, RoleAdmin)
 }
 
-// CanRead checks if API key can read resources
+/* CanRead checks if API key can read resources */
 func CanRead(apiKey *db.APIKey) bool {
 	return HasRole(apiKey, RoleAdmin) || HasRole(apiKey, RoleUser) || HasRole(apiKey, RoleReadOnly)
 }
diff --git a/NeuronAgent/internal/auth/validator.go b/NeuronAgent/internal/auth/validator.go
index 65feef4..f9873f7 100644
--- a/NeuronAgent/internal/auth/validator.go
+++ b/NeuronAgent/internal/auth/validator.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * validator.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/auth/validator.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package auth
 
 import (
@@ -29,7 +42,7 @@ func (r *RateLimiter) CheckLimit(keyID string, limitPerMin int) bool {
 	rl, exists := r.limits[keyID]
 
 	if !exists || now.After(rl.resetTime) {
-		// Reset or create
+   /* Reset or create */
 		r.limits[keyID] = &rateLimit{
 			count:     1,
 			resetTime: now.Add(1 * time.Minute),
@@ -45,5 +58,5 @@ func (r *RateLimiter) CheckLimit(keyID string, limitPerMin int) bool {
 	return true
 }
 
-// HasRole and RequireRole are now in roles.go
+/* HasRole and RequireRole are now in roles.go */
 
diff --git a/NeuronAgent/internal/config/config.go b/NeuronAgent/internal/config/config.go
index 3d4d877..3ef2a1e 100644
--- a/NeuronAgent/internal/config/config.go
+++ b/NeuronAgent/internal/config/config.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * config.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/config/config.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package config
 
 import (
@@ -54,7 +67,7 @@ func LoadConfig(path string) (*Config, error) {
 		return nil, fmt.Errorf("failed to parse config: %w", err)
 	}
 
-	// Override with environment variables
+  /* Override with environment variables */
 	if err := LoadFromEnv(&config); err != nil {
 		return nil, fmt.Errorf("failed to load from env: %w", err)
 	}
@@ -63,5 +76,5 @@ func LoadConfig(path string) (*Config, error) {
 }
 
 
-// DefaultConfig is now in defaults.go
+/* DefaultConfig is now in defaults.go */
 
diff --git a/NeuronAgent/internal/config/defaults.go b/NeuronAgent/internal/config/defaults.go
index 9cb7aeb..71fa749 100644
--- a/NeuronAgent/internal/config/defaults.go
+++ b/NeuronAgent/internal/config/defaults.go
@@ -1,10 +1,23 @@
+/*-------------------------------------------------------------------------
+ *
+ * defaults.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/config/defaults.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package config
 
 import (
 	"time"
 )
 
-// DefaultConfig returns a configuration with sensible defaults
+/* DefaultConfig returns a configuration with sensible defaults */
 func DefaultConfig() *Config {
 	return &Config{
 		Server: ServerConfig{
@@ -34,7 +47,7 @@ func DefaultConfig() *Config {
 	}
 }
 
-// DevelopmentConfig returns a configuration optimized for development
+/* DevelopmentConfig returns a configuration optimized for development */
 func DevelopmentConfig() *Config {
 	cfg := DefaultConfig()
 	cfg.Logging.Level = "debug"
@@ -44,7 +57,7 @@ func DevelopmentConfig() *Config {
 	return cfg
 }
 
-// ProductionConfig returns a configuration optimized for production
+/* ProductionConfig returns a configuration optimized for production */
 func ProductionConfig() *Config {
 	cfg := DefaultConfig()
 	cfg.Logging.Level = "warn"
diff --git a/NeuronAgent/internal/config/env.go b/NeuronAgent/internal/config/env.go
index 031a90e..3a42c58 100644
--- a/NeuronAgent/internal/config/env.go
+++ b/NeuronAgent/internal/config/env.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * env.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/config/env.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package config
 
 import (
@@ -7,9 +20,9 @@ import (
 	"time"
 )
 
-// LoadFromEnv loads configuration from environment variables
+/* LoadFromEnv loads configuration from environment variables */
 func LoadFromEnv(cfg *Config) error {
-	// Server config
+  /* Server config */
 	if host := os.Getenv("SERVER_HOST"); host != "" {
 		cfg.Server.Host = host
 	}
@@ -29,7 +42,7 @@ func LoadFromEnv(cfg *Config) error {
 		}
 	}
 
-	// Database config
+  /* Database config */
 	if host := os.Getenv("DB_HOST"); host != "" {
 		cfg.Database.Host = host
 	}
@@ -63,12 +76,12 @@ func LoadFromEnv(cfg *Config) error {
 		}
 	}
 
-	// Auth config
+  /* Auth config */
 	if header := os.Getenv("AUTH_API_KEY_HEADER"); header != "" {
 		cfg.Auth.APIKeyHeader = header
 	}
 
-	// Logging config
+  /* Logging config */
 	if level := os.Getenv("LOG_LEVEL"); level != "" {
 		cfg.Logging.Level = level
 	}
@@ -79,7 +92,7 @@ func LoadFromEnv(cfg *Config) error {
 	return nil
 }
 
-// GetEnvOrDefault gets environment variable or returns default
+/* GetEnvOrDefault gets environment variable or returns default */
 func GetEnvOrDefault(key, defaultValue string) string {
 	if value := os.Getenv(key); value != "" {
 		return value
@@ -87,7 +100,7 @@ func GetEnvOrDefault(key, defaultValue string) string {
 	return defaultValue
 }
 
-// GetEnvIntOrDefault gets environment variable as int or returns default
+/* GetEnvIntOrDefault gets environment variable as int or returns default */
 func GetEnvIntOrDefault(key string, defaultValue int) int {
 	if value := os.Getenv(key); value != "" {
 		if n, err := strconv.Atoi(value); err == nil {
@@ -97,7 +110,7 @@ func GetEnvIntOrDefault(key string, defaultValue int) int {
 	return defaultValue
 }
 
-// GetEnvDurationOrDefault gets environment variable as duration or returns default
+/* GetEnvDurationOrDefault gets environment variable as duration or returns default */
 func GetEnvDurationOrDefault(key string, defaultValue time.Duration) time.Duration {
 	if value := os.Getenv(key); value != "" {
 		if d, err := time.ParseDuration(value); err == nil {
@@ -107,7 +120,7 @@ func GetEnvDurationOrDefault(key string, defaultValue time.Duration) time.Durati
 	return defaultValue
 }
 
-// ValidateEnv validates required environment variables
+/* ValidateEnv validates required environment variables */
 func ValidateEnv() error {
 	required := []string{"DB_HOST", "DB_NAME", "DB_USER", "DB_PASSWORD"}
 	for _, key := range required {
diff --git a/NeuronAgent/internal/db/connection.go b/NeuronAgent/internal/db/connection.go
index 2273f24..f09facb 100644
--- a/NeuronAgent/internal/db/connection.go
+++ b/NeuronAgent/internal/db/connection.go
@@ -1,3 +1,19 @@
+/*-------------------------------------------------------------------------
+ *
+ * connection.go
+ *    Database connection management for NeuronAgent
+ *
+ * Provides PostgreSQL connection pooling, retry logic, and connection
+ * management with health checks.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/db/connection.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package db
 
 import (
@@ -11,7 +27,7 @@ import (
 	"github.com/neurondb/NeuronAgent/internal/utils"
 )
 
-// ConnectionInfo holds details about the database connection
+/* ConnectionInfo holds details about the database connection */
 type ConnectionInfo struct {
 	Host     string
 	Port     int
@@ -19,11 +35,11 @@ type ConnectionInfo struct {
 	User     string
 }
 
-// DB manages PostgreSQL connections
+/* DB manages PostgreSQL connections */
 type DB struct {
 	*sqlx.DB
 	poolConfig PoolConfig
-	connInfo   *ConnectionInfo // Stores connection details for error messages
+	connInfo   *ConnectionInfo
 }
 
 type PoolConfig struct {
@@ -33,14 +49,13 @@ type PoolConfig struct {
 	ConnMaxIdleTime time.Duration
 }
 
-// NewDB creates a new database instance
+/* NewDB creates a new database instance */
 func NewDB(connStr string, poolConfig PoolConfig) (*DB, error) {
 	return NewDBWithRetry(connStr, poolConfig, 3, 2*time.Second)
 }
 
-// NewDBWithRetry creates a new database instance with retry logic
+/* NewDBWithRetry creates a new database instance with retry logic */
 func NewDBWithRetry(connStr string, poolConfig PoolConfig, maxRetries int, retryDelay time.Duration) (*DB, error) {
-	// Parse connection string to extract connection info
 	connInfo := parseConnectionInfo(connStr)
 	
 	var db *sqlx.DB
@@ -49,7 +64,6 @@ func NewDBWithRetry(connStr string, poolConfig PoolConfig, maxRetries int, retry
 	for attempt := 0; attempt < maxRetries; attempt++ {
 		db, err = sqlx.Connect("postgres", connStr)
 		if err == nil {
-			// Test the connection
 			ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
 			pingErr := db.PingContext(ctx)
 			cancel()
@@ -71,7 +85,7 @@ func NewDBWithRetry(connStr string, poolConfig PoolConfig, maxRetries int, retry
 		
 		if attempt < maxRetries-1 {
 			time.Sleep(retryDelay)
-			retryDelay *= 2 // Exponential backoff
+			retryDelay *= 2
 		}
 	}
 	
@@ -79,7 +93,7 @@ func NewDBWithRetry(connStr string, poolConfig PoolConfig, maxRetries int, retry
 	return nil, fmt.Errorf("failed to connect to %s after %d attempts (last error: %w)", connInfoStr, maxRetries, err)
 }
 
-// parseConnectionInfo extracts connection information from connection string
+/* parseConnectionInfo extracts connection information from connection string */
 func parseConnectionInfo(connStr string) *ConnectionInfo {
 	info := &ConnectionInfo{
 		Host:     "unknown",
@@ -88,7 +102,6 @@ func parseConnectionInfo(connStr string) *ConnectionInfo {
 		User:     "unknown",
 	}
 	
-	// Simple parsing - in production, use proper connection string parser
 	parts := strings.Split(connStr, " ")
 	for _, part := range parts {
 		if strings.HasPrefix(part, "host=") {
@@ -105,7 +118,7 @@ func parseConnectionInfo(connStr string) *ConnectionInfo {
 	return info
 }
 
-// GetConnInfoString returns a formatted string of connection details
+/* GetConnInfoString returns a formatted string of connection details */
 func (d *DB) GetConnInfoString() string {
 	if d.connInfo == nil {
 		return "unknown database connection"
@@ -113,7 +126,7 @@ func (d *DB) GetConnInfoString() string {
 	return utils.FormatConnectionInfo(d.connInfo.Host, d.connInfo.Port, d.connInfo.Database, d.connInfo.User)
 }
 
-// HealthCheck tests the database connection
+/* HealthCheck tests the database connection */
 func (d *DB) HealthCheck(ctx context.Context) error {
 	if d.DB == nil {
 		return fmt.Errorf("database connection not established: %s (connection pool is nil, ensure NewDB() was called successfully)", d.GetConnInfoString())
@@ -127,7 +140,7 @@ func (d *DB) HealthCheck(ctx context.Context) error {
 	return nil
 }
 
-// Close closes the connection pool
+/* Close closes the connection pool */
 func (d *DB) Close() error {
 	if d.DB == nil {
 		return nil
diff --git a/NeuronAgent/internal/db/jsonb_scanner.go b/NeuronAgent/internal/db/jsonb_scanner.go
index 24b3e5f..e55950d 100644
--- a/NeuronAgent/internal/db/jsonb_scanner.go
+++ b/NeuronAgent/internal/db/jsonb_scanner.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * jsonb_scanner.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/db/jsonb_scanner.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package db
 
 import (
@@ -5,12 +18,12 @@ import (
 	"encoding/json"
 )
 
-// JSONBMap is a custom type for scanning JSONB into map[string]interface{}
+/* JSONBMap is a custom type for scanning JSONB into map[string]interface{} */
 type JSONBMap map[string]interface{}
 
-// Scan implements the sql.Scanner interface for JSONBMap
+/* Scan implements the sql.Scanner interface for JSONBMap */
 func (m *JSONBMap) Scan(value interface{}) error {
-	// Always initialize to empty map first
+  /* Always initialize to empty map first */
 	*m = make(JSONBMap)
 	
 	if value == nil {
@@ -24,7 +37,7 @@ func (m *JSONBMap) Scan(value interface{}) error {
 	case string:
 		bytes = []byte(v)
 	default:
-		// For unknown types, just return empty map (don't error)
+   /* For unknown types, just return empty map (don't error) */
 		return nil
 	}
 
@@ -34,8 +47,8 @@ func (m *JSONBMap) Scan(value interface{}) error {
 
 	var result map[string]interface{}
 	if err := json.Unmarshal(bytes, &result); err != nil {
-		// If unmarshal fails, return empty map instead of error
-		// This handles cases where the JSONB might be malformed
+   /* If unmarshal fails, return empty map instead of error */
+   /* This handles cases where the JSONB might be malformed */
 		return nil
 	}
 
@@ -43,7 +56,7 @@ func (m *JSONBMap) Scan(value interface{}) error {
 	return nil
 }
 
-// Value implements the driver.Valuer interface for JSONBMap
+/* Value implements the driver.Valuer interface for JSONBMap */
 func (m JSONBMap) Value() (driver.Value, error) {
 	if m == nil || len(m) == 0 {
 		return "{}", nil
@@ -51,7 +64,7 @@ func (m JSONBMap) Value() (driver.Value, error) {
 	return json.Marshal(m)
 }
 
-// ToMap converts JSONBMap to map[string]interface{}
+/* ToMap converts JSONBMap to map[string]interface{} */
 func (m JSONBMap) ToMap() map[string]interface{} {
 	if m == nil {
 		return make(map[string]interface{})
@@ -59,7 +72,7 @@ func (m JSONBMap) ToMap() map[string]interface{} {
 	return map[string]interface{}(m)
 }
 
-// FromMap creates JSONBMap from map[string]interface{}
+/* FromMap creates JSONBMap from map[string]interface{} */
 func FromMap(m map[string]interface{}) JSONBMap {
 	if m == nil {
 		return make(JSONBMap)
diff --git a/NeuronAgent/internal/db/migrations.go b/NeuronAgent/internal/db/migrations.go
index 2c19ce8..f441cda 100644
--- a/NeuronAgent/internal/db/migrations.go
+++ b/NeuronAgent/internal/db/migrations.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * migrations.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/db/migrations.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package db
 
 import (
@@ -17,7 +30,7 @@ type MigrationRunner struct {
 func NewMigrationRunner(db *sqlx.DB, migrationsDir string) (*MigrationRunner, error) {
 	schemaMgr := NewSchemaManager(db)
 	
-	// Get absolute path
+  /* Get absolute path */
 	absPath, err := filepath.Abs(migrationsDir)
 	if err != nil {
 		return nil, fmt.Errorf("failed to get absolute path: %w", err)
@@ -29,7 +42,7 @@ func NewMigrationRunner(db *sqlx.DB, migrationsDir string) (*MigrationRunner, er
 		migrationsDir: absPath,
 	}
 
-	// Load migrations
+  /* Load migrations */
 	if err := schemaMgr.LoadMigrations(absPath); err != nil {
 		return nil, fmt.Errorf("failed to load migrations: %w", err)
 	}
@@ -37,12 +50,12 @@ func NewMigrationRunner(db *sqlx.DB, migrationsDir string) (*MigrationRunner, er
 	return runner, nil
 }
 
-// Run runs all pending migrations
+/* Run runs all pending migrations */
 func (mr *MigrationRunner) Run(ctx context.Context) error {
 	return mr.schemaMgr.Migrate(ctx)
 }
 
-// Status returns migration status
+/* Status returns migration status */
 func (mr *MigrationRunner) Status(ctx context.Context) (int, int, error) {
 	current, err := mr.schemaMgr.GetCurrentVersion(ctx)
 	if err != nil {
@@ -52,7 +65,7 @@ func (mr *MigrationRunner) Status(ctx context.Context) (int, int, error) {
 	return current, total, nil
 }
 
-// Rollback rolls back the last migration
+/* Rollback rolls back the last migration */
 func (mr *MigrationRunner) Rollback(ctx context.Context) error {
 	return mr.schemaMgr.Rollback(ctx)
 }
diff --git a/NeuronAgent/internal/db/models.go b/NeuronAgent/internal/db/models.go
index 3b0baf0..0792c88 100644
--- a/NeuronAgent/internal/db/models.go
+++ b/NeuronAgent/internal/db/models.go
@@ -1,3 +1,19 @@
+/*-------------------------------------------------------------------------
+ *
+ * models.go
+ *    Database models for NeuronAgent
+ *
+ * Defines data structures for agents, sessions, messages, memory chunks,
+ * tools, jobs, and API keys.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/db/models.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package db
 
 import (
@@ -47,13 +63,13 @@ type MemoryChunk struct {
 	SessionID       *uuid.UUID             `db:"session_id"`
 	MessageID       *int64                 `db:"message_id"`
 	Content         string                 `db:"content"`
-	Embedding       []float32              `db:"embedding"` // Will be converted to/from neurondb_vector
+	Embedding       []float32              `db:"embedding"`
 	ImportanceScore float64                `db:"importance_score"`
 	Metadata        JSONBMap               `db:"metadata"`
 	CreatedAt       time.Time              `db:"created_at"`
 }
 
-// MemoryChunkWithSimilarity includes similarity score from vector search
+/* MemoryChunkWithSimilarity includes similarity score from vector search */
 type MemoryChunkWithSimilarity struct {
 	MemoryChunk
 	Similarity float64 `db:"similarity"`
diff --git a/NeuronAgent/internal/db/queries.go b/NeuronAgent/internal/db/queries.go
index 651013f..52b9b3d 100644
--- a/NeuronAgent/internal/db/queries.go
+++ b/NeuronAgent/internal/db/queries.go
@@ -1,3 +1,19 @@
+/*-------------------------------------------------------------------------
+ *
+ * queries.go
+ *    Database queries for NeuronAgent
+ *
+ * Provides database query functions for agents, sessions, messages,
+ * memory chunks, tools, jobs, and API keys.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/db/queries.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package db
 
 import (
@@ -10,7 +26,7 @@ import (
 	"github.com/neurondb/NeuronAgent/internal/utils"
 )
 
-// Agent queries
+/* Agent queries */
 const (
 	createAgentQuery = `
 		INSERT INTO neurondb_agent.agents 
@@ -32,7 +48,7 @@ const (
 	deleteAgentQuery = `DELETE FROM neurondb_agent.agents WHERE id = $1`
 )
 
-// Session queries
+/* Session queries */
 const (
 	createSessionQuery = `
 		INSERT INTO neurondb_agent.sessions (agent_id, external_user_id, metadata)
@@ -50,7 +66,7 @@ const (
 	deleteSessionQuery = `DELETE FROM neurondb_agent.sessions WHERE id = $1`
 )
 
-// Message queries
+/* Message queries */
 const (
 	createMessageQuery = `
 		INSERT INTO neurondb_agent.messages 
@@ -71,7 +87,7 @@ const (
 		LIMIT $2`
 )
 
-// Memory chunk queries
+/* Memory chunk queries */
 const (
 	createMemoryChunkQuery = `
 		INSERT INTO neurondb_agent.memory_chunks 
@@ -88,7 +104,7 @@ const (
 		LIMIT $3`
 )
 
-// Tool queries
+/* Tool queries */
 const (
 	createToolQuery = `
 		INSERT INTO neurondb_agent.tools 
@@ -110,7 +126,7 @@ const (
 	deleteToolQuery = `DELETE FROM neurondb_agent.tools WHERE name = $1`
 )
 
-// Job queries
+/* Job queries */
 const (
 	createJobQuery = `
 		INSERT INTO neurondb_agent.jobs 
@@ -149,7 +165,7 @@ const (
 		LIMIT $3 OFFSET $4`
 )
 
-// API Key queries
+/* API Key queries */
 const (
 	createAPIKeyQuery = `
 		INSERT INTO neurondb_agent.api_keys 
@@ -174,7 +190,7 @@ const (
 	deleteAPIKeyQuery = `DELETE FROM neurondb_agent.api_keys WHERE id = $1`
 )
 
-// NeuronDB function wrappers
+/* NeuronDB function wrappers */
 const (
 	embedTextQuery   = `SELECT neurondb_embed($1, $2) AS embedding`
 	llmGenerateQuery = `SELECT neurondb_llm_generate($1, $2, $3) AS output`
@@ -182,7 +198,7 @@ const (
 
 type Queries struct {
 	db       *sqlx.DB
-	connInfo func() string // Function to get connection info string
+	connInfo func() string
 }
 
 func NewQueries(db *sqlx.DB) *Queries {
@@ -194,12 +210,12 @@ func NewQueries(db *sqlx.DB) *Queries {
 	}
 }
 
-// SetConnInfoFunc sets a function to retrieve connection info for error messages
+/* SetConnInfoFunc sets a function to retrieve connection info for error messages */
 func (q *Queries) SetConnInfoFunc(fn func() string) {
 	q.connInfo = fn
 }
 
-// getConnInfoString returns connection info string
+/* getConnInfoString returns connection info string */
 func (q *Queries) getConnInfoString() string {
 	if q.connInfo != nil {
 		return q.connInfo()
@@ -207,14 +223,14 @@ func (q *Queries) getConnInfoString() string {
 	return "unknown database connection"
 }
 
-// formatQueryError formats a detailed query error message
+/* formatQueryError formats a detailed query error message */
 func (q *Queries) formatQueryError(operation string, query string, paramCount int, table string, err error) error {
 	queryContext := utils.FormatQueryContext(query, paramCount, operation, table)
 	connInfo := q.getConnInfoString()
 	return fmt.Errorf("query execution failed on %s: %s, error=%w", connInfo, queryContext, err)
 }
 
-// Agent methods
+/* Agent methods */
 func (q *Queries) CreateAgent(ctx context.Context, agent *Agent) error {
 	params := []interface{}{agent.Name, agent.Description, agent.SystemPrompt, agent.ModelName,
 		agent.MemoryTable, agent.EnabledTools, agent.Config}
@@ -274,7 +290,7 @@ func (q *Queries) DeleteAgent(ctx context.Context, id uuid.UUID) error {
 	return nil
 }
 
-// Session methods
+/* Session methods */
 func (q *Queries) CreateSession(ctx context.Context, session *Session) error {
 	params := []interface{}{session.AgentID, session.ExternalUserID, session.Metadata}
 	err := q.db.GetContext(ctx, session, createSessionQuery, params...)
@@ -324,7 +340,7 @@ func (q *Queries) DeleteSession(ctx context.Context, id uuid.UUID) error {
 	return nil
 }
 
-// Message methods
+/* Message methods */
 func (q *Queries) CreateMessage(ctx context.Context, message *Message) (*Message, error) {
 	params := []interface{}{message.SessionID, message.Role, message.Content, message.ToolName,
 		message.ToolCallID, message.TokenCount, message.Metadata}
@@ -355,9 +371,8 @@ func (q *Queries) GetRecentMessages(ctx context.Context, sessionID uuid.UUID, li
 	return messages, nil
 }
 
-// Memory chunk methods
+/* Memory chunk methods */
 func (q *Queries) CreateMemoryChunk(ctx context.Context, chunk *MemoryChunk) (*MemoryChunk, error) {
-	// Convert embedding to string format for neurondb_vector
 	embeddingStr := formatVector(chunk.Embedding)
 	params := []interface{}{chunk.AgentID, chunk.SessionID, chunk.MessageID, chunk.Content,
 		embeddingStr, chunk.ImportanceScore, chunk.Metadata}
@@ -384,7 +399,7 @@ func (q *Queries) SearchMemory(ctx context.Context, agentID uuid.UUID, queryEmbe
 	return chunks, nil
 }
 
-// Tool methods
+/* Tool methods */
 func (q *Queries) CreateTool(ctx context.Context, tool *Tool) error {
 	params := []interface{}{tool.Name, tool.Description, tool.ArgSchema, tool.HandlerType,
 		tool.HandlerConfig, tool.Enabled}
@@ -446,7 +461,7 @@ func (q *Queries) DeleteTool(ctx context.Context, name string) error {
 	return nil
 }
 
-// Job methods
+/* Job methods */
 func (q *Queries) CreateJob(ctx context.Context, job *Job) (*Job, error) {
 	params := []interface{}{job.AgentID, job.SessionID, job.Type, job.Status, job.Priority,
 		job.Payload, job.MaxRetries}
@@ -477,9 +492,9 @@ func (q *Queries) GetJob(ctx context.Context, id int64) (*Job, error) {
 func (q *Queries) ClaimJob(ctx context.Context) (*Job, error) {
 	var job Job
 	err := q.db.GetContext(ctx, &job, claimJobQuery)
-	if err == sql.ErrNoRows {
-		return nil, nil // No jobs available
-	}
+		if err == sql.ErrNoRows {
+			return nil, nil
+		}
 	if err != nil {
 		return nil, q.formatQueryError("UPDATE", claimJobQuery, 0, "neurondb_agent.jobs", err)
 	}
@@ -513,9 +528,8 @@ func (q *Queries) ListJobs(ctx context.Context, agentID *uuid.UUID, sessionID *u
 	return jobs, nil
 }
 
-// API Key methods
+/* API Key methods */
 func (q *Queries) CreateAPIKey(ctx context.Context, apiKey *APIKey) error {
-	// Convert metadata to JSONB-compatible format using JSONBMap.Value()
 	metadataValue, err := apiKey.Metadata.Value()
 	if err != nil {
 		return fmt.Errorf("failed to convert metadata: %w", err)
@@ -540,7 +554,6 @@ func (q *Queries) GetAPIKeyByPrefix(ctx context.Context, prefix string) (*APIKey
 			return nil, fmt.Errorf("API key not found on %s: query='%s', key_prefix='%s', table='neurondb_agent.api_keys', error=%w",
 				q.getConnInfoString(), getAPIKeyByPrefixQuery, prefix, err)
 		}
-		// Return detailed error for debugging
 		return nil, fmt.Errorf("API key lookup failed on %s: query='%s', key_prefix='%s', error=%w (error_type=%T)",
 			q.getConnInfoString(), getAPIKeyByPrefixQuery, prefix, err, err)
 	}
@@ -594,7 +607,7 @@ func (q *Queries) DeleteAPIKey(ctx context.Context, id uuid.UUID) error {
 	return nil
 }
 
-// Helper function to format vector for PostgreSQL
+/* formatVector formats vector for PostgreSQL */
 func formatVector(vec []float32) string {
 	if len(vec) == 0 {
 		return "[]"
diff --git a/NeuronAgent/internal/db/schema.go b/NeuronAgent/internal/db/schema.go
index 9b88393..2c23470 100644
--- a/NeuronAgent/internal/db/schema.go
+++ b/NeuronAgent/internal/db/schema.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * schema.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/db/schema.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package db
 
 import (
@@ -29,7 +42,7 @@ func NewSchemaManager(db *sqlx.DB) *SchemaManager {
 	}
 }
 
-// LoadMigrations loads migrations from directory
+/* LoadMigrations loads migrations from directory */
 func (sm *SchemaManager) LoadMigrations(dir string) error {
 	files, err := os.ReadDir(dir)
 	if err != nil {
@@ -41,7 +54,7 @@ func (sm *SchemaManager) LoadMigrations(dir string) error {
 			continue
 		}
 
-		// Parse version from filename (e.g., "001_initial_schema.sql" -> 1)
+   /* Parse version from filename (e.g., "001_initial_schema.sql" -> 1) */
 		var version int
 		var name string
 		parts := strings.SplitN(strings.TrimSuffix(file.Name(), ".sql"), "_", 2)
@@ -52,7 +65,7 @@ func (sm *SchemaManager) LoadMigrations(dir string) error {
 			name = parts[1]
 		}
 
-		// Read SQL file
+   /* Read SQL file */
 		path := filepath.Join(dir, file.Name())
 		sql, err := os.ReadFile(path)
 		if err != nil {
@@ -66,7 +79,7 @@ func (sm *SchemaManager) LoadMigrations(dir string) error {
 		})
 	}
 
-	// Sort by version
+  /* Sort by version */
 	sort.Slice(sm.migrations, func(i, j int) bool {
 		return sm.migrations[i].Version < sm.migrations[j].Version
 	})
@@ -74,9 +87,9 @@ func (sm *SchemaManager) LoadMigrations(dir string) error {
 	return nil
 }
 
-// GetCurrentVersion gets the current migration version
+/* GetCurrentVersion gets the current migration version */
 func (sm *SchemaManager) GetCurrentVersion(ctx context.Context) (int, error) {
-	// Check if schema_migrations table exists
+  /* Check if schema_migrations table exists */
 	var exists bool
 	err := sm.db.GetContext(ctx, &exists, `
 		SELECT EXISTS (
@@ -101,9 +114,9 @@ func (sm *SchemaManager) GetCurrentVersion(ctx context.Context) (int, error) {
 	return version, nil
 }
 
-// Migrate runs all pending migrations
+/* Migrate runs all pending migrations */
 func (sm *SchemaManager) Migrate(ctx context.Context) error {
-	// Create schema_migrations table if it doesn't exist
+  /* Create schema_migrations table if it doesn't exist */
 	_, err := sm.db.ExecContext(ctx, `
 		CREATE SCHEMA IF NOT EXISTS neurondb_agent;
 		CREATE TABLE IF NOT EXISTS neurondb_agent.schema_migrations (
@@ -121,13 +134,13 @@ func (sm *SchemaManager) Migrate(ctx context.Context) error {
 		return fmt.Errorf("failed to get current version: %w", err)
 	}
 
-	// Run pending migrations
+  /* Run pending migrations */
 	for _, migration := range sm.migrations {
 		if migration.Version <= currentVersion {
 			continue
 		}
 
-		// Run migration in transaction
+   /* Run migration in transaction */
 		tx, err := sm.db.BeginTxx(ctx, nil)
 		if err != nil {
 			return fmt.Errorf("failed to begin transaction: %w", err)
@@ -139,7 +152,7 @@ func (sm *SchemaManager) Migrate(ctx context.Context) error {
 			return fmt.Errorf("failed to run migration %d: %w", migration.Version, err)
 		}
 
-		// Record migration
+   /* Record migration */
 		_, err = tx.ExecContext(ctx, `
 			INSERT INTO neurondb_agent.schema_migrations (version, name)
 			VALUES ($1, $2)
@@ -157,7 +170,7 @@ func (sm *SchemaManager) Migrate(ctx context.Context) error {
 	return nil
 }
 
-// Rollback rolls back the last migration (if supported)
+/* Rollback rolls back the last migration (if supported) */
 func (sm *SchemaManager) Rollback(ctx context.Context) error {
 	currentVersion, err := sm.GetCurrentVersion(ctx)
 	if err != nil {
@@ -168,7 +181,7 @@ func (sm *SchemaManager) Rollback(ctx context.Context) error {
 		return fmt.Errorf("no migrations to rollback")
 	}
 
-	// Find migration to rollback
+  /* Find migration to rollback */
 	var migrationToRollback *Migration
 	for _, m := range sm.migrations {
 		if m.Version == currentVersion {
@@ -181,8 +194,8 @@ func (sm *SchemaManager) Rollback(ctx context.Context) error {
 		return fmt.Errorf("migration version %d not found", currentVersion)
 	}
 
-	// Note: Full rollback requires storing rollback SQL
-	// For now, we just remove the version record
+  /* Note: Full rollback requires storing rollback SQL */
+  /* For now, we just remove the version record */
 	_, err = sm.db.ExecContext(ctx, `
 		DELETE FROM neurondb_agent.schema_migrations 
 		WHERE version = $1
diff --git a/NeuronAgent/internal/db/transactions.go b/NeuronAgent/internal/db/transactions.go
index a2f904c..dd40e8c 100644
--- a/NeuronAgent/internal/db/transactions.go
+++ b/NeuronAgent/internal/db/transactions.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * transactions.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/db/transactions.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package db
 
 import (
@@ -8,12 +21,12 @@ import (
 	"github.com/jmoiron/sqlx"
 )
 
-// Transaction represents a database transaction
+/* Transaction represents a database transaction */
 type Transaction struct {
 	tx *sqlx.Tx
 }
 
-// BeginTransaction begins a new transaction
+/* BeginTransaction begins a new transaction */
 func (d *DB) BeginTransaction(ctx context.Context) (*Transaction, error) {
 	tx, err := d.BeginTxx(ctx, nil)
 	if err != nil {
@@ -22,38 +35,38 @@ func (d *DB) BeginTransaction(ctx context.Context) (*Transaction, error) {
 	return &Transaction{tx: tx}, nil
 }
 
-// Commit commits the transaction
+/* Commit commits the transaction */
 func (t *Transaction) Commit() error {
 	return t.tx.Commit()
 }
 
-// Rollback rolls back the transaction
+/* Rollback rolls back the transaction */
 func (t *Transaction) Rollback() error {
 	return t.tx.Rollback()
 }
 
-// Exec executes a query in the transaction
+/* Exec executes a query in the transaction */
 func (t *Transaction) Exec(ctx context.Context, query string, args ...interface{}) error {
 	_, err := t.tx.ExecContext(ctx, query, args...)
 	return err
 }
 
-// Query executes a query and returns rows
+/* Query executes a query and returns rows */
 func (t *Transaction) Query(ctx context.Context, query string, args ...interface{}) (*sqlx.Rows, error) {
 	return t.tx.QueryxContext(ctx, query, args...)
 }
 
-// Get executes a query and scans into dest
+/* Get executes a query and scans into dest */
 func (t *Transaction) Get(ctx context.Context, dest interface{}, query string, args ...interface{}) error {
 	return t.tx.GetContext(ctx, dest, query, args...)
 }
 
-// Select executes a query and scans into dest slice
+/* Select executes a query and scans into dest slice */
 func (t *Transaction) Select(ctx context.Context, dest interface{}, query string, args ...interface{}) error {
 	return t.tx.SelectContext(ctx, dest, query, args...)
 }
 
-// RetryWithBackoff retries a function with exponential backoff
+/* RetryWithBackoff retries a function with exponential backoff */
 func RetryWithBackoff(ctx context.Context, maxRetries int, fn func() error) error {
 	var lastErr error
 	backoff := 100 * time.Millisecond
@@ -70,7 +83,7 @@ func RetryWithBackoff(ctx context.Context, maxRetries int, fn func() error) erro
 			case <-ctx.Done():
 				return ctx.Err()
 			case <-time.After(backoff):
-				backoff *= 2 // Exponential backoff
+    				backoff *= 2 /* Exponential backoff */
 			}
 		}
 	}
@@ -78,7 +91,7 @@ func RetryWithBackoff(ctx context.Context, maxRetries int, fn func() error) erro
 	return fmt.Errorf("max retries exceeded: %w", lastErr)
 }
 
-// WithTransaction executes a function within a transaction
+/* WithTransaction executes a function within a transaction */
 func (d *DB) WithTransaction(ctx context.Context, fn func(*Transaction) error) error {
 	tx, err := d.BeginTransaction(ctx)
 	if err != nil {
diff --git a/NeuronAgent/internal/jobs/processor.go b/NeuronAgent/internal/jobs/processor.go
index e4c2de3..7447144 100644
--- a/NeuronAgent/internal/jobs/processor.go
+++ b/NeuronAgent/internal/jobs/processor.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * processor.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/jobs/processor.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package jobs
 
 import (
@@ -60,7 +73,7 @@ func (p *Processor) processHTTPCall(ctx context.Context, job *db.Job) (map[strin
 		return nil, fmt.Errorf("failed to create request: %w", err)
 	}
 
-	// Add headers
+  /* Add headers */
 	if headers, ok := job.Payload["headers"].(map[string]interface{}); ok {
 		for k, v := range headers {
 			if str, ok := v.(string); ok {
@@ -75,7 +88,7 @@ func (p *Processor) processHTTPCall(ctx context.Context, job *db.Job) (map[strin
 	}
 	defer resp.Body.Close()
 
-	// Read response body (limit to 1MB)
+  /* Read response body (limit to 1MB) */
 	bodyBytes, err := io.ReadAll(io.LimitReader(resp.Body, 1024*1024))
 	if err != nil {
 		return nil, fmt.Errorf("failed to read response: %w", err)
@@ -100,13 +113,13 @@ func (p *Processor) processSQLTask(ctx context.Context, job *db.Job) (map[string
 		return nil, fmt.Errorf("query is required")
 	}
 
-	// Security: Only allow SELECT queries
+  /* Security: Only allow SELECT queries */
 	queryUpper := strings.TrimSpace(strings.ToUpper(query))
 	if !strings.HasPrefix(queryUpper, "SELECT") {
 		return nil, fmt.Errorf("only SELECT queries are allowed in background jobs")
 	}
 
-	// Check for dangerous keywords
+  /* Check for dangerous keywords */
 	dangerous := []string{"DROP", "DELETE", "UPDATE", "INSERT", "ALTER", "CREATE", "TRUNCATE", "EXEC", "EXECUTE"}
 	for _, keyword := range dangerous {
 		if strings.Contains(queryUpper, keyword) {
@@ -114,14 +127,14 @@ func (p *Processor) processSQLTask(ctx context.Context, job *db.Job) (map[string
 		}
 	}
 
-	// Execute query
+  /* Execute query */
 	rows, err := p.db.QueryContext(ctx, query)
 	if err != nil {
 		return nil, fmt.Errorf("query execution failed: %w", err)
 	}
 	defer rows.Close()
 
-	// Convert results to JSON
+  /* Convert results to JSON */
 	var results []map[string]interface{}
 	columns, err := rows.Columns()
 	if err != nil {
@@ -141,7 +154,7 @@ func (p *Processor) processSQLTask(ctx context.Context, job *db.Job) (map[string
 
 		row := make(map[string]interface{})
 		for i, col := range columns {
-			// Handle different types
+    /* Handle different types */
 			val := values[i]
 			if val != nil {
 				switch v := val.(type) {
@@ -175,7 +188,7 @@ func (p *Processor) processShellTask(ctx context.Context, job *db.Job) (map[stri
 		return nil, fmt.Errorf("command is required")
 	}
 
-	// Security: Only allow whitelisted commands
+  /* Security: Only allow whitelisted commands */
 	allowedCommands := []string{"ls", "pwd", "cat", "grep", "find", "head", "tail", "wc", "sort", "uniq", "echo", "date", "whoami"}
 	
 	parts := strings.Fields(command)
@@ -196,11 +209,11 @@ func (p *Processor) processShellTask(ctx context.Context, job *db.Job) (map[stri
 		return nil, fmt.Errorf("command not allowed: %s", cmdName)
 	}
 
-	// Create context with timeout
+  /* Create context with timeout */
 	ctx, cancel := context.WithTimeout(ctx, 30*time.Second)
 	defer cancel()
 
-	// Execute command
+  /* Execute command */
 	cmd := exec.CommandContext(ctx, cmdName, parts[1:]...)
 	output, err := cmd.CombinedOutput()
 	
@@ -215,7 +228,7 @@ func (p *Processor) processShellTask(ctx context.Context, job *db.Job) (map[stri
 			result["exit_code"] = exitError.ExitCode()
 		}
 		result["error"] = err.Error()
-		return result, nil // Return result with error info, don't fail the job
+  		return result, nil /* Return result with error info, don't fail the job */
 	}
 
 	return result, nil
diff --git a/NeuronAgent/internal/jobs/queue.go b/NeuronAgent/internal/jobs/queue.go
index c0f27c6..52b10d0 100644
--- a/NeuronAgent/internal/jobs/queue.go
+++ b/NeuronAgent/internal/jobs/queue.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * queue.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/jobs/queue.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package jobs
 
 import (
@@ -17,7 +30,7 @@ func NewQueue(queries *db.Queries) *Queue {
 	return &Queue{queries: queries}
 }
 
-// Enqueue adds a job to the queue
+/* Enqueue adds a job to the queue */
 func (q *Queue) Enqueue(ctx context.Context, jobType string, agentID, sessionID *uuid.UUID, payload map[string]interface{}, priority int) (*db.Job, error) {
 	job := &db.Job{
 		Type:     jobType,
@@ -36,12 +49,12 @@ func (q *Queue) Enqueue(ctx context.Context, jobType string, agentID, sessionID
 	return job, err
 }
 
-// ClaimJob claims the next available job using SKIP LOCKED
+/* ClaimJob claims the next available job using SKIP LOCKED */
 func (q *Queue) ClaimJob(ctx context.Context) (*db.Job, error) {
 	return q.queries.ClaimJob(ctx)
 }
 
-// UpdateJob updates a job's status and result
+/* UpdateJob updates a job's status and result */
 func (q *Queue) UpdateJob(ctx context.Context, id int64, status string, result map[string]interface{}, errorMsg *string, retryCount int, completedAt *sql.NullTime) error {
 	return q.queries.UpdateJob(ctx, id, status, result, errorMsg, retryCount, completedAt)
 }
diff --git a/NeuronAgent/internal/jobs/retry.go b/NeuronAgent/internal/jobs/retry.go
index 60cc78b..9aa3b91 100644
--- a/NeuronAgent/internal/jobs/retry.go
+++ b/NeuronAgent/internal/jobs/retry.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * retry.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/jobs/retry.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package jobs
 
 import (
@@ -6,7 +19,7 @@ import (
 	"time"
 )
 
-// RetryConfig configures retry behavior
+/* RetryConfig configures retry behavior */
 type RetryConfig struct {
 	MaxRetries      int
 	InitialDelay    time.Duration
@@ -15,7 +28,7 @@ type RetryConfig struct {
 	Jitter          bool
 }
 
-// DefaultRetryConfig returns default retry configuration
+/* DefaultRetryConfig returns default retry configuration */
 func DefaultRetryConfig() RetryConfig {
 	return RetryConfig{
 		MaxRetries:       3,
@@ -26,19 +39,19 @@ func DefaultRetryConfig() RetryConfig {
 	}
 }
 
-// CalculateDelay calculates the delay for a retry attempt
+/* CalculateDelay calculates the delay for a retry attempt */
 func CalculateDelay(attempt int, config RetryConfig) time.Duration {
-	// Exponential backoff: delay = initial * (multiplier ^ attempt)
+  /* Exponential backoff: delay = initial * (multiplier ^ attempt) */
 	delay := float64(config.InitialDelay) * math.Pow(config.BackoffMultiplier, float64(attempt))
 	
-	// Cap at max delay
+  /* Cap at max delay */
 	if delay > float64(config.MaxDelay) {
 		delay = float64(config.MaxDelay)
 	}
 	
-	// Add jitter (random variation) to prevent thundering herd
+  /* Add jitter (random variation) to prevent thundering herd */
 	if config.Jitter {
-		// Jitter: Â±25% variation
+   /* Jitter: Â±25% variation */
 		jitter := delay * 0.25
 		delay = delay - jitter + (jitter * 2 * (float64(time.Now().UnixNano()%100) / 100))
 	}
@@ -46,18 +59,18 @@ func CalculateDelay(attempt int, config RetryConfig) time.Duration {
 	return time.Duration(delay)
 }
 
-// ShouldRetry determines if a job should be retried based on error
+/* ShouldRetry determines if a job should be retried based on error */
 func ShouldRetry(err error, attempt int, maxRetries int) bool {
 	if attempt >= maxRetries {
 		return false
 	}
 	
-	// Don't retry on certain errors (e.g., validation errors)
-	// In production, you'd check error types
+  /* Don't retry on certain errors (e.g., validation errors) */
+  /* In production, you'd check error types */
 	return true
 }
 
-// RetryWithBackoff retries a function with exponential backoff
+/* RetryWithBackoff retries a function with exponential backoff */
 func RetryWithBackoff(ctx context.Context, config RetryConfig, fn func() error) error {
 	var lastErr error
 	
@@ -68,7 +81,7 @@ func RetryWithBackoff(ctx context.Context, config RetryConfig, fn func() error)
 			lastErr = err
 		}
 		
-		// Don't wait after last attempt
+   /* Don't wait after last attempt */
 		if attempt < config.MaxRetries {
 			delay := CalculateDelay(attempt, config)
 			
@@ -76,7 +89,7 @@ func RetryWithBackoff(ctx context.Context, config RetryConfig, fn func() error)
 			case <-ctx.Done():
 				return ctx.Err()
 			case <-time.After(delay):
-				// Continue to next attempt
+     /* Continue to next attempt */
 			}
 		}
 	}
diff --git a/NeuronAgent/internal/jobs/scheduler.go b/NeuronAgent/internal/jobs/scheduler.go
index 593f7dc..1d120f4 100644
--- a/NeuronAgent/internal/jobs/scheduler.go
+++ b/NeuronAgent/internal/jobs/scheduler.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * scheduler.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/jobs/scheduler.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package jobs
 
 import (
@@ -32,17 +45,17 @@ func NewScheduler(queue *Queue) *Scheduler {
 		jobs:   make(map[string]*ScheduledJob),
 		ctx:    ctx,
 		cancel: cancel,
-		ticker: time.NewTicker(1 * time.Minute), // Check every minute
+  		ticker: time.NewTicker(1 * time.Minute), /* Check every minute */
 	}
 }
 
-// Start starts the scheduler
+/* Start starts the scheduler */
 func (s *Scheduler) Start() {
 	s.wg.Add(1)
 	go s.run()
 }
 
-// Stop stops the scheduler
+/* Stop stops the scheduler */
 func (s *Scheduler) Stop() {
 	s.cancel()
 	s.ticker.Stop()
@@ -52,7 +65,7 @@ func (s *Scheduler) Stop() {
 func (s *Scheduler) run() {
 	defer s.wg.Done()
 
-	// Check immediately
+  /* Check immediately */
 	s.checkAndRun()
 
 	for {
@@ -77,7 +90,7 @@ func (s *Scheduler) checkAndRun() {
 	}
 	s.mu.RUnlock()
 
-	// Run jobs
+  /* Run jobs */
 	for _, job := range jobsToRun {
 		s.runJob(job)
 	}
@@ -87,22 +100,22 @@ func (s *Scheduler) runJob(job *ScheduledJob) {
 	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
 	defer cancel()
 
-	// Enqueue job
+  /* Enqueue job */
 	_, err := s.queue.Enqueue(ctx, job.JobType, nil, nil, job.Payload, 0)
 	if err != nil {
 		return
 	}
 
-	// Calculate next run time (simplified - in production use cron parser)
+  /* Calculate next run time (simplified - in production use cron parser) */
 	s.mu.Lock()
-	job.NextRun = time.Now().Add(1 * time.Hour) // Default: run every hour
+ 	job.NextRun = time.Now().Add(1 * time.Hour) /* Default: run every hour */
 	s.mu.Unlock()
 }
 
-// Schedule adds a scheduled job
+/* Schedule adds a scheduled job */
 func (s *Scheduler) Schedule(id, cronExpr, jobType string, payload map[string]interface{}) error {
-	// Parse cron expression (simplified - in production use robfig/cron)
-	// For now, parse simple patterns like "0 * * * *" (every hour)
+  /* Parse cron expression (simplified - in production use robfig/cron) */
+  /* For now, parse simple patterns like "0 * * * *" (every hour) */
 	nextRun := parseCronExpression(cronExpr)
 	
 	s.mu.Lock()
@@ -119,21 +132,21 @@ func (s *Scheduler) Schedule(id, cronExpr, jobType string, payload map[string]in
 	return nil
 }
 
-// parseCronExpression parses a simple cron expression (simplified)
+/* parseCronExpression parses a simple cron expression (simplified) */
 func parseCronExpression(expr string) time.Time {
-	// Default: run every hour
-	// In production, use a proper cron parser like robfig/cron
+  /* Default: run every hour */
+  /* In production, use a proper cron parser like robfig/cron */
 	return time.Now().Add(1 * time.Hour)
 }
 
-// Unschedule removes a scheduled job
+/* Unschedule removes a scheduled job */
 func (s *Scheduler) Unschedule(id string) {
 	s.mu.Lock()
 	delete(s.jobs, id)
 	s.mu.Unlock()
 }
 
-// List returns all scheduled jobs
+/* List returns all scheduled jobs */
 func (s *Scheduler) List() []*ScheduledJob {
 	s.mu.RLock()
 	defer s.mu.RUnlock()
diff --git a/NeuronAgent/internal/jobs/worker.go b/NeuronAgent/internal/jobs/worker.go
index c44d5b2..0d7beae 100644
--- a/NeuronAgent/internal/jobs/worker.go
+++ b/NeuronAgent/internal/jobs/worker.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * worker.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/jobs/worker.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package jobs
 
 import (
@@ -78,18 +91,18 @@ func (w *Worker) processJob(job *db.Job) {
 			completedAt = &now
 		} else {
 			status = "queued" // Retry - will be picked up again
-			// Don't set completedAt for retries
+    /* Don't set completedAt for retries */
 		}
 	} else {
-		// Success
+   /* Success */
 		now := time.Now()
 		completedAt = &now
 	}
 
-	// Record metrics
+  /* Record metrics */
 	metrics.RecordJobProcessed(job.Type, status)
 
-	// Use proper time handling for UpdateJob
+  /* Use proper time handling for UpdateJob */
 	var completedAtVal *sql.NullTime
 	if completedAt != nil {
 		completedAtVal = &sql.NullTime{
diff --git a/NeuronAgent/internal/metrics/logging.go b/NeuronAgent/internal/metrics/logging.go
index 09002a0..1345fa7 100644
--- a/NeuronAgent/internal/metrics/logging.go
+++ b/NeuronAgent/internal/metrics/logging.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * logging.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/metrics/logging.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package metrics
 
 import (
@@ -10,7 +23,7 @@ import (
 func InitLogging(level string, format string) {
 	zerolog.TimeFieldFormat = zerolog.TimeFormatUnix
 
-	// Set log level
+  /* Set log level */
 	switch level {
 	case "debug":
 		zerolog.SetGlobalLevel(zerolog.DebugLevel)
@@ -24,7 +37,7 @@ func InitLogging(level string, format string) {
 		zerolog.SetGlobalLevel(zerolog.InfoLevel)
 	}
 
-	// Set format
+  /* Set format */
 	if format == "console" {
 		log.Logger = log.Output(zerolog.ConsoleWriter{Out: os.Stderr})
 	} else {
diff --git a/NeuronAgent/internal/metrics/prometheus.go b/NeuronAgent/internal/metrics/prometheus.go
index 28d0f1f..e664c5b 100644
--- a/NeuronAgent/internal/metrics/prometheus.go
+++ b/NeuronAgent/internal/metrics/prometheus.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * prometheus.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/metrics/prometheus.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package metrics
 
 import (
@@ -10,7 +23,7 @@ import (
 )
 
 var (
-	// Request metrics
+  /* Request metrics */
 	httpRequestsTotal = promauto.NewCounterVec(
 		prometheus.CounterOpts{
 			Name: "neurondb_agent_http_requests_total",
@@ -28,7 +41,7 @@ var (
 		[]string{"method", "endpoint"},
 	)
 
-	// Agent metrics
+  /* Agent metrics */
 	agentExecutionsTotal = promauto.NewCounterVec(
 		prometheus.CounterOpts{
 			Name: "neurondb_agent_executions_total",
@@ -46,7 +59,7 @@ var (
 		[]string{"agent_id"},
 	)
 
-	// LLM metrics
+  /* LLM metrics */
 	llmCallsTotal = promauto.NewCounterVec(
 		prometheus.CounterOpts{
 			Name: "neurondb_agent_llm_calls_total",
@@ -63,7 +76,7 @@ var (
 		[]string{"model", "type"},
 	)
 
-	// Memory metrics
+  /* Memory metrics */
 	memoryChunksStored = promauto.NewCounterVec(
 		prometheus.CounterOpts{
 			Name: "neurondb_agent_memory_chunks_stored_total",
@@ -80,7 +93,7 @@ var (
 		[]string{"agent_id"},
 	)
 
-	// Tool metrics
+  /* Tool metrics */
 	toolExecutionsTotal = promauto.NewCounterVec(
 		prometheus.CounterOpts{
 			Name: "neurondb_agent_tool_executions_total",
@@ -98,7 +111,7 @@ var (
 		[]string{"tool_name"},
 	)
 
-	// Job metrics
+  /* Job metrics */
 	jobsQueued = promauto.NewGauge(
 		prometheus.GaugeOpts{
 			Name: "neurondb_agent_jobs_queued",
@@ -115,53 +128,53 @@ var (
 	)
 )
 
-// RecordHTTPRequest records an HTTP request
+/* RecordHTTPRequest records an HTTP request */
 func RecordHTTPRequest(method, endpoint string, status int, duration time.Duration) {
 	httpRequestsTotal.WithLabelValues(method, endpoint, http.StatusText(status)).Inc()
 	httpRequestDuration.WithLabelValues(method, endpoint).Observe(duration.Seconds())
 }
 
-// RecordAgentExecution records an agent execution
+/* RecordAgentExecution records an agent execution */
 func RecordAgentExecution(agentID, status string, duration time.Duration) {
 	agentExecutionsTotal.WithLabelValues(agentID, status).Inc()
 	agentExecutionDuration.WithLabelValues(agentID).Observe(duration.Seconds())
 }
 
-// RecordLLMCall records an LLM call
+/* RecordLLMCall records an LLM call */
 func RecordLLMCall(model, status string, promptTokens, completionTokens int) {
 	llmCallsTotal.WithLabelValues(model, status).Inc()
 	llmTokensTotal.WithLabelValues(model, "prompt").Add(float64(promptTokens))
 	llmTokensTotal.WithLabelValues(model, "completion").Add(float64(completionTokens))
 }
 
-// RecordMemoryChunkStored records a memory chunk being stored
+/* RecordMemoryChunkStored records a memory chunk being stored */
 func RecordMemoryChunkStored(agentID string) {
 	memoryChunksStored.WithLabelValues(agentID).Inc()
 }
 
-// RecordMemoryRetrieval records a memory retrieval
+/* RecordMemoryRetrieval records a memory retrieval */
 func RecordMemoryRetrieval(agentID string) {
 	memoryRetrievalsTotal.WithLabelValues(agentID).Inc()
 }
 
-// RecordToolExecution records a tool execution
+/* RecordToolExecution records a tool execution */
 func RecordToolExecution(toolName, status string, duration time.Duration) {
 	toolExecutionsTotal.WithLabelValues(toolName, status).Inc()
 	toolExecutionDuration.WithLabelValues(toolName).Observe(duration.Seconds())
 }
 
-// RecordJobQueued records a job being queued
+/* RecordJobQueued records a job being queued */
 func RecordJobQueued() {
 	jobsQueued.Inc()
 }
 
-// RecordJobProcessed records a job being processed
+/* RecordJobProcessed records a job being processed */
 func RecordJobProcessed(jobType, status string) {
 	jobsProcessedTotal.WithLabelValues(jobType, status).Inc()
 	jobsQueued.Dec()
 }
 
-// Handler returns the Prometheus metrics handler
+/* Handler returns the Prometheus metrics handler */
 func Handler() http.Handler {
 	return promhttp.Handler()
 }
diff --git a/NeuronAgent/internal/metrics/tracing.go b/NeuronAgent/internal/metrics/tracing.go
index 0d3d105..00e2392 100644
--- a/NeuronAgent/internal/metrics/tracing.go
+++ b/NeuronAgent/internal/metrics/tracing.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * tracing.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/metrics/tracing.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package metrics
 
 import (
@@ -30,12 +43,12 @@ func NewTracer() *Tracer {
 	}
 }
 
-// StartSpan starts a new span
+/* StartSpan starts a new span */
 func (t *Tracer) StartSpan(ctx context.Context, name string) (context.Context, string) {
 	spanID := uuid.New().String()
 	traceID := TraceID(uuid.New().String())
 
-	// Try to get trace ID from context
+  /* Try to get trace ID from context */
 	if existingTraceID := ctx.Value("trace_id"); existingTraceID != nil {
 		traceID = existingTraceID.(TraceID)
 	}
@@ -48,37 +61,37 @@ func (t *Tracer) StartSpan(ctx context.Context, name string) (context.Context, s
 		Attributes: make(map[string]interface{}),
 	}
 
-	// Get parent span ID if exists
+  /* Get parent span ID if exists */
 	if parentSpanID := ctx.Value("span_id"); parentSpanID != nil {
 		span.ParentID = parentSpanID.(string)
 	}
 
 	t.spans[spanID] = span
 
-	// Add to context
+  /* Add to context */
 	ctx = context.WithValue(ctx, "trace_id", traceID)
 	ctx = context.WithValue(ctx, "span_id", spanID)
 
 	return ctx, spanID
 }
 
-// EndSpan ends a span
+/* EndSpan ends a span */
 func (t *Tracer) EndSpan(spanID string) {
 	if span, exists := t.spans[spanID]; exists {
 		span.EndTime = time.Now()
-		// In production, send to tracing backend
+   /* In production, send to tracing backend */
 		delete(t.spans, spanID)
 	}
 }
 
-// AddAttribute adds an attribute to a span
+/* AddAttribute adds an attribute to a span */
 func (t *Tracer) AddAttribute(spanID string, key string, value interface{}) {
 	if span, exists := t.spans[spanID]; exists {
 		span.Attributes[key] = value
 	}
 }
 
-// GetSpan returns a span by ID
+/* GetSpan returns a span by ID */
 func (t *Tracer) GetSpan(spanID string) (*Span, error) {
 	span, exists := t.spans[spanID]
 	if !exists {
@@ -87,19 +100,19 @@ func (t *Tracer) GetSpan(spanID string) (*Span, error) {
 	return span, nil
 }
 
-// GetTraceIDFromContext gets trace ID from context
+/* GetTraceIDFromContext gets trace ID from context */
 func GetTraceIDFromContext(ctx context.Context) (TraceID, bool) {
 	traceID, ok := ctx.Value("trace_id").(TraceID)
 	return traceID, ok
 }
 
-// GetSpanIDFromContext gets span ID from context
+/* GetSpanIDFromContext gets span ID from context */
 func GetSpanIDFromContext(ctx context.Context) (string, bool) {
 	spanID, ok := ctx.Value("span_id").(string)
 	return spanID, ok
 }
 
-// FormatTraceHeader formats trace ID for HTTP headers
+/* FormatTraceHeader formats trace ID for HTTP headers */
 func (t TraceID) String() string {
 	return string(t)
 }
diff --git a/NeuronAgent/internal/session/cache.go b/NeuronAgent/internal/session/cache.go
index f04370b..af69474 100644
--- a/NeuronAgent/internal/session/cache.go
+++ b/NeuronAgent/internal/session/cache.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * cache.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/session/cache.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package session
 
 import (
@@ -29,7 +42,7 @@ func NewCache(ttl time.Duration) *Cache {
 		stop:          make(chan struct{}),
 	}
 
-	// Start cleanup goroutine
+  /* Start cleanup goroutine */
 	go cache.runCleanup()
 
 	return cache
@@ -55,7 +68,7 @@ func (c *Cache) Get(id uuid.UUID) *db.Session {
 	}
 
 	if time.Now().After(cached.expiresAt) {
-		// Expired, remove it
+   /* Expired, remove it */
 		c.mu.RUnlock()
 		c.mu.Lock()
 		delete(c.sessions, id)
diff --git a/NeuronAgent/internal/session/cleanup.go b/NeuronAgent/internal/session/cleanup.go
index 9391feb..8f13bcd 100644
--- a/NeuronAgent/internal/session/cleanup.go
+++ b/NeuronAgent/internal/session/cleanup.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * cleanup.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/session/cleanup.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package session
 
 import (
@@ -28,12 +41,12 @@ func NewCleanupService(queries *db.Queries, interval, maxAge time.Duration) *Cle
 	}
 }
 
-// Start starts the cleanup service
+/* Start starts the cleanup service */
 func (s *CleanupService) Start() {
 	go s.run()
 }
 
-// Stop stops the cleanup service
+/* Stop stops the cleanup service */
 func (s *CleanupService) Stop() {
 	s.cancel()
 	<-s.done
@@ -45,7 +58,7 @@ func (s *CleanupService) run() {
 	ticker := time.NewTicker(s.interval)
 	defer ticker.Stop()
 
-	// Run immediately on start
+  /* Run immediately on start */
 	s.cleanup()
 
 	for {
@@ -62,10 +75,10 @@ func (s *CleanupService) cleanup() {
 	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
 	defer cancel()
 
-	// Delete sessions older than maxAge
+  /* Delete sessions older than maxAge */
 	cutoffTime := time.Now().Add(-s.maxAge)
 	
-	// Get all agents to check their sessions
+  /* Get all agents to check their sessions */
 	agents, err := s.queries.ListAgents(ctx)
 	if err != nil {
 		return
diff --git a/NeuronAgent/internal/session/manager.go b/NeuronAgent/internal/session/manager.go
index 14b54d1..0c7d3a3 100644
--- a/NeuronAgent/internal/session/manager.go
+++ b/NeuronAgent/internal/session/manager.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * manager.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/session/manager.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package session
 
 import (
@@ -19,7 +32,7 @@ func NewManager(queries *db.Queries, cache *Cache) *Manager {
 	}
 }
 
-// Create creates a new session
+/* Create creates a new session */
 func (m *Manager) Create(ctx context.Context, agentID uuid.UUID, externalUserID *string, metadata map[string]interface{}) (*db.Session, error) {
 	session := &db.Session{
 		AgentID:       agentID,
@@ -31,7 +44,7 @@ func (m *Manager) Create(ctx context.Context, agentID uuid.UUID, externalUserID
 		return nil, err
 	}
 
-	// Cache the session
+  /* Cache the session */
 	if m.cache != nil {
 		m.cache.Set(session.ID, session)
 	}
@@ -39,22 +52,22 @@ func (m *Manager) Create(ctx context.Context, agentID uuid.UUID, externalUserID
 	return session, nil
 }
 
-// Get retrieves a session by ID
+/* Get retrieves a session by ID */
 func (m *Manager) Get(ctx context.Context, id uuid.UUID) (*db.Session, error) {
-	// Try cache first
+  /* Try cache first */
 	if m.cache != nil {
 		if session := m.cache.Get(id); session != nil {
 			return session, nil
 		}
 	}
 
-	// Get from database
+  /* Get from database */
 	session, err := m.queries.GetSession(ctx, id)
 	if err != nil {
 		return nil, err
 	}
 
-	// Cache it
+  /* Cache it */
 	if m.cache != nil {
 		m.cache.Set(id, session)
 	}
@@ -62,18 +75,18 @@ func (m *Manager) Get(ctx context.Context, id uuid.UUID) (*db.Session, error) {
 	return session, nil
 }
 
-// List lists sessions for an agent
+/* List lists sessions for an agent */
 func (m *Manager) List(ctx context.Context, agentID uuid.UUID, limit, offset int) ([]db.Session, error) {
 	return m.queries.ListSessions(ctx, agentID, limit, offset)
 }
 
-// Delete deletes a session
+/* Delete deletes a session */
 func (m *Manager) Delete(ctx context.Context, id uuid.UUID) error {
 	if err := m.queries.DeleteSession(ctx, id); err != nil {
 		return err
 	}
 
-	// Remove from cache
+  /* Remove from cache */
 	if m.cache != nil {
 		m.cache.Delete(id)
 	}
@@ -81,9 +94,9 @@ func (m *Manager) Delete(ctx context.Context, id uuid.UUID) error {
 	return nil
 }
 
-// UpdateActivity updates the last activity time for a session
+/* UpdateActivity updates the last activity time for a session */
 func (m *Manager) UpdateActivity(ctx context.Context, id uuid.UUID) error {
-	// This is handled by the database trigger, but we can refresh cache
+  /* This is handled by the database trigger, but we can refresh cache */
 	if m.cache != nil {
 		if session, err := m.queries.GetSession(ctx, id); err == nil {
 			m.cache.Set(id, session)
diff --git a/NeuronAgent/internal/tools/code_tool.go b/NeuronAgent/internal/tools/code_tool.go
index 253cd85..f7649ae 100644
--- a/NeuronAgent/internal/tools/code_tool.go
+++ b/NeuronAgent/internal/tools/code_tool.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * code_tool.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/tools/code_tool.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -12,7 +25,7 @@ import (
 )
 
 type CodeTool struct {
-	allowedDirs []string // Allowed directories for code analysis
+ 	allowedDirs []string /* Allowed directories for code analysis */
 }
 
 func NewCodeTool() *CodeTool {
@@ -32,7 +45,7 @@ func (t *CodeTool) Execute(ctx context.Context, tool *db.Tool, args map[string]i
 			tool.Name, len(args), argKeys)
 	}
 
-	// Security: Check if path is in allowed directories
+  /* Security: Check if path is in allowed directories */
 	allowed := false
 	absPath, err := filepath.Abs(path)
 	if err != nil {
@@ -129,7 +142,7 @@ func (t *CodeTool) listDirectory(tool *db.Tool, path string) (string, error) {
 }
 
 func (t *CodeTool) analyzeCode(tool *db.Tool, path string) (string, error) {
-	// Simple code analysis - count lines, functions, etc.
+  /* Simple code analysis - count lines, functions, etc. */
 	content, err := os.ReadFile(path)
 	if err != nil {
 		return "", fmt.Errorf("code tool file read failed: tool_name='%s', handler_type='code', path='%s', action='analyze', error=%w",
diff --git a/NeuronAgent/internal/tools/executor.go b/NeuronAgent/internal/tools/executor.go
index 82d938a..cf64d60 100644
--- a/NeuronAgent/internal/tools/executor.go
+++ b/NeuronAgent/internal/tools/executor.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * executor.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/tools/executor.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -9,13 +22,13 @@ import (
 	"github.com/neurondb/NeuronAgent/internal/metrics"
 )
 
-// Executor handles tool execution with timeout and error handling
+/* Executor handles tool execution with timeout and error handling */
 type Executor struct {
 	registry *Registry
 	timeout  time.Duration
 }
 
-// NewExecutor creates a new tool executor
+/* NewExecutor creates a new tool executor */
 func NewExecutor(registry *Registry, timeout time.Duration) *Executor {
 	return &Executor{
 		registry: registry,
@@ -23,19 +36,19 @@ func NewExecutor(registry *Registry, timeout time.Duration) *Executor {
 	}
 }
 
-// Execute executes a tool with timeout
+/* Execute executes a tool with timeout */
 func (e *Executor) Execute(ctx context.Context, tool *db.Tool, args map[string]interface{}) (string, error) {
 	start := time.Now()
 	
-	// Create context with timeout
+  /* Create context with timeout */
 	ctx, cancel := context.WithTimeout(ctx, e.timeout)
 	defer cancel()
 
-	// Execute tool
+  /* Execute tool */
 	result, err := e.registry.Execute(ctx, tool, args)
 	duration := time.Since(start)
 	
-	// Record metrics
+  /* Record metrics */
 	status := "success"
 	if err != nil {
 		status = "error"
@@ -54,7 +67,7 @@ func (e *Executor) Execute(ctx context.Context, tool *db.Tool, args map[string]i
 	return result, nil
 }
 
-// ExecuteByName executes a tool by name
+/* ExecuteByName executes a tool by name */
 func (e *Executor) ExecuteByName(ctx context.Context, toolName string, args map[string]interface{}) (string, error) {
 	tool, err := e.registry.Get(toolName)
 	if err != nil {
diff --git a/NeuronAgent/internal/tools/http_tool.go b/NeuronAgent/internal/tools/http_tool.go
index 5552817..59a7a44 100644
--- a/NeuronAgent/internal/tools/http_tool.go
+++ b/NeuronAgent/internal/tools/http_tool.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * http_tool.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/tools/http_tool.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -14,7 +27,7 @@ import (
 
 type HTTPTool struct {
 	client  *http.Client
-	allowed map[string]bool // URL allowlist
+ 	allowed map[string]bool /* URL allowlist */
 }
 
 func NewHTTPTool() *HTTPTool {
@@ -37,10 +50,10 @@ func (t *HTTPTool) Execute(ctx context.Context, tool *db.Tool, args map[string]i
 			tool.Name, len(args), argKeys)
 	}
 
-	// Check allowlist if configured
+  /* Check allowlist if configured */
 	allowlistSize := len(t.allowed)
 	if allowlistSize > 0 && !t.allowed[url] {
-		// Check if any allowed prefix matches
+   /* Check if any allowed prefix matches */
 		allowed := false
 		for allowedURL := range t.allowed {
 			if strings.HasPrefix(url, allowedURL) {
@@ -69,14 +82,14 @@ func (t *HTTPTool) Execute(ctx context.Context, tool *db.Tool, args map[string]i
 		bodySize = len(body)
 	}
 
-	// Create request
+  /* Create request */
 	req, err := http.NewRequestWithContext(ctx, method, url, nil)
 	if err != nil {
 		return "", fmt.Errorf("HTTP tool request creation failed: tool_name='%s', handler_type='http', method='%s', url='%s', headers_count=%d, body_size=%d, timeout=%v, error=%w",
 			tool.Name, method, url, headerCount, bodySize, t.client.Timeout, err)
 	}
 
-	// Add headers
+  /* Add headers */
 	if headers, ok := args["headers"].(map[string]interface{}); ok {
 		for k, v := range headers {
 			if str, ok := v.(string); ok {
@@ -85,13 +98,13 @@ func (t *HTTPTool) Execute(ctx context.Context, tool *db.Tool, args map[string]i
 		}
 	}
 
-	// Add body for POST/PUT
+  /* Add body for POST/PUT */
 	if body, ok := args["body"].(string); ok && (method == "POST" || method == "PUT" || method == "PATCH") {
 		req.Body = io.NopCloser(strings.NewReader(body))
 		req.ContentLength = int64(len(body))
 	}
 
-	// Execute request
+  /* Execute request */
 	resp, err := t.client.Do(req)
 	if err != nil {
 		return "", fmt.Errorf("HTTP tool request execution failed: tool_name='%s', handler_type='http', method='%s', url='%s', headers_count=%d, body_size=%d, timeout=%v, error=%w",
@@ -99,7 +112,7 @@ func (t *HTTPTool) Execute(ctx context.Context, tool *db.Tool, args map[string]i
 	}
 	defer resp.Body.Close()
 
-	// Limit response size (1MB)
+  /* Limit response size (1MB) */
 	maxResponseSize := 1024 * 1024
 	limitedReader := io.LimitReader(resp.Body, int64(maxResponseSize))
 	body, err := io.ReadAll(limitedReader)
@@ -108,7 +121,7 @@ func (t *HTTPTool) Execute(ctx context.Context, tool *db.Tool, args map[string]i
 			tool.Name, method, url, resp.StatusCode, maxResponseSize, err)
 	}
 
-	// Format response
+  /* Format response */
 	result := map[string]interface{}{
 		"status_code": resp.StatusCode,
 		"headers":     resp.Header,
diff --git a/NeuronAgent/internal/tools/registry.go b/NeuronAgent/internal/tools/registry.go
index e2266ad..8f0d3a0 100644
--- a/NeuronAgent/internal/tools/registry.go
+++ b/NeuronAgent/internal/tools/registry.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * registry.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/tools/registry.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,7 +21,7 @@ import (
 	"github.com/neurondb/NeuronAgent/internal/db"
 )
 
-// Registry manages tool registration and execution
+/* Registry manages tool registration and execution */
 type Registry struct {
 	queries  *db.Queries
 	db       *db.DB
@@ -16,7 +29,7 @@ type Registry struct {
 	mu       sync.RWMutex
 }
 
-// NewRegistry creates a new tool registry
+/* NewRegistry creates a new tool registry */
 func NewRegistry(queries *db.Queries, database *db.DB) *Registry {
 	registry := &Registry{
 		queries:  queries,
@@ -24,7 +37,7 @@ func NewRegistry(queries *db.Queries, database *db.DB) *Registry {
 		handlers: make(map[string]ToolHandler),
 	}
 
-	// Register built-in handlers
+  /* Register built-in handlers */
 	sqlTool := NewSQLTool(queries)
 	sqlTool.db = database
 	registry.RegisterHandler("sql", sqlTool)
@@ -35,15 +48,15 @@ func NewRegistry(queries *db.Queries, database *db.DB) *Registry {
 	return registry
 }
 
-// RegisterHandler registers a tool handler for a specific handler type
+/* RegisterHandler registers a tool handler for a specific handler type */
 func (r *Registry) RegisterHandler(handlerType string, handler ToolHandler) {
 	r.mu.Lock()
 	defer r.mu.Unlock()
 	r.handlers[handlerType] = handler
 }
 
-// Get retrieves a tool from the database
-// Implements agent.ToolRegistry interface
+/* Get retrieves a tool from the database */
+/* Implements agent.ToolRegistry interface */
 func (r *Registry) Get(name string) (*db.Tool, error) {
 	tool, err := r.queries.GetTool(context.Background(), name)
 	if err != nil {
@@ -52,13 +65,13 @@ func (r *Registry) Get(name string) (*db.Tool, error) {
 	return tool, nil
 }
 
-// Execute executes a tool with the given arguments
-// Implements agent.ToolRegistry interface
+/* Execute executes a tool with the given arguments */
+/* Implements agent.ToolRegistry interface */
 func (r *Registry) Execute(ctx context.Context, tool *db.Tool, args map[string]interface{}) (string, error) {
 	return r.ExecuteTool(ctx, tool, args)
 }
 
-// ExecuteTool executes a tool with the given arguments (internal method)
+/* ExecuteTool executes a tool with the given arguments (internal method) */
 func (r *Registry) ExecuteTool(ctx context.Context, tool *db.Tool, args map[string]interface{}) (string, error) {
 	if !tool.Enabled {
 		argKeys := make([]string, 0, len(args))
@@ -69,7 +82,7 @@ func (r *Registry) ExecuteTool(ctx context.Context, tool *db.Tool, args map[stri
 			tool.Name, tool.HandlerType, len(args), argKeys)
 	}
 
-	// Validate arguments
+  /* Validate arguments */
 	if err := ValidateArgs(args, tool.ArgSchema); err != nil {
 		argKeys := make([]string, 0, len(args))
 		for k := range args {
@@ -79,7 +92,7 @@ func (r *Registry) ExecuteTool(ctx context.Context, tool *db.Tool, args map[stri
 			tool.Name, tool.HandlerType, len(args), argKeys, err)
 	}
 
-	// Get handler
+  /* Get handler */
 	r.mu.RLock()
 	handler, exists := r.handlers[tool.HandlerType]
 	r.mu.RUnlock()
@@ -97,7 +110,7 @@ func (r *Registry) ExecuteTool(ctx context.Context, tool *db.Tool, args map[stri
 			tool.Name, tool.HandlerType, len(args), argKeys, availableHandlers)
 	}
 
-	// Execute tool
+  /* Execute tool */
 	result, err := handler.Execute(ctx, tool, args)
 	if err != nil {
 		argKeys := make([]string, 0, len(args))
@@ -110,7 +123,7 @@ func (r *Registry) ExecuteTool(ctx context.Context, tool *db.Tool, args map[stri
 	return result, nil
 }
 
-// ListTools returns all enabled tools
+/* ListTools returns all enabled tools */
 func (r *Registry) ListTools(ctx context.Context) ([]db.Tool, error) {
 	return r.queries.ListTools(ctx)
 }
diff --git a/NeuronAgent/internal/tools/sandbox.go b/NeuronAgent/internal/tools/sandbox.go
index f990791..1a34986 100644
--- a/NeuronAgent/internal/tools/sandbox.go
+++ b/NeuronAgent/internal/tools/sandbox.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * sandbox.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/tools/sandbox.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -10,14 +23,14 @@ import (
 	"time"
 )
 
-// Sandbox provides security sandboxing for tool execution
+/* Sandbox provides security sandboxing for tool execution */
 type Sandbox struct {
 	chrootPath string
-	maxMemory  int64 // in bytes
-	maxCPU     int   // percentage
+ 	maxMemory  int64 /* in bytes */
+ 	maxCPU     int /* percentage */
 }
 
-// NewSandbox creates a new sandbox (Unix/Linux only)
+/* NewSandbox creates a new sandbox (Unix/Linux only) */
 func NewSandbox(chrootPath string, maxMemory int64, maxCPU int) *Sandbox {
 	return &Sandbox{
 		chrootPath: chrootPath,
@@ -26,7 +39,7 @@ func NewSandbox(chrootPath string, maxMemory int64, maxCPU int) *Sandbox {
 	}
 }
 
-// ApplyResourceLimits applies resource limits to a command
+/* ApplyResourceLimits applies resource limits to a command */
 func (s *Sandbox) ApplyResourceLimits(cmd *exec.Cmd) error {
 	if runtime.GOOS == "linux" || runtime.GOOS == "darwin" {
 		if cmd.SysProcAttr == nil {
@@ -34,7 +47,7 @@ func (s *Sandbox) ApplyResourceLimits(cmd *exec.Cmd) error {
 		}
 		cmd.SysProcAttr.Setpgid = true
 
-		// Set resource limits using rlimit
+   /* Set resource limits using rlimit */
 		if s.maxMemory > 0 {
 			var rlimit syscall.Rlimit
 			if err := syscall.Getrlimit(syscall.RLIMIT_AS, &rlimit); err == nil {
@@ -44,26 +57,26 @@ func (s *Sandbox) ApplyResourceLimits(cmd *exec.Cmd) error {
 			}
 		}
 
-		// Set CPU time limit (soft limit)
+   /* Set CPU time limit (soft limit) */
 		if s.maxCPU > 0 {
 			var rlimit syscall.Rlimit
 			if err := syscall.Getrlimit(syscall.RLIMIT_CPU, &rlimit); err == nil {
-				// maxCPU is percentage, convert to seconds (approximate)
-				cpuSeconds := uint64(s.maxCPU * 60) // Allow maxCPU minutes
+     /* maxCPU is percentage, convert to seconds (approximate) */
+    				cpuSeconds := uint64(s.maxCPU * 60) /* Allow maxCPU minutes */
 				rlimit.Cur = cpuSeconds
 				rlimit.Max = cpuSeconds
 				syscall.Setrlimit(syscall.RLIMIT_CPU, &rlimit)
 			}
 		}
 
-		// Context timeout is handled by exec.CommandContext when creating the command
+   /* Context timeout is handled by exec.CommandContext when creating the command */
 	}
 	return nil
 }
 
-// Chroot applies chroot if configured (requires root privileges)
-// Note: This requires the process to run as root and the chroot directory
-// to be properly set up with necessary files (binaries, libraries, etc.)
+/* Chroot applies chroot if configured (requires root privileges) */
+/* Note: This requires the process to run as root and the chroot directory */
+/* to be properly set up with necessary files (binaries, libraries, etc.) */
 func (s *Sandbox) Chroot(cmd *exec.Cmd) error {
 	if s.chrootPath == "" {
 		return nil
@@ -73,33 +86,33 @@ func (s *Sandbox) Chroot(cmd *exec.Cmd) error {
 		return fmt.Errorf("chroot path does not exist: %s", s.chrootPath)
 	}
 
-	// Chroot requires root privileges
-	// In production, this should be handled by the system or container
-	// For now, we set the working directory as a safer alternative
+  /* Chroot requires root privileges */
+  /* In production, this should be handled by the system or container */
+  /* For now, we set the working directory as a safer alternative */
 	if cmd.Dir == "" {
 		cmd.Dir = s.chrootPath
 	}
 
-	// Actual chroot would be:
-	// if runtime.GOOS == "linux" {
-	//     if cmd.SysProcAttr == nil {
-	//         cmd.SysProcAttr = &syscall.SysProcAttr{}
-	//     }
-	//     cmd.SysProcAttr.Chroot = s.chrootPath
-	// }
-	// But this requires root and proper setup
+  /* Actual chroot would be: */
+  /* if runtime.GOOS == "linux" { */
+  /* if cmd.SysProcAttr == nil { */
+  /* cmd.SysProcAttr = &syscall.SysProcAttr{} */
+  /* } */
+  /* cmd.SysProcAttr.Chroot = s.chrootPath */
+  /* } */
+  /* But this requires root and proper setup */
 
 	return nil
 }
 
-// SetTimeout sets a timeout for command execution
-// Note: exec.Cmd doesn't have a settable Context field directly.
-// The context should be set when creating the command with exec.CommandContext
+/* SetTimeout sets a timeout for command execution */
+/* Note: exec.Cmd doesn't have a settable Context field directly. */
+/* The context should be set when creating the command with exec.CommandContext */
 func SetTimeout(cmd *exec.Cmd, timeout time.Duration) *exec.Cmd {
 	ctx, cancel := context.WithTimeout(context.Background(), timeout)
-	// Note: cancel should be called after command completes
-	// For now, we return a new command with context
-	// In practice, use exec.CommandContext when creating the command
+  /* Note: cancel should be called after command completes */
+  /* For now, we return a new command with context */
+  /* In practice, use exec.CommandContext when creating the command */
 	_ = cancel
 	_ = ctx
 	return cmd
diff --git a/NeuronAgent/internal/tools/shell_tool.go b/NeuronAgent/internal/tools/shell_tool.go
index 72989ca..65c5573 100644
--- a/NeuronAgent/internal/tools/shell_tool.go
+++ b/NeuronAgent/internal/tools/shell_tool.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * shell_tool.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/tools/shell_tool.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -12,7 +25,7 @@ import (
 )
 
 type ShellTool struct {
-	allowedCommands []string // Whitelist of allowed commands
+ 	allowedCommands []string /* Whitelist of allowed commands */
 	timeout         time.Duration
 }
 
@@ -27,7 +40,7 @@ func NewShellTool() *ShellTool {
 }
 
 func (t *ShellTool) Execute(ctx context.Context, tool *db.Tool, args map[string]interface{}) (string, error) {
-	// Shell tool is heavily restricted - only allow specific commands
+  /* Shell tool is heavily restricted - only allow specific commands */
 	command, ok := args["command"].(string)
 	if !ok {
 		argKeys := make([]string, 0, len(args))
@@ -38,7 +51,7 @@ func (t *ShellTool) Execute(ctx context.Context, tool *db.Tool, args map[string]
 			tool.Name, len(args), argKeys)
 	}
 
-	// Parse command
+  /* Parse command */
 	parts := strings.Fields(command)
 	if len(parts) == 0 {
 		return "", fmt.Errorf("shell tool execution failed: tool_name='%s', handler_type='shell', command='%s', command_length=%d, validation_error='empty command'",
@@ -47,7 +60,7 @@ func (t *ShellTool) Execute(ctx context.Context, tool *db.Tool, args map[string]
 
 	cmdName := parts[0]
 
-	// Check if command is in allowlist
+  /* Check if command is in allowlist */
 	allowed := false
 	for _, allowedCmd := range t.allowedCommands {
 		if cmdName == allowedCmd {
@@ -65,11 +78,11 @@ func (t *ShellTool) Execute(ctx context.Context, tool *db.Tool, args map[string]
 			tool.Name, commandPreview, len(command), cmdName, t.allowedCommands)
 	}
 
-	// Create context with timeout
+  /* Create context with timeout */
 	ctx, cancel := context.WithTimeout(ctx, t.timeout)
 	defer cancel()
 
-	// Execute command
+  /* Execute command */
 	cmd := exec.CommandContext(ctx, cmdName, parts[1:]...)
 	output, err := cmd.CombinedOutput()
 	exitCode := 0
diff --git a/NeuronAgent/internal/tools/sql_tool.go b/NeuronAgent/internal/tools/sql_tool.go
index 83f4484..1e12daf 100644
--- a/NeuronAgent/internal/tools/sql_tool.go
+++ b/NeuronAgent/internal/tools/sql_tool.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * sql_tool.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/tools/sql_tool.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -14,7 +27,7 @@ type SQLTool struct {
 }
 
 func NewSQLTool(queries *db.Queries) *SQLTool {
-	// DB will be set by the registry during initialization
+  /* DB will be set by the registry during initialization */
 	return &SQLTool{db: nil}
 }
 
@@ -29,7 +42,7 @@ func (t *SQLTool) Execute(ctx context.Context, tool *db.Tool, args map[string]in
 			tool.Name, len(args), argKeys)
 	}
 
-	// Security: Only allow SELECT, EXPLAIN, and schema introspection queries
+  /* Security: Only allow SELECT, EXPLAIN, and schema introspection queries */
 	queryUpper := strings.TrimSpace(strings.ToUpper(query))
 	queryType := "UNKNOWN"
 	if strings.HasPrefix(queryUpper, "SELECT") {
@@ -53,7 +66,7 @@ func (t *SQLTool) Execute(ctx context.Context, tool *db.Tool, args map[string]in
 			tool.Name, queryType, queryPreview, len(query))
 	}
 
-	// Check for dangerous keywords
+  /* Check for dangerous keywords */
 	dangerous := []string{"DROP", "DELETE", "UPDATE", "INSERT", "ALTER", "CREATE", "TRUNCATE"}
 	var foundKeywords []string
 	for _, keyword := range dangerous {
@@ -70,7 +83,7 @@ func (t *SQLTool) Execute(ctx context.Context, tool *db.Tool, args map[string]in
 			tool.Name, queryType, queryPreview, len(query), foundKeywords)
 	}
 
-	// Execute query (read-only)
+  /* Execute query (read-only) */
 	if t.db == nil {
 		return "", fmt.Errorf("SQL tool execution failed: tool_name='%s', handler_type='sql', query_type='%s', query_length=%d, database_connection='not_initialized'",
 			tool.Name, queryType, len(query))
@@ -92,7 +105,7 @@ func (t *SQLTool) Execute(ctx context.Context, tool *db.Tool, args map[string]in
 	}
 	defer rows.Close()
 
-	// Convert results to JSON
+  /* Convert results to JSON */
 	var results []map[string]interface{}
 	columns, err := rows.Columns()
 	if err != nil {
diff --git a/NeuronAgent/internal/tools/types.go b/NeuronAgent/internal/tools/types.go
index d4af8e2..89efafc 100644
--- a/NeuronAgent/internal/tools/types.go
+++ b/NeuronAgent/internal/tools/types.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * types.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/tools/types.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -6,13 +19,13 @@ import (
 	"github.com/neurondb/NeuronAgent/internal/db"
 )
 
-// ToolHandler is the interface that all tool handlers must implement
+/* ToolHandler is the interface that all tool handlers must implement */
 type ToolHandler interface {
 	Execute(ctx context.Context, tool *db.Tool, args map[string]interface{}) (string, error)
 	Validate(args map[string]interface{}, schema map[string]interface{}) error
 }
 
-// ExecutionResult represents the result of tool execution
+/* ExecutionResult represents the result of tool execution */
 type ExecutionResult struct {
 	Output string
 	Error  error
diff --git a/NeuronAgent/internal/tools/validator.go b/NeuronAgent/internal/tools/validator.go
index df84111..c6a5eb6 100644
--- a/NeuronAgent/internal/tools/validator.go
+++ b/NeuronAgent/internal/tools/validator.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * validator.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/tools/validator.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -5,7 +18,7 @@ import (
 	"reflect"
 )
 
-// ValidateArgs validates arguments against a JSON Schema
+/* ValidateArgs validates arguments against a JSON Schema */
 func ValidateArgs(args map[string]interface{}, schema map[string]interface{}) error {
 	properties, ok := schema["properties"].(map[string]interface{})
 	if !ok {
@@ -14,7 +27,7 @@ func ValidateArgs(args map[string]interface{}, schema map[string]interface{}) er
 
 	required, _ := schema["required"].([]interface{})
 
-	// Check required fields
+  /* Check required fields */
 	for _, req := range required {
 		reqStr, ok := req.(string)
 		if !ok {
@@ -25,11 +38,11 @@ func ValidateArgs(args map[string]interface{}, schema map[string]interface{}) er
 		}
 	}
 
-	// Validate each argument
+  /* Validate each argument */
 	for key, value := range args {
 		propSchema, exists := properties[key]
 		if !exists {
-			// Allow extra fields (could be strict mode later)
+    /* Allow extra fields (could be strict mode later) */
 			continue
 		}
 
@@ -49,7 +62,7 @@ func ValidateArgs(args map[string]interface{}, schema map[string]interface{}) er
 func validateType(value interface{}, schema map[string]interface{}) error {
 	expectedType, ok := schema["type"].(string)
 	if !ok {
-		return nil // No type constraint
+  		return nil /* No type constraint */
 	}
 
 	actualType := reflect.TypeOf(value).Kind()
@@ -76,7 +89,7 @@ func validateType(value interface{}, schema map[string]interface{}) error {
 			return fmt.Errorf("expected object, got %v", actualType)
 		}
 	default:
-		// Unknown type, skip validation
+   /* Unknown type, skip validation */
 	}
 
 	return nil
diff --git a/NeuronAgent/internal/utils/errors.go b/NeuronAgent/internal/utils/errors.go
index 136fb2a..bc42145 100644
--- a/NeuronAgent/internal/utils/errors.go
+++ b/NeuronAgent/internal/utils/errors.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * errors.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/utils/errors.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package utils
 
 import (
@@ -5,7 +18,7 @@ import (
 	"strings"
 )
 
-// SanitizeValue sanitizes sensitive data in error messages
+/* SanitizeValue sanitizes sensitive data in error messages */
 func SanitizeValue(value interface{}) string {
 	if value == nil {
 		return "<nil>"
@@ -13,7 +26,7 @@ func SanitizeValue(value interface{}) string {
 	
 	str := fmt.Sprintf("%v", value)
 	
-	// Sanitize passwords, tokens, and other sensitive data
+  /* Sanitize passwords, tokens, and other sensitive data */
 	if strings.Contains(strings.ToLower(str), "password") ||
 		strings.Contains(strings.ToLower(str), "token") ||
 		strings.Contains(strings.ToLower(str), "secret") ||
@@ -21,7 +34,7 @@ func SanitizeValue(value interface{}) string {
 		return "<redacted>"
 	}
 	
-	// Truncate long strings
+  /* Truncate long strings */
 	if len(str) > 100 {
 		return str[:100] + "..."
 	}
@@ -29,14 +42,14 @@ func SanitizeValue(value interface{}) string {
 	return str
 }
 
-// FormatConnectionInfo formats database connection information for error messages
+/* FormatConnectionInfo formats database connection information for error messages */
 func FormatConnectionInfo(host string, port int, database string, user string) string {
 	return fmt.Sprintf("database '%s' on host '%s:%d' as user '%s'", database, host, port, user)
 }
 
-// FormatQueryContext formats query execution context for error messages
+/* FormatQueryContext formats query execution context for error messages */
 func FormatQueryContext(query string, paramCount int, operation string, table string) string {
-	// Truncate long queries
+  /* Truncate long queries */
 	queryPreview := query
 	if len(queryPreview) > 200 {
 		queryPreview = queryPreview[:200] + "..."
@@ -55,7 +68,7 @@ func FormatQueryContext(query string, paramCount int, operation string, table st
 	return strings.Join(parts, ", ")
 }
 
-// FormatToolContext formats tool execution context for error messages
+/* FormatToolContext formats tool execution context for error messages */
 func FormatToolContext(toolName string, handlerType string, argCount int, argKeys []string) string {
 	parts := []string{
 		fmt.Sprintf("tool_name='%s'", toolName),
@@ -74,7 +87,7 @@ func FormatToolContext(toolName string, handlerType string, argCount int, argKey
 	return strings.Join(parts, ", ")
 }
 
-// FormatParamValues formats parameter values for error messages (sanitized)
+/* FormatParamValues formats parameter values for error messages (sanitized) */
 func FormatParamValues(params []interface{}) string {
 	if len(params) == 0 {
 		return "[]"
@@ -82,7 +95,7 @@ func FormatParamValues(params []interface{}) string {
 	
 	var values []string
 	for i, param := range params {
-		if i >= 5 { // Limit to first 5 params
+  		if i >= 5 { /* Limit to first 5 params */
 			values = append(values, fmt.Sprintf("... (%d more)", len(params)-5))
 			break
 		}
@@ -92,7 +105,7 @@ func FormatParamValues(params []interface{}) string {
 	return "[" + strings.Join(values, ", ") + "]"
 }
 
-// BuildErrorContext builds a detailed error message with context
+/* BuildErrorContext builds a detailed error message with context */
 func BuildErrorContext(operation string, resourceType string, resourceName string, resourceID string, details string, err error) string {
 	parts := []string{operation}
 	
diff --git a/NeuronAgent/internal/utils/json.go b/NeuronAgent/internal/utils/json.go
index e72e2ef..cc10f66 100644
--- a/NeuronAgent/internal/utils/json.go
+++ b/NeuronAgent/internal/utils/json.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * json.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/utils/json.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package utils
 
 import (
@@ -5,17 +18,17 @@ import (
 	"fmt"
 )
 
-// MarshalJSON safely marshals an object to JSON
+/* MarshalJSON safely marshals an object to JSON */
 func MarshalJSON(v interface{}) ([]byte, error) {
 	return json.Marshal(v)
 }
 
-// UnmarshalJSON safely unmarshals JSON to an object
+/* UnmarshalJSON safely unmarshals JSON to an object */
 func UnmarshalJSON(data []byte, v interface{}) error {
 	return json.Unmarshal(data, v)
 }
 
-// MarshalJSONString marshals to JSON string
+/* MarshalJSONString marshals to JSON string */
 func MarshalJSONString(v interface{}) (string, error) {
 	data, err := json.Marshal(v)
 	if err != nil {
@@ -24,32 +37,32 @@ func MarshalJSONString(v interface{}) (string, error) {
 	return string(data), nil
 }
 
-// UnmarshalJSONString unmarshals from JSON string
+/* UnmarshalJSONString unmarshals from JSON string */
 func UnmarshalJSONString(s string, v interface{}) error {
 	return json.Unmarshal([]byte(s), v)
 }
 
-// PrettyJSON returns pretty-printed JSON
+/* PrettyJSON returns pretty-printed JSON */
 func PrettyJSON(v interface{}) ([]byte, error) {
 	return json.MarshalIndent(v, "", "  ")
 }
 
-// ValidateJSON validates JSON string
+/* ValidateJSON validates JSON string */
 func ValidateJSON(s string) error {
 	var v interface{}
 	return json.Unmarshal([]byte(s), &v)
 }
 
-// MergeJSON merges two JSON objects
+/* MergeJSON merges two JSON objects */
 func MergeJSON(dst, src map[string]interface{}) map[string]interface{} {
 	result := make(map[string]interface{})
 	
-	// Copy dst
+  /* Copy dst */
 	for k, v := range dst {
 		result[k] = v
 	}
 	
-	// Merge src (overwrites dst values)
+  /* Merge src (overwrites dst values) */
 	for k, v := range src {
 		result[k] = v
 	}
@@ -57,7 +70,7 @@ func MergeJSON(dst, src map[string]interface{}) map[string]interface{} {
 	return result
 }
 
-// GetJSONField gets a field from JSON object
+/* GetJSONField gets a field from JSON object */
 func GetJSONField(data map[string]interface{}, path ...string) (interface{}, error) {
 	current := data
 	for i, key := range path {
diff --git a/NeuronAgent/internal/utils/time.go b/NeuronAgent/internal/utils/time.go
index e366be2..d7aed92 100644
--- a/NeuronAgent/internal/utils/time.go
+++ b/NeuronAgent/internal/utils/time.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * time.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/utils/time.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package utils
 
 import (
@@ -13,17 +26,17 @@ const (
 	DateTimeFormat    = "2006-01-02 15:04:05"
 )
 
-// FormatTime formats time using ISO8601 format
+/* FormatTime formats time using ISO8601 format */
 func FormatTime(t time.Time) string {
 	return t.Format(ISO8601Format)
 }
 
-// ParseTime parses time from ISO8601 format
+/* ParseTime parses time from ISO8601 format */
 func ParseTime(s string) (time.Time, error) {
 	return time.Parse(ISO8601Format, s)
 }
 
-// FormatDuration formats duration as human-readable string
+/* FormatDuration formats duration as human-readable string */
 func FormatDuration(d time.Duration) string {
 	if d < time.Second {
 		return fmt.Sprintf("%dms", d.Milliseconds())
@@ -37,32 +50,32 @@ func FormatDuration(d time.Duration) string {
 	return fmt.Sprintf("%.2fh", d.Hours())
 }
 
-// ParseDuration parses duration string
+/* ParseDuration parses duration string */
 func ParseDuration(s string) (time.Duration, error) {
 	return time.ParseDuration(s)
 }
 
-// Now returns current time in UTC
+/* Now returns current time in UTC */
 func Now() time.Time {
 	return time.Now().UTC()
 }
 
-// UnixTimestamp returns Unix timestamp
+/* UnixTimestamp returns Unix timestamp */
 func UnixTimestamp(t time.Time) int64 {
 	return t.Unix()
 }
 
-// FromUnixTimestamp creates time from Unix timestamp
+/* FromUnixTimestamp creates time from Unix timestamp */
 func FromUnixTimestamp(ts int64) time.Time {
 	return time.Unix(ts, 0)
 }
 
-// IsExpired checks if a time is before now
+/* IsExpired checks if a time is before now */
 func IsExpired(t time.Time) bool {
 	return t.Before(time.Now())
 }
 
-// TimeAgo returns human-readable time ago string
+/* TimeAgo returns human-readable time ago string */
 func TimeAgo(t time.Time) string {
 	duration := time.Since(t)
 	
diff --git a/NeuronAgent/internal/utils/uuid.go b/NeuronAgent/internal/utils/uuid.go
index 4f5c888..c10bf5e 100644
--- a/NeuronAgent/internal/utils/uuid.go
+++ b/NeuronAgent/internal/utils/uuid.go
@@ -1,31 +1,44 @@
+/*-------------------------------------------------------------------------
+ *
+ * uuid.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/utils/uuid.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package utils
 
 import (
 	"github.com/google/uuid"
 )
 
-// GenerateUUID generates a new UUID
+/* GenerateUUID generates a new UUID */
 func GenerateUUID() uuid.UUID {
 	return uuid.New()
 }
 
-// GenerateUUIDString generates a new UUID as string
+/* GenerateUUIDString generates a new UUID as string */
 func GenerateUUIDString() string {
 	return uuid.New().String()
 }
 
-// ParseUUID parses a UUID string
+/* ParseUUID parses a UUID string */
 func ParseUUID(s string) (uuid.UUID, error) {
 	return uuid.Parse(s)
 }
 
-// IsValidUUID checks if a string is a valid UUID
+/* IsValidUUID checks if a string is a valid UUID */
 func IsValidUUID(s string) bool {
 	_, err := uuid.Parse(s)
 	return err == nil
 }
 
-// MustParseUUID parses a UUID string or panics
+/* MustParseUUID parses a UUID string or panics */
 func MustParseUUID(s string) uuid.UUID {
 	return uuid.MustParse(s)
 }
diff --git a/NeuronAgent/internal/utils/validation.go b/NeuronAgent/internal/utils/validation.go
index 3fd2919..c223033 100644
--- a/NeuronAgent/internal/utils/validation.go
+++ b/NeuronAgent/internal/utils/validation.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * validation.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/internal/utils/validation.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package utils
 
 import (
@@ -12,12 +25,12 @@ var (
 	urlRegex   = regexp.MustCompile(`^https?://[^\s/$.?#].[^\s]*$`)
 )
 
-// ValidateEmail validates an email address
+/* ValidateEmail validates an email address */
 func ValidateEmail(email string) bool {
 	return emailRegex.MatchString(email)
 }
 
-// ValidateURL validates a URL
+/* ValidateURL validates a URL */
 func ValidateURL(urlStr string) bool {
 	if !urlRegex.MatchString(urlStr) {
 		return false
@@ -26,28 +39,28 @@ func ValidateURL(urlStr string) bool {
 	return err == nil
 }
 
-// ValidateRequired checks if a string is not empty
+/* ValidateRequired checks if a string is not empty */
 func ValidateRequired(s string) bool {
 	return strings.TrimSpace(s) != ""
 }
 
-// ValidateLength checks if string length is within range
+/* ValidateLength checks if string length is within range */
 func ValidateLength(s string, min, max int) bool {
 	length := len(strings.TrimSpace(s))
 	return length >= min && length <= max
 }
 
-// ValidateMinLength checks if string meets minimum length
+/* ValidateMinLength checks if string meets minimum length */
 func ValidateMinLength(s string, min int) bool {
 	return len(strings.TrimSpace(s)) >= min
 }
 
-// ValidateMaxLength checks if string doesn't exceed maximum length
+/* ValidateMaxLength checks if string doesn't exceed maximum length */
 func ValidateMaxLength(s string, max int) bool {
 	return len(strings.TrimSpace(s)) <= max
 }
 
-// ValidateIn checks if value is in allowed list
+/* ValidateIn checks if value is in allowed list */
 func ValidateIn(value string, allowed ...string) bool {
 	for _, a := range allowed {
 		if value == a {
@@ -57,7 +70,7 @@ func ValidateIn(value string, allowed ...string) bool {
 	return false
 }
 
-// ValidateRegex validates string against regex pattern
+/* ValidateRegex validates string against regex pattern */
 func ValidateRegex(s, pattern string) bool {
 	re, err := regexp.Compile(pattern)
 	if err != nil {
@@ -66,22 +79,22 @@ func ValidateRegex(s, pattern string) bool {
 	return re.MatchString(s)
 }
 
-// ValidateUUID validates UUID format
+/* ValidateUUID validates UUID format */
 func ValidateUUID(s string) bool {
 	return IsValidUUID(s)
 }
 
-// ValidateIntRange validates integer is in range
+/* ValidateIntRange validates integer is in range */
 func ValidateIntRange(n, min, max int) bool {
 	return n >= min && n <= max
 }
 
-// ValidateFloatRange validates float is in range
+/* ValidateFloatRange validates float is in range */
 func ValidateFloatRange(f, min, max float64) bool {
 	return f >= min && f <= max
 }
 
-// ValidateAll validates all validators and returns first error
+/* ValidateAll validates all validators and returns first error */
 func ValidateAll(validators ...func() error) error {
 	for _, validator := range validators {
 		if err := validator(); err != nil {
@@ -91,7 +104,7 @@ func ValidateAll(validators ...func() error) error {
 	return nil
 }
 
-// ValidateEmailWithError validates email and returns error
+/* ValidateEmailWithError validates email and returns error */
 func ValidateEmailWithError(email string) error {
 	if !ValidateEmail(email) {
 		return fmt.Errorf("invalid email format: %s", email)
@@ -99,7 +112,7 @@ func ValidateEmailWithError(email string) error {
 	return nil
 }
 
-// ValidateURLWithError validates URL and returns error
+/* ValidateURLWithError validates URL and returns error */
 func ValidateURLWithError(urlStr string) error {
 	if !ValidateURL(urlStr) {
 		return fmt.Errorf("invalid URL format: %s", urlStr)
@@ -107,7 +120,7 @@ func ValidateURLWithError(urlStr string) error {
 	return nil
 }
 
-// ValidateRequiredWithError validates required field and returns error
+/* ValidateRequiredWithError validates required field and returns error */
 func ValidateRequiredWithError(s, fieldName string) error {
 	if !ValidateRequired(s) {
 		return fmt.Errorf("%s is required", fieldName)
diff --git a/NeuronAgent/pkg/neurondb/client.go b/NeuronAgent/pkg/neurondb/client.go
index d05b620..7d2f2a0 100644
--- a/NeuronAgent/pkg/neurondb/client.go
+++ b/NeuronAgent/pkg/neurondb/client.go
@@ -1,16 +1,29 @@
+/*-------------------------------------------------------------------------
+ *
+ * client.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/pkg/neurondb/client.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package neurondb
 
 import (
 	"github.com/jmoiron/sqlx"
 )
 
-// Client provides a unified interface to NeuronDB functions
+/* Client provides a unified interface to NeuronDB functions */
 type Client struct {
 	Embedding *EmbeddingClient
 	LLM       *LLMClient
 }
 
-// NewClient creates a new NeuronDB client
+/* NewClient creates a new NeuronDB client */
 func NewClient(db *sqlx.DB) *Client {
 	return &Client{
 		Embedding: NewEmbeddingClient(db),
diff --git a/NeuronAgent/pkg/neurondb/embedding.go b/NeuronAgent/pkg/neurondb/embedding.go
index 0684fc2..89c6763 100644
--- a/NeuronAgent/pkg/neurondb/embedding.go
+++ b/NeuronAgent/pkg/neurondb/embedding.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * embedding.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/pkg/neurondb/embedding.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package neurondb
 
 import (
@@ -8,17 +21,17 @@ import (
 	"github.com/jmoiron/sqlx"
 )
 
-// EmbeddingClient handles embedding generation via NeuronDB
+/* EmbeddingClient handles embedding generation via NeuronDB */
 type EmbeddingClient struct {
 	db *sqlx.DB
 }
 
-// NewEmbeddingClient creates a new embedding client
+/* NewEmbeddingClient creates a new embedding client */
 func NewEmbeddingClient(db *sqlx.DB) *EmbeddingClient {
 	return &EmbeddingClient{db: db}
 }
 
-// Embed generates an embedding for the given text using the specified model
+/* Embed generates an embedding for the given text using the specified model */
 func (c *EmbeddingClient) Embed(ctx context.Context, text string, model string) (Vector, error) {
 	var embeddingStr string
 	query := `SELECT neurondb_embed($1, $2)::text AS embedding`
@@ -29,7 +42,7 @@ func (c *EmbeddingClient) Embed(ctx context.Context, text string, model string)
 			model, len(text), err)
 	}
 
-	// Parse vector string format [1.0, 2.0, 3.0] to []float32
+  /* Parse vector string format [1.0, 2.0, 3.0] to []float32 */
 	embedding, err := parseVector(embeddingStr)
 	if err != nil {
 		embeddingStrPreview := embeddingStr
@@ -43,19 +56,19 @@ func (c *EmbeddingClient) Embed(ctx context.Context, text string, model string)
 	return embedding, nil
 }
 
-// EmbedBatch generates embeddings for multiple texts
+/* EmbedBatch generates embeddings for multiple texts */
 func (c *EmbeddingClient) EmbedBatch(ctx context.Context, texts []string, model string) ([]Vector, error) {
-	// Use array format for batch embedding if available
+  /* Use array format for batch embedding if available */
 	query := `SELECT neurondb_embed_batch($1::text[], $2) AS embeddings`
 	
 	var embeddingsStr string
 	err := c.db.GetContext(ctx, &embeddingsStr, query, texts, model)
 	if err != nil {
-		// Fallback to individual embeddings if batch function not available
+   /* Fallback to individual embeddings if batch function not available */
 		return c.embedBatchFallback(ctx, texts, model)
 	}
 
-	// Parse array of vectors
+  /* Parse array of vectors */
 	embeddings, err := parseVectorArray(embeddingsStr)
 	if err != nil {
 		embeddingsStrPreview := embeddingsStr
@@ -69,7 +82,7 @@ func (c *EmbeddingClient) EmbedBatch(ctx context.Context, texts []string, model
 	return embeddings, nil
 }
 
-// embedBatchFallback generates embeddings one by one
+/* embedBatchFallback generates embeddings one by one */
 func (c *EmbeddingClient) embedBatchFallback(ctx context.Context, texts []string, model string) ([]Vector, error) {
 	embeddings := make([]Vector, len(texts))
 	for i, text := range texts {
@@ -83,15 +96,15 @@ func (c *EmbeddingClient) embedBatchFallback(ctx context.Context, texts []string
 	return embeddings, nil
 }
 
-// parseVector parses a vector string like "[1.0, 2.0, 3.0]" into a Vector
+/* parseVector parses a vector string like "[1.0, 2.0, 3.0]" into a Vector */
 func parseVector(s string) (Vector, error) {
-	// Remove brackets
+  /* Remove brackets */
 	if len(s) < 2 || s[0] != '[' || s[len(s)-1] != ']' {
 		return nil, fmt.Errorf("invalid vector format: %s", s)
 	}
 	s = s[1 : len(s)-1]
 
-	// Split by comma
+  /* Split by comma */
 	var values []float32
 	start := 0
 	for i := 0; i <= len(s); i++ {
@@ -111,12 +124,12 @@ func parseVector(s string) (Vector, error) {
 	return Vector(values), nil
 }
 
-// parseVectorArray parses an array of vectors from PostgreSQL array format
-// Format: "{[1.0,2.0],[3.0,4.0]}" or "[1.0,2.0],[3.0,4.0]"
+/* parseVectorArray parses an array of vectors from PostgreSQL array format */
+/* Format: "{[1.0,2.0],[3.0,4.0]}" or "[1.0,2.0],[3.0,4.0]" */
 func parseVectorArray(s string) ([]Vector, error) {
 	s = strings.TrimSpace(s)
 	
-	// Remove outer braces if present
+  /* Remove outer braces if present */
 	if len(s) > 0 && s[0] == '{' && s[len(s)-1] == '}' {
 		s = s[1 : len(s)-1]
 	}
@@ -125,28 +138,28 @@ func parseVectorArray(s string) ([]Vector, error) {
 		return []Vector{}, nil
 	}
 	
-	// Split by "],[" to separate vectors
-	// Handle both "],[ and ], [" patterns
+  /* Split by "],[" to separate vectors */
+  /* Handle both "],[ and ], [" patterns */
 	parts := strings.Split(s, "],[")
 	var vectors []Vector
 	
 	for _, part := range parts {
-		// Clean up brackets
+   /* Clean up brackets */
 		part = strings.TrimSpace(part)
 		if len(part) == 0 {
 			continue
 		}
 		
-		// Remove leading [ if present
+   /* Remove leading [ if present */
 		if len(part) > 0 && part[0] == '[' {
 			part = part[1:]
 		}
-		// Remove trailing ] if present
+   /* Remove trailing ] if present */
 		if len(part) > 0 && part[len(part)-1] == ']' {
 			part = part[:len(part)-1]
 		}
 		
-		// Add brackets back for parseVector
+   /* Add brackets back for parseVector */
 		vectorStr := "[" + part + "]"
 		vec, err := parseVector(vectorStr)
 		if err != nil {
diff --git a/NeuronAgent/pkg/neurondb/llm.go b/NeuronAgent/pkg/neurondb/llm.go
index 93be4d6..c023e90 100644
--- a/NeuronAgent/pkg/neurondb/llm.go
+++ b/NeuronAgent/pkg/neurondb/llm.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * llm.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/pkg/neurondb/llm.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package neurondb
 
 import (
@@ -10,19 +23,19 @@ import (
 	"github.com/jmoiron/sqlx"
 )
 
-// LLMClient handles LLM generation via NeuronDB
+/* LLMClient handles LLM generation via NeuronDB */
 type LLMClient struct {
 	db *sqlx.DB
 }
 
-// NewLLMClient creates a new LLM client
+/* NewLLMClient creates a new LLM client */
 func NewLLMClient(db *sqlx.DB) *LLMClient {
 	return &LLMClient{db: db}
 }
 
-// Generate generates text using the LLM with the given prompt and config
+/* Generate generates text using the LLM with the given prompt and config */
 func (c *LLMClient) Generate(ctx context.Context, prompt string, config LLMConfig) (*LLMGenerateResult, error) {
-	// Build parameters JSON
+  /* Build parameters JSON */
 	params := make(map[string]interface{})
 	if config.Temperature != nil {
 		params["temperature"] = *config.Temperature
@@ -40,13 +53,13 @@ func (c *LLMClient) Generate(ctx context.Context, prompt string, config LLMConfi
 			config.Model, len(prompt), err)
 	}
 
-	// Try neurondb_llm_generate first, fallback to neurondb_llm_complete
+  /* Try neurondb_llm_generate first, fallback to neurondb_llm_complete */
 	var output string
 	query := `SELECT neurondb_llm_generate($1, $2, $3::jsonb) AS output`
 	
 	err = c.db.GetContext(ctx, &output, query, config.Model, prompt, paramsJSON)
 	if err != nil {
-		// Fallback to neurondb_llm_complete if available
+   /* Fallback to neurondb_llm_complete if available */
 		query = `SELECT neurondb_llm_complete($1, $2, $3::jsonb) AS output`
 		err = c.db.GetContext(ctx, &output, query, config.Model, prompt, paramsJSON)
 		if err != nil {
@@ -70,15 +83,15 @@ func (c *LLMClient) Generate(ctx context.Context, prompt string, config LLMConfi
 
 	return &LLMGenerateResult{
 		Output:      output,
-		TokensUsed:  0, // Token count would need to be extracted from response
+  		TokensUsed:  0, /* Token count would need to be extracted from response */
 		FinishReason: "stop",
 	}, nil
 }
 
-// GenerateStream generates text with streaming support
-// Uses a cursor-based approach to stream results chunk by chunk
+/* GenerateStream generates text with streaming support */
+/* Uses a cursor-based approach to stream results chunk by chunk */
 func (c *LLMClient) GenerateStream(ctx context.Context, prompt string, config LLMConfig, writer io.Writer) error {
-	// Build parameters JSON
+  /* Build parameters JSON */
 	params := make(map[string]interface{})
 	if config.Temperature != nil {
 		params["temperature"] = *config.Temperature
@@ -96,12 +109,12 @@ func (c *LLMClient) GenerateStream(ctx context.Context, prompt string, config LL
 		return fmt.Errorf("failed to marshal params: %w", err)
 	}
 
-	// Try streaming query - if not supported, fall back to chunked writes
+  /* Try streaming query - if not supported, fall back to chunked writes */
 	query := `SELECT neurondb_llm_generate_stream($1, $2, $3::jsonb) AS chunk`
 	
 	rows, err := c.db.QueryContext(ctx, query, config.Model, prompt, paramsJSON)
 	if err != nil {
-		// Fallback: generate full response and write in chunks
+   /* Fallback: generate full response and write in chunks */
 		result, err := c.Generate(ctx, prompt, config)
 		if err != nil {
 			promptTokens := len(strings.Split(prompt, " "))
@@ -109,7 +122,7 @@ func (c *LLMClient) GenerateStream(ctx context.Context, prompt string, config LL
 				config.Model, len(prompt), promptTokens, err)
 		}
 		
-		// Write in chunks to simulate streaming
+   /* Write in chunks to simulate streaming */
 		chunkSize := 100
 		output := []byte(result.Output)
 		for i := 0; i < len(output); i += chunkSize {
@@ -120,7 +133,7 @@ func (c *LLMClient) GenerateStream(ctx context.Context, prompt string, config LL
 			if _, err := writer.Write(output[i:end]); err != nil {
 				return err
 			}
-			// Small delay to simulate streaming
+    /* Small delay to simulate streaming */
 			select {
 			case <-ctx.Done():
 				return ctx.Err()
@@ -131,7 +144,7 @@ func (c *LLMClient) GenerateStream(ctx context.Context, prompt string, config LL
 	}
 	defer rows.Close()
 
-	// Stream chunks from database
+  /* Stream chunks from database */
 		for rows.Next() {
 		var chunk string
 		if err := rows.Scan(&chunk); err != nil {
@@ -143,7 +156,7 @@ func (c *LLMClient) GenerateStream(ctx context.Context, prompt string, config LL
 			return err
 		}
 		
-		// Check context
+   /* Check context */
 		select {
 		case <-ctx.Done():
 			return ctx.Err()
@@ -154,52 +167,52 @@ func (c *LLMClient) GenerateStream(ctx context.Context, prompt string, config LL
 	return rows.Err()
 }
 
-// GenerateWithTools generates text with tool calling support
+/* GenerateWithTools generates text with tool calling support */
 func (c *LLMClient) GenerateWithTools(ctx context.Context, prompt string, config LLMConfig, tools []ToolDefinition) (*LLMGenerateResult, []ToolCall, error) {
-	// Add tools to the prompt or use a tool-aware function
-	// This is a simplified version - full implementation would handle tool schemas
+  /* Add tools to the prompt or use a tool-aware function */
+  /* This is a simplified version - full implementation would handle tool schemas */
 	result, err := c.Generate(ctx, prompt, config)
 	if err != nil {
 		return nil, nil, err
 	}
 
-	// Parse tool calls from response (would need proper parsing logic)
+  /* Parse tool calls from response (would need proper parsing logic) */
 	toolCalls := parseToolCalls(result.Output)
 
 	return result, toolCalls, nil
 }
 
-// ToolDefinition represents a tool that can be called by the LLM
+/* ToolDefinition represents a tool that can be called by the LLM */
 type ToolDefinition struct {
 	Name        string                 `json:"name"`
 	Description string                 `json:"description"`
 	Parameters  map[string]interface{} `json:"parameters"`
 }
 
-// ToolCall represents a tool call from the LLM
+/* ToolCall represents a tool call from the LLM */
 type ToolCall struct {
 	ID        string                 `json:"id"`
 	Name      string                 `json:"name"`
 	Arguments map[string]interface{} `json:"arguments"`
 }
 
-// parseToolCalls parses tool calls from LLM response
-// Attempts to extract tool calls from various formats
+/* parseToolCalls parses tool calls from LLM response */
+/* Attempts to extract tool calls from various formats */
 func parseToolCalls(output string) []ToolCall {
 	var toolCalls []ToolCall
 	
-	// Try to find JSON tool calls in the response
-	// Look for patterns like {"tool_calls": [...]} or "function_call": {...}
+  /* Try to find JSON tool calls in the response */
+  /* Look for patterns like {"tool_calls": [...]} or "function_call": {...} */
 	
-	// Method 1: Look for complete JSON object with tool_calls
+  /* Method 1: Look for complete JSON object with tool_calls */
 	if strings.Contains(output, "tool_calls") {
-		// Try to extract JSON object
+   /* Try to extract JSON object */
 		start := strings.Index(output, `"tool_calls"`)
 		if start != -1 {
-			// Find the opening brace before tool_calls
+    /* Find the opening brace before tool_calls */
 			objStart := strings.LastIndex(output[:start], "{")
 			if objStart != -1 {
-				// Find matching closing brace
+     /* Find matching closing brace */
 				braceCount := 0
 				objEnd := -1
 				for i := objStart; i < len(output); i++ {
@@ -232,7 +245,7 @@ func parseToolCalls(output string) []ToolCall {
 										if argsMap, ok := args.(map[string]interface{}); ok {
 											call.Arguments = argsMap
 										} else if argsStr, ok := args.(string); ok {
-											// Try to parse JSON string
+            /* Try to parse JSON string */
 											var argsMap map[string]interface{}
 											if err := json.Unmarshal([]byte(argsStr), &argsMap); err == nil {
 												call.Arguments = argsMap
@@ -252,9 +265,9 @@ func parseToolCalls(output string) []ToolCall {
 		}
 	}
 	
-	// Method 2: Look for function_call pattern
+  /* Method 2: Look for function_call pattern */
 	if len(toolCalls) == 0 && strings.Contains(output, "function_call") {
-		// Similar extraction logic for function_call format
+   /* Similar extraction logic for function_call format */
 		start := strings.Index(output, `"function_call"`)
 		if start != -1 {
 			objStart := strings.LastIndex(output[:start], "{")
diff --git a/NeuronAgent/pkg/neurondb/types.go b/NeuronAgent/pkg/neurondb/types.go
index 1f95aae..78f1cef 100644
--- a/NeuronAgent/pkg/neurondb/types.go
+++ b/NeuronAgent/pkg/neurondb/types.go
@@ -1,22 +1,35 @@
+/*-------------------------------------------------------------------------
+ *
+ * types.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/pkg/neurondb/types.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package neurondb
 
-// Vector represents a NeuronDB vector type
+/* Vector represents a NeuronDB vector type */
 type Vector []float32
 
-// EmbeddingResult contains the result of an embedding operation
+/* EmbeddingResult contains the result of an embedding operation */
 type EmbeddingResult struct {
 	Embedding Vector
 	Dimension int
 }
 
-// LLMGenerateResult contains the result of an LLM generation
+/* LLMGenerateResult contains the result of an LLM generation */
 type LLMGenerateResult struct {
 	Output      string
 	TokensUsed  int
 	FinishReason string
 }
 
-// LLMConfig contains configuration for LLM generation
+/* LLMConfig contains configuration for LLM generation */
 type LLMConfig struct {
 	Model       string
 	Temperature *float64
diff --git a/NeuronAgent/pkg/neurondb/vector.go b/NeuronAgent/pkg/neurondb/vector.go
index d030d4c..6969f2b 100644
--- a/NeuronAgent/pkg/neurondb/vector.go
+++ b/NeuronAgent/pkg/neurondb/vector.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * vector.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronAgent/pkg/neurondb/vector.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package neurondb
 
 import (
@@ -5,10 +18,10 @@ import (
 	"math"
 )
 
-// Distance calculates the distance between two vectors
+/* Distance calculates the distance between two vectors */
 func Distance(a, b Vector, metric string) float64 {
 	if len(a) != len(b) {
-		return math.Inf(1) // Return infinity for mismatched dimensions
+  		return math.Inf(1) /* Return infinity for mismatched dimensions */
 	}
 
 	switch metric {
@@ -19,11 +32,11 @@ func Distance(a, b Vector, metric string) float64 {
 	case "inner_product", "dot":
 		return InnerProduct(a, b)
 	default:
-		return L2Distance(a, b) // Default to L2
+  		return L2Distance(a, b) /* Default to L2 */
 	}
 }
 
-// DistanceWithError calculates the distance between two vectors and returns detailed error
+/* DistanceWithError calculates the distance between two vectors and returns detailed error */
 func DistanceWithError(a, b Vector, metric string) (float64, error) {
 	if len(a) != len(b) {
 		return math.Inf(1), fmt.Errorf("vector distance calculation failed: vector_a_dimension=%d, vector_b_dimension=%d, metric='%s', error='dimension mismatch'",
@@ -38,11 +51,11 @@ func DistanceWithError(a, b Vector, metric string) (float64, error) {
 	case "inner_product", "dot":
 		return InnerProduct(a, b), nil
 	default:
-		return L2Distance(a, b), nil // Default to L2
+  		return L2Distance(a, b), nil /* Default to L2 */
 	}
 }
 
-// CosineDistance calculates cosine distance (1 - cosine similarity)
+/* CosineDistance calculates cosine distance (1 - cosine similarity) */
 func CosineDistance(a, b Vector) float64 {
 	dot := 0.0
 	normA := 0.0
@@ -62,7 +75,7 @@ func CosineDistance(a, b Vector) float64 {
 	return 1.0 - similarity
 }
 
-// L2Distance calculates L2 (Euclidean) distance
+/* L2Distance calculates L2 (Euclidean) distance */
 func L2Distance(a, b Vector) float64 {
 	sum := 0.0
 	for i := range a {
@@ -72,7 +85,7 @@ func L2Distance(a, b Vector) float64 {
 	return math.Sqrt(sum)
 }
 
-// InnerProduct calculates inner product (negative dot product for distance)
+/* InnerProduct calculates inner product (negative dot product for distance) */
 func InnerProduct(a, b Vector) float64 {
 	dot := 0.0
 	for i := range a {
@@ -81,7 +94,7 @@ func InnerProduct(a, b Vector) float64 {
 	return -dot
 }
 
-// Normalize normalizes a vector to unit length
+/* Normalize normalizes a vector to unit length */
 func Normalize(v Vector) Vector {
 	norm := 0.0
 	for _, val := range v {
@@ -90,7 +103,7 @@ func Normalize(v Vector) Vector {
 	norm = math.Sqrt(norm)
 
 	if norm == 0 {
-		return v // Return original if zero vector
+  		return v /* Return original if zero vector */
 	}
 
 	normalized := make(Vector, len(v))
@@ -100,7 +113,7 @@ func Normalize(v Vector) Vector {
 	return normalized
 }
 
-// NormalizeWithError normalizes a vector to unit length and returns detailed error
+/* NormalizeWithError normalizes a vector to unit length and returns detailed error */
 func NormalizeWithError(v Vector) (Vector, error) {
 	if len(v) == 0 {
 		return nil, fmt.Errorf("vector normalization failed: vector_dimension=0, error='empty vector'")
@@ -113,7 +126,7 @@ func NormalizeWithError(v Vector) (Vector, error) {
 	norm = math.Sqrt(norm)
 
 	if norm == 0 {
-		return v, nil // Return original if zero vector
+  		return v, nil /* Return original if zero vector */
 	}
 
 	normalized := make(Vector, len(v))
diff --git a/NeuronAgent/start_server.sh b/NeuronAgent/start_server.sh
index c577099..b9c4c76 100755
--- a/NeuronAgent/start_server.sh
+++ b/NeuronAgent/start_server.sh
@@ -19,3 +19,4 @@ go run cmd/agent-server/main.go
 
 
 
+
diff --git a/NeuronAgent/test_auth_debug.sh b/NeuronAgent/test_auth_debug.sh
index 7e06156..d289b6e 100644
--- a/NeuronAgent/test_auth_debug.sh
+++ b/NeuronAgent/test_auth_debug.sh
@@ -84,3 +84,4 @@ echo "=== Debug Complete ==="
 
 
 
+
diff --git a/NeuronAgent/test_complete.sh b/NeuronAgent/test_complete.sh
index 33a3863..e101c77 100644
--- a/NeuronAgent/test_complete.sh
+++ b/NeuronAgent/test_complete.sh
@@ -267,3 +267,4 @@ fi
 
 
 
+
diff --git a/NeuronAgent/test_integration_complete.sh b/NeuronAgent/test_integration_complete.sh
index 995fd1f..8131fbb 100644
--- a/NeuronAgent/test_integration_complete.sh
+++ b/NeuronAgent/test_integration_complete.sh
@@ -639,3 +639,4 @@ fi
 
 
 
+
diff --git a/NeuronDB/demo/ML/sql/015_gpu.sql b/NeuronDB/demo/ML/sql/015_gpu.sql
index e6d4d81..2377cf0 100644
--- a/NeuronDB/demo/ML/sql/015_gpu.sql
+++ b/NeuronDB/demo/ML/sql/015_gpu.sql
@@ -40,7 +40,7 @@ GROUP BY i;
 \echo 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•'
 \echo ''
 
-SET neurondb.gpu_enabled = false;
+SET neurondb.compute_mode = false;
 
 \echo 'CPU: Finding top-10 nearest neighbors for 100 queries'
 \echo '  (100 queries Ã— 50,000 vectors Ã— 2048 dims = 5M distances)'
@@ -77,7 +77,7 @@ WHERE rank <= 10;
 \echo 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•'
 \echo ''
 
-SET neurondb.gpu_enabled = true;
+SET neurondb.compute_mode = true;
 SET neurondb.gpu_backend = 'metal';
 
 \echo 'GPU: Finding top-10 nearest neighbors for 100 queries'
@@ -115,7 +115,7 @@ WHERE rank <= 10;
 \echo 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•'
 \echo ''
 
-SET neurondb.gpu_enabled = false;
+SET neurondb.compute_mode = false;
 
 \echo 'CPU: Computing 500,000 distance calculations'
 \echo '  (500 queries Ã— 1,000 targets Ã— 2048 dimensions)'
@@ -149,7 +149,7 @@ FROM (
 \echo 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•'
 \echo ''
 
-SET neurondb.gpu_enabled = true;
+SET neurondb.compute_mode = true;
 SET neurondb.gpu_backend = 'metal';
 
 \echo 'GPU: Computing 500,000 distance calculations'
@@ -184,7 +184,7 @@ FROM (
 \echo 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•'
 \echo ''
 
-SET neurondb.gpu_enabled = false;
+SET neurondb.compute_mode = false;
 
 \echo 'CPU: Computing 1 MILLION distances for clustering'
 \echo '  (1,000 Ã— 1,000 vectors Ã— 2048 dimensions)'
@@ -215,7 +215,7 @@ FROM (
 \echo 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•'
 \echo ''
 
-SET neurondb.gpu_enabled = true;
+SET neurondb.compute_mode = true;
 SET neurondb.gpu_backend = 'metal';
 
 \echo 'GPU: Computing 1 MILLION distances for clustering'
@@ -255,7 +255,7 @@ FROM (
 \echo ''
 \echo 'GPU Configuration:'
 SELECT name, setting FROM pg_settings 
-WHERE name LIKE 'neurondb.gpu%' AND name IN ('neurondb.gpu_enabled', 'neurondb.gpu_backend');
+WHERE name LIKE 'neurondb.gpu%' AND name IN ('neurondb.compute_mode', 'neurondb.gpu_backend');
 \echo ''
 \echo 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•'
 
diff --git a/NeuronDB/demo/run_demo.sh b/NeuronDB/demo/run_demo.sh
index 45404a4..ebde6f8 100755
--- a/NeuronDB/demo/run_demo.sh
+++ b/NeuronDB/demo/run_demo.sh
@@ -76,13 +76,13 @@ mkdir -p "$OUT_DIR"
 echo "Step 2: Configuring $GPU_MODE mode..."
 if [ "$GPU_MODE" = "gpu" ]; then
     $PSQL -p $DB_PORT -U $DB_USER -d $DB_NAME -q <<EOF
-SET neurondb.gpu_enabled = true;
+SET neurondb.compute_mode = true;
 SET neurondb.gpu_backend = 'metal';
 EOF
     echo "âœ“ GPU mode enabled (Metal backend)"
 else
     $PSQL -p $DB_PORT -U $DB_USER -d $DB_NAME -q <<EOF
-SET neurondb.gpu_enabled = false;
+SET neurondb.compute_mode = false;
 EOF
     echo "âœ“ CPU-only mode enabled"
 fi
@@ -171,7 +171,7 @@ if [ "$GPU_MODE" = "gpu" ]; then
     echo "GPU Configuration:"
     $PSQL -p $DB_PORT -U $DB_USER -d $DB_NAME -t -q <<EOF
 SELECT '  ' || name || ': ' || setting FROM pg_settings 
-WHERE name LIKE 'neurondb.gpu%' AND name IN ('neurondb.gpu_enabled', 'neurondb.gpu_backend');
+WHERE name LIKE 'neurondb.gpu%' AND name IN ('neurondb.compute_mode', 'neurondb.gpu_backend');
 EOF
     echo ""
 fi
diff --git a/NeuronDB/docker/README.md b/NeuronDB/docker/README.md
index d1cce80..1642df4 100644
--- a/NeuronDB/docker/README.md
+++ b/NeuronDB/docker/README.md
@@ -206,14 +206,14 @@ During `initdb`, the container sets the following defaults in `postgresql.conf`:
 
 ```conf
 shared_preload_libraries = 'neurondb'
-neurondb.gpu_enabled = off
+neurondb.compute_mode = off
 neurondb.automl.use_gpu = off
 ```
 
 Modify at runtime:
 
 ```sql
-ALTER SYSTEM SET neurondb.gpu_enabled = on;
+ALTER SYSTEM SET neurondb.compute_mode = on;
 SELECT pg_reload_conf();
 ```
 
@@ -429,7 +429,7 @@ docker compose --profile cuda up -d neurondb-cuda
 # Test GPU functions
 psql "postgresql://neurondb:neurondb@localhost:5434/neurondb" <<EOF
 SELECT neurondb.gpu_device_info();
-SELECT neurondb.gpu_enabled();
+SELECT neurondb.compute_mode();
 EOF
 ```
 
diff --git a/NeuronDB/docs/gpu/auto-detection.md b/NeuronDB/docs/gpu/auto-detection.md
index bf418e6..982bf6a 100644
--- a/NeuronDB/docs/gpu/auto-detection.md
+++ b/NeuronDB/docs/gpu/auto-detection.md
@@ -13,7 +13,7 @@ NeuronDB automatically detects available GPUs and falls back to CPU if GPU is un
 SELECT neurondb_gpu_info();
 
 -- Enable auto-detection (default)
-SET neurondb.gpu_enabled = true;
+SET neurondb.compute_mode = true;
 SET neurondb.gpu_auto_detect = true;
 ```
 
diff --git a/NeuronDB/docs/gpu/cuda-support.md b/NeuronDB/docs/gpu/cuda-support.md
index 54e8784..472e40b 100644
--- a/NeuronDB/docs/gpu/cuda-support.md
+++ b/NeuronDB/docs/gpu/cuda-support.md
@@ -12,7 +12,7 @@ Enable CUDA in `postgresql.conf`:
 
 ```conf
 shared_preload_libraries = 'neurondb'
-neurondb.gpu_enabled = true
+neurondb.compute_mode = true
 neurondb.gpu_backend = 'cuda'
 neurondb.gpu_device = 0
 ```
diff --git a/NeuronDB/docs/gpu/metal-support.md b/NeuronDB/docs/gpu/metal-support.md
index d30570f..8eb3672 100644
--- a/NeuronDB/docs/gpu/metal-support.md
+++ b/NeuronDB/docs/gpu/metal-support.md
@@ -10,7 +10,7 @@ Metal support enables GPU acceleration on Apple Silicon (M1, M2, M3) Macs.
 
 ```conf
 shared_preload_libraries = 'neurondb'
-neurondb.gpu_enabled = true
+neurondb.compute_mode = true
 neurondb.gpu_backend = 'metal'
 ```
 
diff --git a/NeuronDB/docs/gpu/rocm-support.md b/NeuronDB/docs/gpu/rocm-support.md
index 620f45d..c198d43 100644
--- a/NeuronDB/docs/gpu/rocm-support.md
+++ b/NeuronDB/docs/gpu/rocm-support.md
@@ -10,7 +10,7 @@ ROCm support enables AMD GPU acceleration for NeuronDB operations.
 
 ```conf
 shared_preload_libraries = 'neurondb'
-neurondb.gpu_enabled = true
+neurondb.compute_mode = true
 neurondb.gpu_backend = 'rocm'
 neurondb.gpu_device = 0
 ```
diff --git a/NeuronDB/docs/sql-api.md b/NeuronDB/docs/sql-api.md
index 1fb27d8..c172baa 100644
--- a/NeuronDB/docs/sql-api.md
+++ b/NeuronDB/docs/sql-api.md
@@ -71,7 +71,7 @@ SELECT neurondb_embed_batch(
 
 ## GPU Distance Functions
 
-GPU-accelerated distance computation functions. Require `neurondb.gpu_enabled = true`.
+GPU-accelerated distance computation functions. Require `neurondb.compute_mode = true`.
 
 ### `vector_l2_distance_gpu(a, b)`
 Compute L2 distance on GPU.
@@ -80,7 +80,7 @@ Compute L2 distance on GPU.
 **Returns:** float8
 
 ```sql
-SET neurondb.gpu_enabled = true;
+SET neurondb.compute_mode = true;
 SELECT vector_l2_distance_gpu(embedding, '[1,2,3]'::vector) FROM documents;
 ```
 
@@ -194,7 +194,7 @@ Get project information and statistics.
 
 | Parameter | Type | Default | Description |
 |-----------|------|---------|-------------|
-| `neurondb.gpu_enabled` | bool | false | Enable GPU acceleration |
+| `neurondb.compute_mode` | bool | false | Enable GPU acceleration |
 | `neurondb.gpu_device` | int | 0 | GPU device ID to use |
 | `neurondb.gpu_batch_size` | int | 1000 | Batch size for GPU operations |
 | `neurondb.gpu_streams` | int | 4 | Number of CUDA streams |
diff --git a/NeuronDB/include/neurondb_macros.h b/NeuronDB/include/neurondb_macros.h
index 267185e..0c7790d 100644
--- a/NeuronDB/include/neurondb_macros.h
+++ b/NeuronDB/include/neurondb_macros.h
@@ -20,12 +20,13 @@
  * Usage:
  *	 nalloc(model, LinRegModel, 1);
  *	 nalloc(coeffs, double, n_features);
+ *
+ * TEMPORARY DEBUGGING: Forced to pure palloc0 to isolate allocator issues
  */
+#undef nalloc
 #define nalloc(ptr, type, count)				\
-	Assert((ptr) == NULL);						\
 	do {									\
-		Assert((ptr) == NULL);				\
-		(ptr) = (type *) palloc0(sizeof(type) * (count));	\
+		(ptr) = (type *) palloc0(sizeof(type) * (Size) (count));	\
 	} while (0)
 
 /*
@@ -50,13 +51,15 @@
  * Usage:
  *	 nfree(model);
  *	 nfree(coeffs);
+ *
+ * TEMPORARY DEBUGGING: Forced to pure pfree to isolate allocator issues
  */
+#undef nfree
 #define nfree(ptr)					\
 	do {								\
 		if ((ptr) != NULL)				\
 		{								\
 			pfree(ptr);					\
-			(ptr) = NULL;				\
 		}								\
 	} while (0)
 
diff --git a/NeuronDB/neurondb--1.0.sql b/NeuronDB/neurondb--1.0.sql
index 7f18584..85bffe8 100644
--- a/NeuronDB/neurondb--1.0.sql
+++ b/NeuronDB/neurondb--1.0.sql
@@ -707,7 +707,7 @@ COMMENT ON FUNCTION neurondb_onnx_info IS 'Return ONNX runtime availability and
 CREATE FUNCTION embed_text(text, text DEFAULT NULL) RETURNS vector
     AS 'MODULE_PATHNAME', 'embed_text'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION embed_text IS 'Generate text embedding with GPU acceleration support: (text, model_name). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION embed_text IS 'Generate text embedding with GPU acceleration support: (text, model_name). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION embed_text_batch(text[], text DEFAULT NULL) RETURNS vector[]
     AS 'MODULE_PATHNAME', 'embed_text_batch'
@@ -814,7 +814,7 @@ CREATE FUNCTION rerank_cross_encoder(text, text[], text DEFAULT 'ms-marco-MiniLM
     RETURNS TABLE(idx integer, score real)
     AS 'MODULE_PATHNAME', 'rerank_cross_encoder'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION rerank_cross_encoder IS 'Cross-encoder reranking with GPU acceleration support: (query, candidates, model, top_k). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION rerank_cross_encoder IS 'Cross-encoder reranking with GPU acceleration support: (query, candidates, model, top_k). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION rerank_llm(text, text[], text DEFAULT 'gpt-3.5-turbo', integer DEFAULT 10)
     RETURNS TABLE(idx integer, score real)
@@ -3745,7 +3745,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 --   neurondb.llm_fail_open      - Fail open on provider errors (default: true)
 --
 -- GPU Settings:
---   neurondb.gpu_enabled        - Enable GPU acceleration (default: false)
+--   neurondb.compute_mode        - Compute execution mode: 0=cpu, 1=gpu, 2=auto (default: 0)
 --   neurondb.gpu_device         - GPU device ID to use (default: 0)
 --   neurondb.gpu_batch_size     - Batch size for GPU operations (default: 1000)
 --   neurondb.gpu_streams        - Number of CUDA streams (default: 4)
@@ -3763,7 +3763,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 --
 -- GPU Preference:
 --   When neurondb.llm_provider is set to 'huggingface-local' or 'hf-local',
---   and neurondb.gpu_enabled is true, the system will:
+--   and neurondb.compute_mode is set to 'gpu' (1) or 'auto' (2), the system will:
 --   1. Try CUDA-accelerated inference first (if GPU available)
 --   2. Fall back to ONNX Runtime with GPU (if available)
 --   3. Fall back to ONNX Runtime with CPU (if GPU unavailable)
@@ -3772,7 +3772,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 -- Example configuration:
 --   SET neurondb.llm_provider = 'huggingface-local';
 --   SET neurondb.llm_model = 'sentence-transformers/all-MiniLM-L6-v2';
---   SET neurondb.gpu_enabled = true;
+--   SET neurondb.compute_mode = 1;  -- 1=gpu, 2=auto, 0=cpu
 --   SET neurondb.gpu_device = 0;
 -- ============================================================================
 
@@ -3781,7 +3781,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 CREATE FUNCTION ndb_llm_complete(prompt text, params text DEFAULT '{}') RETURNS text
     AS 'MODULE_PATHNAME', 'ndb_llm_complete'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_complete IS 'LLM completion with GPU acceleration support: ndb_llm_complete(prompt, params) where params is JSON text with optional model, max_tokens, temperature, etc. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_complete IS 'LLM completion with GPU acceleration support: ndb_llm_complete(prompt, params) where params is JSON text with optional model, max_tokens, temperature, etc. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_image_analyze(image_data bytea, prompt text DEFAULT NULL, params text DEFAULT '{}', model text DEFAULT NULL) RETURNS text
     AS 'MODULE_PATHNAME', 'ndb_llm_image_analyze'
@@ -3791,12 +3791,12 @@ COMMENT ON FUNCTION ndb_llm_image_analyze IS 'LLM image analysis (vision) using
 CREATE FUNCTION ndb_llm_embed(text_input text, model text) RETURNS vector
     AS 'MODULE_PATHNAME', 'ndb_llm_embed'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_embed IS 'LLM embedding with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_embed IS 'LLM embedding with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_rerank(query text, documents text[], model text, top_k integer) RETURNS TABLE(idx integer, score real)
     AS 'MODULE_PATHNAME', 'ndb_llm_rerank'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_rerank IS 'LLM-based reranking with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_rerank IS 'LLM-based reranking with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_enqueue(operation text, model text, input_text text, tenant_id text) RETURNS bigint
     AS 'MODULE_PATHNAME', 'ndb_llm_enqueue'
@@ -3835,7 +3835,7 @@ CREATE FUNCTION ndb_llm_complete_batch(
 )
 	AS 'MODULE_PATHNAME', 'ndb_llm_complete_batch'
 	LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_complete_batch IS 'Batch LLM completion with GPU acceleration support: ndb_llm_complete_batch(prompts, params) processes multiple prompts in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_complete_batch IS 'Batch LLM completion with GPU acceleration support: ndb_llm_complete_batch(prompts, params) processes multiple prompts in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_rerank_batch(
 	queries text[],
@@ -3849,7 +3849,7 @@ CREATE FUNCTION ndb_llm_rerank_batch(
 )
 	AS 'MODULE_PATHNAME', 'ndb_llm_rerank_batch'
 	LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_rerank_batch IS 'Batch LLM reranking with GPU acceleration support: ndb_llm_rerank_batch(queries, documents_array, model, top_k) processes multiple query-document pairs in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_rerank_batch IS 'Batch LLM reranking with GPU acceleration support: ndb_llm_rerank_batch(queries, documents_array, model, top_k) processes multiple query-document pairs in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 -- ============================================================================
 -- DISTRIBUTED QUERY FUNCTIONS
@@ -4937,7 +4937,7 @@ CREATE OR REPLACE FUNCTION neurondb.predict(
 	model_id integer,
 	features vector
 ) RETURNS double precision
-LANGUAGE plpgsql STABLE AS $$
+LANGUAGE plpgsql VOLATILE AS $$
 DECLARE
 	algo text;
 	model_id_param integer;
@@ -4946,9 +4946,16 @@ BEGIN
 	IF model_id_param IS NULL THEN
 		RAISE EXCEPTION 'model_id cannot be NULL';
 	END IF;
-	SELECT m.algorithm INTO algo FROM neurondb.ml_models m WHERE m.model_id = model_id_param;
+	BEGIN
+		SELECT m.algorithm INTO STRICT algo FROM neurondb.ml_models m WHERE m.model_id = model_id_param;
+	EXCEPTION
+		WHEN NO_DATA_FOUND THEN
+			RAISE EXCEPTION 'Model % not found', model_id_param;
+		WHEN TOO_MANY_ROWS THEN
+			RAISE EXCEPTION 'Multiple models found with model_id %', model_id_param;
+	END;
 	IF algo IS NULL THEN
-		RAISE EXCEPTION 'Model % not found', model_id;
+		RAISE EXCEPTION 'Model % not found', model_id_param;
 	END IF;
 	CASE algo
 		WHEN 'random_forest' THEN
@@ -5108,7 +5115,7 @@ BEGIN
 	END CASE;
 END;
 $$;
-COMMENT ON FUNCTION neurondb.embed IS 'Unified embedding function with GPU acceleration support: embed(model, text, task). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION neurondb.embed IS 'Unified embedding function with GPU acceleration support: embed(model, text, task). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE OR REPLACE FUNCTION neurondb.classify(
 	model text,
@@ -5419,7 +5426,7 @@ COMMENT ON FUNCTION neurondb.llm IS
 	'Unified LLM/HuggingFace API: llm(task, model, input_text, input_array, params, max_length) returns JSONB result. 
 	Supported tasks: tokenize, detokenize, embed, complete, classify, ner, qa, rerank, summarize, translate, fill_mask, text2text, zero_shot_classify.
 	Uses GPU acceleration when available, falls back to ONNX Runtime or remote API.
-	Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+	Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 -- ============================================================================
 -- UNIFIED TOKENIZATION API
@@ -5531,7 +5538,7 @@ COMMENT ON FUNCTION neurondb.transform IS
 	'Unified transform API (PostgresML-compatible): transform(model, text, task) returns JSONB result. 
 	Supported tasks: embedding, classification, ner, summarization, translation, fill_mask, text2text, zero_shot_classification.
 	Uses GPU acceleration when available, falls back to ONNX Runtime or remote API.
-	Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+	Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 -- ============================================================================
 -- TASK-SPECIFIC FUNCTIONS
diff --git a/NeuronDB/neurondb--1.0.sql.linux b/NeuronDB/neurondb--1.0.sql.linux
index 7f18584..5f57f90 100644
--- a/NeuronDB/neurondb--1.0.sql.linux
+++ b/NeuronDB/neurondb--1.0.sql.linux
@@ -707,7 +707,7 @@ COMMENT ON FUNCTION neurondb_onnx_info IS 'Return ONNX runtime availability and
 CREATE FUNCTION embed_text(text, text DEFAULT NULL) RETURNS vector
     AS 'MODULE_PATHNAME', 'embed_text'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION embed_text IS 'Generate text embedding with GPU acceleration support: (text, model_name). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION embed_text IS 'Generate text embedding with GPU acceleration support: (text, model_name). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION embed_text_batch(text[], text DEFAULT NULL) RETURNS vector[]
     AS 'MODULE_PATHNAME', 'embed_text_batch'
@@ -814,7 +814,7 @@ CREATE FUNCTION rerank_cross_encoder(text, text[], text DEFAULT 'ms-marco-MiniLM
     RETURNS TABLE(idx integer, score real)
     AS 'MODULE_PATHNAME', 'rerank_cross_encoder'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION rerank_cross_encoder IS 'Cross-encoder reranking with GPU acceleration support: (query, candidates, model, top_k). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION rerank_cross_encoder IS 'Cross-encoder reranking with GPU acceleration support: (query, candidates, model, top_k). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION rerank_llm(text, text[], text DEFAULT 'gpt-3.5-turbo', integer DEFAULT 10)
     RETURNS TABLE(idx integer, score real)
@@ -3745,7 +3745,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 --   neurondb.llm_fail_open      - Fail open on provider errors (default: true)
 --
 -- GPU Settings:
---   neurondb.gpu_enabled        - Enable GPU acceleration (default: false)
+--   neurondb.compute_mode        - Enable GPU acceleration (default: false)
 --   neurondb.gpu_device         - GPU device ID to use (default: 0)
 --   neurondb.gpu_batch_size     - Batch size for GPU operations (default: 1000)
 --   neurondb.gpu_streams        - Number of CUDA streams (default: 4)
@@ -3763,7 +3763,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 --
 -- GPU Preference:
 --   When neurondb.llm_provider is set to 'huggingface-local' or 'hf-local',
---   and neurondb.gpu_enabled is true, the system will:
+--   and neurondb.compute_mode is set to 'gpu' (1) or 'auto' (2), the system will:
 --   1. Try CUDA-accelerated inference first (if GPU available)
 --   2. Fall back to ONNX Runtime with GPU (if available)
 --   3. Fall back to ONNX Runtime with CPU (if GPU unavailable)
@@ -3772,7 +3772,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 -- Example configuration:
 --   SET neurondb.llm_provider = 'huggingface-local';
 --   SET neurondb.llm_model = 'sentence-transformers/all-MiniLM-L6-v2';
---   SET neurondb.gpu_enabled = true;
+--   SET neurondb.compute_mode = 1;  -- 1=gpu, 2=auto, 0=cpu
 --   SET neurondb.gpu_device = 0;
 -- ============================================================================
 
@@ -3781,7 +3781,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 CREATE FUNCTION ndb_llm_complete(prompt text, params text DEFAULT '{}') RETURNS text
     AS 'MODULE_PATHNAME', 'ndb_llm_complete'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_complete IS 'LLM completion with GPU acceleration support: ndb_llm_complete(prompt, params) where params is JSON text with optional model, max_tokens, temperature, etc. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_complete IS 'LLM completion with GPU acceleration support: ndb_llm_complete(prompt, params) where params is JSON text with optional model, max_tokens, temperature, etc. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_image_analyze(image_data bytea, prompt text DEFAULT NULL, params text DEFAULT '{}', model text DEFAULT NULL) RETURNS text
     AS 'MODULE_PATHNAME', 'ndb_llm_image_analyze'
@@ -3791,12 +3791,12 @@ COMMENT ON FUNCTION ndb_llm_image_analyze IS 'LLM image analysis (vision) using
 CREATE FUNCTION ndb_llm_embed(text_input text, model text) RETURNS vector
     AS 'MODULE_PATHNAME', 'ndb_llm_embed'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_embed IS 'LLM embedding with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_embed IS 'LLM embedding with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_rerank(query text, documents text[], model text, top_k integer) RETURNS TABLE(idx integer, score real)
     AS 'MODULE_PATHNAME', 'ndb_llm_rerank'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_rerank IS 'LLM-based reranking with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_rerank IS 'LLM-based reranking with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_enqueue(operation text, model text, input_text text, tenant_id text) RETURNS bigint
     AS 'MODULE_PATHNAME', 'ndb_llm_enqueue'
@@ -3835,7 +3835,7 @@ CREATE FUNCTION ndb_llm_complete_batch(
 )
 	AS 'MODULE_PATHNAME', 'ndb_llm_complete_batch'
 	LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_complete_batch IS 'Batch LLM completion with GPU acceleration support: ndb_llm_complete_batch(prompts, params) processes multiple prompts in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_complete_batch IS 'Batch LLM completion with GPU acceleration support: ndb_llm_complete_batch(prompts, params) processes multiple prompts in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_rerank_batch(
 	queries text[],
@@ -3849,7 +3849,7 @@ CREATE FUNCTION ndb_llm_rerank_batch(
 )
 	AS 'MODULE_PATHNAME', 'ndb_llm_rerank_batch'
 	LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_rerank_batch IS 'Batch LLM reranking with GPU acceleration support: ndb_llm_rerank_batch(queries, documents_array, model, top_k) processes multiple query-document pairs in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_rerank_batch IS 'Batch LLM reranking with GPU acceleration support: ndb_llm_rerank_batch(queries, documents_array, model, top_k) processes multiple query-document pairs in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 -- ============================================================================
 -- DISTRIBUTED QUERY FUNCTIONS
@@ -5108,7 +5108,7 @@ BEGIN
 	END CASE;
 END;
 $$;
-COMMENT ON FUNCTION neurondb.embed IS 'Unified embedding function with GPU acceleration support: embed(model, text, task). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION neurondb.embed IS 'Unified embedding function with GPU acceleration support: embed(model, text, task). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE OR REPLACE FUNCTION neurondb.classify(
 	model text,
@@ -5419,7 +5419,7 @@ COMMENT ON FUNCTION neurondb.llm IS
 	'Unified LLM/HuggingFace API: llm(task, model, input_text, input_array, params, max_length) returns JSONB result. 
 	Supported tasks: tokenize, detokenize, embed, complete, classify, ner, qa, rerank, summarize, translate, fill_mask, text2text, zero_shot_classify.
 	Uses GPU acceleration when available, falls back to ONNX Runtime or remote API.
-	Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+	Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 -- ============================================================================
 -- UNIFIED TOKENIZATION API
@@ -5531,7 +5531,7 @@ COMMENT ON FUNCTION neurondb.transform IS
 	'Unified transform API (PostgresML-compatible): transform(model, text, task) returns JSONB result. 
 	Supported tasks: embedding, classification, ner, summarization, translation, fill_mask, text2text, zero_shot_classification.
 	Uses GPU acceleration when available, falls back to ONNX Runtime or remote API.
-	Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+	Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 -- ============================================================================
 -- TASK-SPECIFIC FUNCTIONS
diff --git a/NeuronDB/neurondb--1.0.sql.macos b/NeuronDB/neurondb--1.0.sql.macos
index 7f18584..5f57f90 100644
--- a/NeuronDB/neurondb--1.0.sql.macos
+++ b/NeuronDB/neurondb--1.0.sql.macos
@@ -707,7 +707,7 @@ COMMENT ON FUNCTION neurondb_onnx_info IS 'Return ONNX runtime availability and
 CREATE FUNCTION embed_text(text, text DEFAULT NULL) RETURNS vector
     AS 'MODULE_PATHNAME', 'embed_text'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION embed_text IS 'Generate text embedding with GPU acceleration support: (text, model_name). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION embed_text IS 'Generate text embedding with GPU acceleration support: (text, model_name). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION embed_text_batch(text[], text DEFAULT NULL) RETURNS vector[]
     AS 'MODULE_PATHNAME', 'embed_text_batch'
@@ -814,7 +814,7 @@ CREATE FUNCTION rerank_cross_encoder(text, text[], text DEFAULT 'ms-marco-MiniLM
     RETURNS TABLE(idx integer, score real)
     AS 'MODULE_PATHNAME', 'rerank_cross_encoder'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION rerank_cross_encoder IS 'Cross-encoder reranking with GPU acceleration support: (query, candidates, model, top_k). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION rerank_cross_encoder IS 'Cross-encoder reranking with GPU acceleration support: (query, candidates, model, top_k). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION rerank_llm(text, text[], text DEFAULT 'gpt-3.5-turbo', integer DEFAULT 10)
     RETURNS TABLE(idx integer, score real)
@@ -3745,7 +3745,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 --   neurondb.llm_fail_open      - Fail open on provider errors (default: true)
 --
 -- GPU Settings:
---   neurondb.gpu_enabled        - Enable GPU acceleration (default: false)
+--   neurondb.compute_mode        - Enable GPU acceleration (default: false)
 --   neurondb.gpu_device         - GPU device ID to use (default: 0)
 --   neurondb.gpu_batch_size     - Batch size for GPU operations (default: 1000)
 --   neurondb.gpu_streams        - Number of CUDA streams (default: 4)
@@ -3763,7 +3763,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 --
 -- GPU Preference:
 --   When neurondb.llm_provider is set to 'huggingface-local' or 'hf-local',
---   and neurondb.gpu_enabled is true, the system will:
+--   and neurondb.compute_mode is set to 'gpu' (1) or 'auto' (2), the system will:
 --   1. Try CUDA-accelerated inference first (if GPU available)
 --   2. Fall back to ONNX Runtime with GPU (if available)
 --   3. Fall back to ONNX Runtime with CPU (if GPU unavailable)
@@ -3772,7 +3772,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 -- Example configuration:
 --   SET neurondb.llm_provider = 'huggingface-local';
 --   SET neurondb.llm_model = 'sentence-transformers/all-MiniLM-L6-v2';
---   SET neurondb.gpu_enabled = true;
+--   SET neurondb.compute_mode = 1;  -- 1=gpu, 2=auto, 0=cpu
 --   SET neurondb.gpu_device = 0;
 -- ============================================================================
 
@@ -3781,7 +3781,7 @@ COMMENT ON FUNCTION neurondb.llm_gpu_utilization IS 'Get GPU utilization metrics
 CREATE FUNCTION ndb_llm_complete(prompt text, params text DEFAULT '{}') RETURNS text
     AS 'MODULE_PATHNAME', 'ndb_llm_complete'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_complete IS 'LLM completion with GPU acceleration support: ndb_llm_complete(prompt, params) where params is JSON text with optional model, max_tokens, temperature, etc. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_complete IS 'LLM completion with GPU acceleration support: ndb_llm_complete(prompt, params) where params is JSON text with optional model, max_tokens, temperature, etc. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_image_analyze(image_data bytea, prompt text DEFAULT NULL, params text DEFAULT '{}', model text DEFAULT NULL) RETURNS text
     AS 'MODULE_PATHNAME', 'ndb_llm_image_analyze'
@@ -3791,12 +3791,12 @@ COMMENT ON FUNCTION ndb_llm_image_analyze IS 'LLM image analysis (vision) using
 CREATE FUNCTION ndb_llm_embed(text_input text, model text) RETURNS vector
     AS 'MODULE_PATHNAME', 'ndb_llm_embed'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_embed IS 'LLM embedding with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_embed IS 'LLM embedding with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_rerank(query text, documents text[], model text, top_k integer) RETURNS TABLE(idx integer, score real)
     AS 'MODULE_PATHNAME', 'ndb_llm_rerank'
     LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_rerank IS 'LLM-based reranking with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_rerank IS 'LLM-based reranking with GPU acceleration support. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_enqueue(operation text, model text, input_text text, tenant_id text) RETURNS bigint
     AS 'MODULE_PATHNAME', 'ndb_llm_enqueue'
@@ -3835,7 +3835,7 @@ CREATE FUNCTION ndb_llm_complete_batch(
 )
 	AS 'MODULE_PATHNAME', 'ndb_llm_complete_batch'
 	LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_complete_batch IS 'Batch LLM completion with GPU acceleration support: ndb_llm_complete_batch(prompts, params) processes multiple prompts in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_complete_batch IS 'Batch LLM completion with GPU acceleration support: ndb_llm_complete_batch(prompts, params) processes multiple prompts in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE FUNCTION ndb_llm_rerank_batch(
 	queries text[],
@@ -3849,7 +3849,7 @@ CREATE FUNCTION ndb_llm_rerank_batch(
 )
 	AS 'MODULE_PATHNAME', 'ndb_llm_rerank_batch'
 	LANGUAGE C STABLE;
-COMMENT ON FUNCTION ndb_llm_rerank_batch IS 'Batch LLM reranking with GPU acceleration support: ndb_llm_rerank_batch(queries, documents_array, model, top_k) processes multiple query-document pairs in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION ndb_llm_rerank_batch IS 'Batch LLM reranking with GPU acceleration support: ndb_llm_rerank_batch(queries, documents_array, model, top_k) processes multiple query-document pairs in parallel using GPU when available. Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 -- ============================================================================
 -- DISTRIBUTED QUERY FUNCTIONS
@@ -5108,7 +5108,7 @@ BEGIN
 	END CASE;
 END;
 $$;
-COMMENT ON FUNCTION neurondb.embed IS 'Unified embedding function with GPU acceleration support: embed(model, text, task). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+COMMENT ON FUNCTION neurondb.embed IS 'Unified embedding function with GPU acceleration support: embed(model, text, task). Uses CUDA-accelerated inference when available, falls back to ONNX Runtime or remote API. Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 CREATE OR REPLACE FUNCTION neurondb.classify(
 	model text,
@@ -5419,7 +5419,7 @@ COMMENT ON FUNCTION neurondb.llm IS
 	'Unified LLM/HuggingFace API: llm(task, model, input_text, input_array, params, max_length) returns JSONB result. 
 	Supported tasks: tokenize, detokenize, embed, complete, classify, ner, qa, rerank, summarize, translate, fill_mask, text2text, zero_shot_classify.
 	Uses GPU acceleration when available, falls back to ONNX Runtime or remote API.
-	Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+	Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 -- ============================================================================
 -- UNIFIED TOKENIZATION API
@@ -5531,7 +5531,7 @@ COMMENT ON FUNCTION neurondb.transform IS
 	'Unified transform API (PostgresML-compatible): transform(model, text, task) returns JSONB result. 
 	Supported tasks: embedding, classification, ner, summarization, translation, fill_mask, text2text, zero_shot_classification.
 	Uses GPU acceleration when available, falls back to ONNX Runtime or remote API.
-	Configure via neurondb.llm_provider and neurondb.gpu_enabled.';
+	Configure via neurondb.llm_provider and neurondb.compute_mode.';
 
 -- ============================================================================
 -- TASK-SPECIFIC FUNCTIONS
diff --git a/NeuronDB/sql/09_gpu_features.sql b/NeuronDB/sql/09_gpu_features.sql
index f676719..02fb2b9 100644
--- a/NeuronDB/sql/09_gpu_features.sql
+++ b/NeuronDB/sql/09_gpu_features.sql
@@ -46,17 +46,21 @@ COMMENT ON FUNCTION ivf_knn_search_gpu(text, vector, int) IS
 -- Extension assumed to be created in 01_types_basic
 
 -- ==== ENVIRONMENT PREP: Ensure deterministic fallback ====
-SET neurondb.gpu_enabled = off;  -- Guarantee CPU for baseline/consistency
+SET neurondb.compute_mode = off;  -- Guarantee CPU for baseline/consistency
 
 -- ==== 1. GPU Info and Status Functions: all call scenarios ====
 -- Actual info/stats output is environment-dependent, so always run, don't just skip
 -- Info before anything
 SELECT neurondb_gpu_info() AS initial_gpu_info;
 
--- Try both enabling (true) and disabling (false), and toggle back and forth
-SELECT neurondb_gpu_enable(true)  AS gpu_enabled_set_true;
-SELECT neurondb_gpu_enable(false) AS gpu_enabled_set_false;
-SELECT neurondb_gpu_enable(NULL)  AS gpu_enabled_set_null;
+-- Try enabling GPU and toggling compute_mode
+SELECT neurondb_gpu_enable() AS gpu_enabled;
+-- Set to GPU mode
+SET neurondb.compute_mode = 1;
+SELECT neurondb_gpu_enable() AS gpu_enabled_after_set_gpu;
+-- Set to CPU mode
+SET neurondb.compute_mode = 0;
+SELECT neurondb_gpu_enable() AS gpu_enabled_after_set_cpu;
 
 -- Again check info after toggling
 SELECT neurondb_gpu_info() AS post_toggle_gpu_info;
diff --git a/NeuronDB/sql/10_gpu_distance_wrappers.sql b/NeuronDB/sql/10_gpu_distance_wrappers.sql
index 7f7bb5d..64f61ba 100644
--- a/NeuronDB/sql/10_gpu_distance_wrappers.sql
+++ b/NeuronDB/sql/10_gpu_distance_wrappers.sql
@@ -1,7 +1,7 @@
 -- Extension created in 01_types_basic
 
 -- Disable GPU acceleration for accurate baseline testing
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 
 -- =======================
 -- Test all supported GPU distance functions in detail
diff --git a/NeuronDB/sql/11_quantization_detail.sql b/NeuronDB/sql/11_quantization_detail.sql
index 3e82c8f..f2cf428 100644
--- a/NeuronDB/sql/11_quantization_detail.sql
+++ b/NeuronDB/sql/11_quantization_detail.sql
@@ -1,5 +1,5 @@
 -- Extension created in 01_types_basic
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 
 -- Quantization detail: All supported formats, their storage size, and characteristics
 -- Each query below provides the quantized binary size for a vector of four dimensions: [1, -1, 0, 3]
diff --git a/NeuronDB/src/core/neurondb.c b/NeuronDB/src/core/neurondb.c
index b0e20e5..123bf94 100644
--- a/NeuronDB/src/core/neurondb.c
+++ b/NeuronDB/src/core/neurondb.c
@@ -72,7 +72,12 @@ new_vector(int dim)
 						VECTOR_MAX_DIM)));
 
 	size = VECTOR_SIZE(dim);
-	result = (Vector *) palloc0(size);
+	{
+		char *tmp = NULL;
+		nalloc(tmp, char, size);
+		result = (Vector *) tmp;
+		MemSet(result, 0, size);
+	}
 	SET_VARSIZE(result, size);
 	result->dim = dim;
 
@@ -121,7 +126,11 @@ copy_vector(Vector *vector)
 				(errcode(ERRCODE_DATA_CORRUPTED),
 				 errmsg("invalid vector size: %d", size)));
 
-	result = (Vector *) palloc(size);
+	{
+		char *tmp = NULL;
+		nalloc(tmp, char, size);
+		result = (Vector *) tmp;
+	}
 	memcpy(result, vector, size);
 	return result;
 }
@@ -160,7 +169,7 @@ vector_in_internal(char *str, int *out_dim, bool check)
 	if (*ptr == '[' || *ptr == '{')
 		ptr++;
 
-	data = (float4 *) palloc(sizeof(float4) * capacity);
+	nalloc(data, float4, capacity);
 
 	while (*ptr && *ptr != ']' && *ptr != '}')
 	{
@@ -825,7 +834,7 @@ vector_to_array(PG_FUNCTION_ARGS)
 	vec = PG_GETARG_VECTOR_P(0);
 	NDB_CHECK_VECTOR_VALID(vec);
 
-	elems = (Datum *) palloc(sizeof(Datum) * vec->dim);
+	nalloc(elems, Datum, vec->dim);
 
 	for (i = 0; i < vec->dim; i++)
 		elems[i] = Float4GetDatum(vec->data[i]);
diff --git a/NeuronDB/src/core/types_core.c b/NeuronDB/src/core/types_core.c
index 20c1d29..e5989a6 100644
--- a/NeuronDB/src/core/types_core.c
+++ b/NeuronDB/src/core/types_core.c
@@ -66,7 +66,7 @@ vectorp_in(PG_FUNCTION_ARGS)
 	if (*ptr == '[')
 		ptr++;
 
-	temp_data = (float4 *) palloc(sizeof(float4) * capacity);
+	nalloc(temp_data, float4, capacity);
 
 	while (*ptr && *ptr != ']')
 	{
@@ -100,7 +100,12 @@ vectorp_in(PG_FUNCTION_ARGS)
 						"dimension")));
 
 	size = VECTORP_SIZE(dim);
-	result = (VectorPacked *) palloc0(size);
+		{
+			char *tmp = NULL;
+			nalloc(tmp, char, size);
+			result = (VectorPacked *) tmp;
+			MemSet(result, 0, size);
+		}
 	SET_VARSIZE(result, size);
 
 	/* Compute fingerprint (CRC32 of dimension count) */
@@ -325,7 +330,12 @@ vecmap_in(PG_FUNCTION_ARGS)
 	}
 
 	size = sizeof(VectorMap) + sizeof(int32) * nnz + sizeof(float4) * nnz;
-	result = (VectorMap *) palloc0(size);
+		{
+			char *tmp = NULL;
+			nalloc(tmp, char, size);
+			result = (VectorMap *) tmp;
+			MemSet(result, 0, size);
+		}
 	SET_VARSIZE(result, size);
 
 	result->total_dim = dim;
@@ -414,7 +424,12 @@ rtext_in(PG_FUNCTION_ARGS)
 
 	/* Basic implementation: store text, tokenize later */
 	size = sizeof(RetrievableText) + text_len + 1;
-	result = (RetrievableText *) palloc0(size);
+		{
+			char *tmp = NULL;
+			nalloc(tmp, char, size);
+			result = (RetrievableText *) tmp;
+			MemSet(result, 0, size);
+		}
 	SET_VARSIZE(result, size);
 
 	result->text_len = text_len;
@@ -445,7 +460,7 @@ rtext_out(PG_FUNCTION_ARGS)
 
 	rt = (RetrievableText *) PG_GETARG_POINTER(0);
 
-	result = (char *) palloc(rt->text_len + 1);
+	nalloc(result, char, rt->text_len + 1);
 	memcpy(result, RTEXT_DATA(rt), rt->text_len);
 	result[rt->text_len] = '\0';
 
@@ -632,7 +647,12 @@ vgraph_in(PG_FUNCTION_ARGS)
 
 	/* Build result - simplified: no node IDs, just edges */
 	size = sizeof(VectorGraph) + sizeof(GraphEdge) * num_edges;
-	result = (VectorGraph *) palloc0(size);
+		{
+			char *tmp = NULL;
+			nalloc(tmp, char, size);
+			result = (VectorGraph *) tmp;
+			MemSet(result, 0, size);
+		}
 	SET_VARSIZE(result, size);
 
 	result->num_nodes = num_nodes;
diff --git a/NeuronDB/src/gpu/common/gpu_backend_registry.c b/NeuronDB/src/gpu/common/gpu_backend_registry.c
index 91bdf02..3411401 100644
--- a/NeuronDB/src/gpu/common/gpu_backend_registry.c
+++ b/NeuronDB/src/gpu/common/gpu_backend_registry.c
@@ -350,8 +350,18 @@ ndb_gpu_lr_train(const float *features,
 
 	if (errstr)
 		*errstr = NULL;
-	if (!active_backend || active_backend->lr_train == NULL)
+	if (!active_backend)
+	{
+		if (errstr)
+			*errstr = pstrdup("ndb_gpu_lr_train: active_backend is NULL");
+		return -1;
+	}
+	if (active_backend->lr_train == NULL)
+	{
+		if (errstr)
+			*errstr = psprintf("ndb_gpu_lr_train: backend->lr_train is NULL (backend=%s)", active_backend->name ? active_backend->name : "unknown");
 		return -1;
+	}
 
 	elog(DEBUG1,
 		 "ndb_gpu_lr_train: calling backend->lr_train, metrics=%p",
@@ -409,35 +419,47 @@ ndb_gpu_linreg_train(const float *features,
 					 Jsonb * *metrics,
 					 char **errstr)
 {
+	int			rc;
+
+	/* CPU mode: never run GPU code */
+	if (NDB_COMPUTE_MODE_IS_CPU())
+	{
+		if (errstr)
+			*errstr = NULL;
+		return -1;
+	}
+
 	if (errstr)
 		*errstr = NULL;
 	if (!active_backend)
 	{
-		elog(DEBUG1, "ndb_gpu_linreg_train: active_backend is NULL");
+		if (errstr)
+			*errstr = pstrdup("ndb_gpu_linreg_train: active_backend is NULL");
 		return -1;
 	}
 	if (active_backend->linreg_train == NULL)
 	{
-		elog(DEBUG1, "ndb_gpu_linreg_train: active_backend->linreg_train is NULL (backend=%s)",
-			 active_backend->name ? active_backend->name : "unknown");
+		if (errstr)
+			*errstr = psprintf("ndb_gpu_linreg_train: backend->linreg_train is NULL (backend=%s)", active_backend->name ? active_backend->name : "unknown");
 		return -1;
 	}
-	ereport(DEBUG2,
-			(errmsg("ndb_gpu_linreg_train: calling backend->linreg_train"),
-			 errdetail("backend=%s, features=%p, targets=%p, n_samples=%d, feature_dim=%d",
-					   active_backend->name ? active_backend->name : "unknown",
-					   (void *) features,
-					   (void *) targets,
-					   n_samples,
-					   feature_dim)));
-	return active_backend->linreg_train(features,
-										targets,
-										n_samples,
-										feature_dim,
-										hyperparams,
-										model_data,
-										metrics,
-										errstr);
+
+	elog(DEBUG1,
+		 "ndb_gpu_linreg_train: calling backend->linreg_train, metrics=%p",
+		 (void *) metrics);
+	rc = active_backend->linreg_train(features,
+									  targets,
+									  n_samples,
+									  feature_dim,
+									  hyperparams,
+									  model_data,
+									  metrics,
+									  errstr);
+	elog(DEBUG1,
+		 "ndb_gpu_linreg_train: backend->linreg_train returned %d, *metrics=%p",
+		 rc,
+		 metrics ? (void *) *metrics : NULL);
+	return rc;
 }
 
 int
diff --git a/NeuronDB/src/gpu/common/gpu_core.c b/NeuronDB/src/gpu/common/gpu_core.c
index a11891b..1ebb3ac 100644
--- a/NeuronDB/src/gpu/common/gpu_core.c
+++ b/NeuronDB/src/gpu/common/gpu_core.c
@@ -381,7 +381,8 @@ neurondb_gpu_get_device_info(int device_id)
 	GPUDeviceInfo *info = NULL;
 	NDBGpuDeviceInfo native;
 
-	info = (GPUDeviceInfo *) palloc0(sizeof(GPUDeviceInfo));
+		nalloc(info, GPUDeviceInfo, 1);
+		MemSet(info, 0, sizeof(GPUDeviceInfo));
 	info->device_id = device_id;
 	info->is_available = false;
 
@@ -440,7 +441,8 @@ neurondb_gpu_set_device(int device_id)
 GPUStats *
 neurondb_gpu_get_stats(void)
 {
-	GPUStats   *stats = (GPUStats *) palloc(sizeof(GPUStats));
+	GPUStats *stats = NULL;
+	nalloc(stats, GPUStats, 1);
 
 	memcpy(stats, &gpu_stats, sizeof(GPUStats));
 	if (stats->queries_executed > 0)
@@ -704,8 +706,9 @@ neurondb_gpu_rf_best_split_binary(const float *feature_values,
 	best_lc = 0;
 	best_rc = 0;
 
-	i_idx = (int *) palloc(sizeof(int) * n);
-	prefix_pos = (int *) palloc0(sizeof(int) * n);
+	nalloc(i_idx, int, n);
+	nalloc(prefix_pos, int, n);
+	MemSet(prefix_pos, 0, sizeof(int) * n);
 	total_pos = 0;
 	for (i = 0; i < n; i++)
 	{
@@ -990,8 +993,10 @@ neurondb_gpu_hf_rerank_batch(const char *model_name,
 		|| !scores_out || !nscores_out || num_queries <= 0)
 		goto out;
 
-	*scores_out = (float **) palloc0(num_queries * sizeof(float *));
-	*nscores_out = (int *) palloc0(num_queries * sizeof(int));
+	nalloc(*scores_out, float *, num_queries);
+	MemSet(*scores_out, 0, sizeof(float *) * num_queries);
+	nalloc(*nscores_out, int, num_queries);
+	MemSet(*nscores_out, 0, sizeof(int) * num_queries);
 
 	for (i = 0; i < num_queries; i++)
 	{
diff --git a/NeuronDB/src/gpu/common/gpu_model_bridge.c b/NeuronDB/src/gpu/common/gpu_model_bridge.c
index 40b672e..cb261a0 100644
--- a/NeuronDB/src/gpu/common/gpu_model_bridge.c
+++ b/NeuronDB/src/gpu/common/gpu_model_bridge.c
@@ -27,6 +27,9 @@
 #include "neurondb_safe_memory.h"
 #include "neurondb_macros.h"
 #include "neurondb_constants.h"
+#ifdef NDB_GPU_CUDA
+#include "neurondb_cuda_linreg.h"
+#endif
 
 extern int	ndb_gpu_dt_train(const float *features,
 							 const double *labels,
@@ -56,7 +59,10 @@ extern int	ndb_gpu_lasso_train(const float *features,
 #include "miscadmin.h"
 #include "utils/builtins.h"
 #include "utils/memutils.h"
+#include "utils/jsonb.h"
+#include "lib/stringinfo.h"
 #include <string.h>
+#include <math.h>
 
 static void
 ndb_gpu_init_train_result(MLGpuTrainResult *result)
@@ -112,29 +118,45 @@ ndb_gpu_try_train_model(const char *algorithm,
 
 	if (NDB_COMPUTE_MODE_IS_CPU())
 	{
+		if (errstr)
+			*errstr = pstrdup("ndb_gpu_try_train_model: CPU mode - GPU code should not be called");
 		return false;
 	}
 
 	if (feature_matrix == NULL || label_vector == NULL || sample_count <= 0 || feature_dim <= 0)
 	{
+		if (errstr)
+			*errstr = psprintf("ndb_gpu_try_train_model: invalid parameters (feature_matrix=%p, label_vector=%p, sample_count=%d, feature_dim=%d)",
+							   (void *) feature_matrix, (void *) label_vector, sample_count, feature_dim);
 		return false;
 	}
 
 	if (!neurondb_gpu_is_available())
 	{
+		if (errstr)
+			*errstr = pstrdup("ndb_gpu_try_train_model: GPU is not available");
 		return false;
 	}
 
 	ops = ndb_gpu_lookup_model_ops(algorithm);
 	if (ops == NULL || ops->train == NULL || ops->serialize == NULL)
 	{
-		return false;
+		/* For algorithms with direct paths (linear_regression, logistic_regression), ops might be NULL */
+		/* This is OK - we'll use the direct path instead */
+		if (algorithm != NULL && strcmp(algorithm, "linear_regression") != 0 && strcmp(algorithm, "logistic_regression") != 0)
+		{
+			if (errstr)
+				*errstr = psprintf("ndb_gpu_try_train_model: no GPU ops for algorithm '%s'", algorithm);
+			return false;
+		}
 	}
 
 	memset(&ctx, 0, sizeof(MLGpuContext));
 	ctx.backend = ndb_gpu_get_active_backend();
 	if (ctx.backend == NULL)
 	{
+		if (errstr)
+			*errstr = pstrdup("ndb_gpu_try_train_model: active_backend is NULL");
 		return false;
 	}
 
@@ -213,7 +235,7 @@ ndb_gpu_try_train_model(const char *algorithm,
 	if (ops != NULL && ops->train != NULL && ops->serialize != NULL
 		&& feature_matrix != NULL && label_vector != NULL
 		&& sample_count > 0 && feature_dim > 0
-		&& (algorithm == NULL || strcmp(algorithm, "linear_regression") != 0))
+		&& (algorithm == NULL || (strcmp(algorithm, "linear_regression") != 0 && strcmp(algorithm, "logistic_regression") != 0)))
 	{
 		TimestampTz train_start;
 		TimestampTz train_end;
@@ -421,18 +443,56 @@ ndb_gpu_try_train_model(const char *algorithm,
 			 backend ? (void *) backend->lr_train : NULL,
 			 sample_count,
 			 feature_dim);
+		
+		/* Set errstr if backend is NULL so we have a specific error message */
+		if (backend == NULL)
+		{
+			if (errstr && *errstr == NULL)
+				*errstr = pstrdup("logistic_regression: ndb_gpu_get_active_backend() returned NULL in direct path");
+		}
+		else if (backend->lr_train == NULL)
+		{
+			if (errstr && *errstr == NULL)
+				*errstr = psprintf("logistic_regression: backend->lr_train is NULL (backend=%s)", backend->name ? backend->name : "unknown");
+		}
+
+		/* GPU mode: ensure we're in GPU mode - no fallback allowed */
+		if (NDB_REQUIRE_GPU() && (backend == NULL || backend->lr_train == NULL))
+		{
+			ereport(ERROR,
+					(errcode(ERRCODE_INTERNAL_ERROR),
+					 errmsg("logistic_regression: GPU mode requires GPU backend but backend or lr_train is NULL"),
+					 errdetail("backend=%p, lr_train=%p",
+							   (void *) backend,
+							   backend ? (void *) backend->lr_train : NULL),
+					 errhint("GPU mode requires GPU to be available - cannot fall back to CPU")));
+		}
 
 		if (backend == NULL || backend->lr_train == NULL)
 		{
 			elog(DEBUG1,
 				 "logistic_regression: backend or lr_train is "
 				 "NULL, skipping GPU");
+			/* Ensure errstr is set before returning */
+			if (errstr && *errstr == NULL)
+			{
+				if (backend == NULL)
+					*errstr = pstrdup("logistic_regression: ndb_gpu_get_active_backend() returned NULL");
+				else
+					*errstr = psprintf("logistic_regression: backend->lr_train is NULL (backend=%s)", backend->name ? backend->name : "unknown");
+			}
 			goto lr_fallback;
 		}
 
 		/* Defensive: Validate all parameters before calling CUDA function */
 		if (feature_matrix == NULL)
 		{
+			if (NDB_REQUIRE_GPU())
+			{
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+						 errmsg("logistic_regression: feature_matrix is NULL - GPU mode requires valid input")));
+			}
 			if (errstr)
 				*errstr = pstrdup("logistic_regression: feature_matrix is NULL");
 			elog(WARNING,
@@ -442,6 +502,12 @@ ndb_gpu_try_train_model(const char *algorithm,
 
 		if (label_vector == NULL)
 		{
+			if (NDB_REQUIRE_GPU())
+			{
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+						 errmsg("logistic_regression: label_vector is NULL - GPU mode requires valid input")));
+			}
 			if (errstr)
 				*errstr = pstrdup("logistic_regression: label_vector is NULL");
 			elog(WARNING,
@@ -451,6 +517,13 @@ ndb_gpu_try_train_model(const char *algorithm,
 
 		if (sample_count <= 0 || sample_count > 10000000)
 		{
+			if (NDB_REQUIRE_GPU())
+			{
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+						 errmsg("logistic_regression: invalid sample_count %d - GPU mode requires valid input",
+								sample_count)));
+			}
 			if (errstr)
 				*errstr = psprintf("logistic_regression: invalid sample_count %d",
 								   sample_count);
@@ -462,6 +535,13 @@ ndb_gpu_try_train_model(const char *algorithm,
 
 		if (feature_dim <= 0 || feature_dim > 10000)
 		{
+			if (NDB_REQUIRE_GPU())
+			{
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+						 errmsg("logistic_regression: invalid feature_dim %d - GPU mode requires valid input",
+								feature_dim)));
+			}
 			if (errstr)
 				*errstr = psprintf("logistic_regression: invalid feature_dim %d",
 								   feature_dim);
@@ -474,6 +554,9 @@ ndb_gpu_try_train_model(const char *algorithm,
 		/* Defensive: Wrap CUDA call in error handling */
 		PG_TRY();
 		{
+			elog(DEBUG1,
+				 "logistic_regression: calling ndb_gpu_lr_train: samples=%d, dim=%d, errstr=%p",
+				 sample_count, feature_dim, (void *) errstr);
 			gpu_rc = ndb_gpu_lr_train(feature_matrix,
 									  label_vector,
 									  sample_count,
@@ -482,10 +565,38 @@ ndb_gpu_try_train_model(const char *algorithm,
 									  &payload,
 									  &metadata,
 									  errstr);
+			elog(DEBUG1,
+				 "logistic_regression: ndb_gpu_lr_train returned %d, errstr=%s",
+				 gpu_rc,
+				 (errstr && *errstr) ? *errstr : "NULL");
 		}
 		PG_CATCH();
 		{
 			/* Catch any PostgreSQL-level errors from CUDA code */
+			/* Capture error message from error state before re-throwing */
+			ErrorData *edata = NULL;
+			char *error_msg = NULL;
+			
+			if (CurrentMemoryContext != ErrorContext)
+			{
+				edata = CopyErrorData();
+				if (edata)
+				{
+					/* Prefer detail message over main message for more specific errors */
+					if (edata->detail && strlen(edata->detail) > 0)
+					{
+						error_msg = pstrdup(edata->detail);
+					}
+					else if (edata->message && strlen(edata->message) > 0)
+					{
+						error_msg = pstrdup(edata->message);
+					}
+					/* Set errstr so it's available after re-throw */
+					if (errstr && *errstr == NULL && error_msg != NULL)
+						*errstr = pstrdup(error_msg);
+				}
+			}
+			
 			/* GPU mode: re-raise error, no fallback */
 			if (NDB_REQUIRE_GPU())
 			{
@@ -499,6 +610,10 @@ ndb_gpu_try_train_model(const char *algorithm,
 					nfree(metadata);
 					metadata = NULL;
 				}
+				if (edata)
+					FreeErrorData(edata);
+				if (error_msg)
+					pfree(error_msg);
 				PG_RE_THROW();
 			}
 			/* AUTO mode: fall back to CPU */
@@ -506,7 +621,12 @@ ndb_gpu_try_train_model(const char *algorithm,
 				 "logistic_regression: exception caught during GPU training, falling back to CPU");
 			gpu_rc = -1;
 			if (errstr && *errstr == NULL)
-				*errstr = pstrdup("Exception during GPU training");
+			{
+				if (error_msg)
+					*errstr = pstrdup(error_msg);
+				else
+					*errstr = pstrdup("Exception during GPU training");
+			}
 			if (payload != NULL)
 			{
 				nfree(payload);
@@ -517,6 +637,10 @@ ndb_gpu_try_train_model(const char *algorithm,
 				nfree(metadata);
 				metadata = NULL;
 			}
+			if (edata)
+				FreeErrorData(edata);
+			if (error_msg)
+				pfree(error_msg);
 			FlushErrorState();
 		}
 		PG_END_TRY();
@@ -563,13 +687,31 @@ ndb_gpu_try_train_model(const char *algorithm,
 			/* CPU mode: never error, just return false for fallback */
 			if (!NDB_COMPUTE_MODE_IS_CPU() && NDB_REQUIRE_GPU())
 			{
+				/* Capture error message before raising ERROR */
+				/* Also ensure errstr is set so it's available to the caller */
+				char *error_detail = NULL;
+				if (errstr && *errstr)
+				{
+					error_detail = pstrdup(*errstr);
+				}
+				else
+				{
+					/* If errstr is not set, try to get a meaningful error message */
+					error_detail = psprintf("ndb_gpu_lr_train returned %d (gpu_rc=%d) - no error details available", gpu_rc, gpu_rc);
+					/* Set errstr so it's available to the caller */
+					if (errstr && *errstr == NULL)
+						*errstr = pstrdup(error_detail);
+				}
 				ereport(ERROR,
 						(errcode(ERRCODE_INTERNAL_ERROR),
 						 errmsg("logistic_regression: GPU training failed - GPU mode requires GPU to be available"),
-						 errdetail("GPU attempt elapsed %.3f ms (%s)",
+						 errdetail("GPU attempt elapsed %.3f ms. Error: %s",
 								   elapsed_ms,
-								   (errstr && *errstr) ? *errstr : "no error"),
+								   error_detail),
 						 errhint("Set compute_mode='auto' for automatic CPU fallback.")));
+				/* Should not reach here, but included for safety */
+				if (error_detail)
+					pfree(error_detail);
 			}
 			/* AUTO mode: fall back to CPU */
 			ndb_gpu_stats_record(false, 0.0, elapsed_ms, true);
@@ -611,6 +753,18 @@ lr_fallback:;
 						   (void *) feature_matrix,
 						   (void *) label_vector)));
 
+		/* GPU mode: ensure we're in GPU mode - no fallback allowed */
+		if (NDB_REQUIRE_GPU() && (backend == NULL || backend->linreg_train == NULL))
+		{
+			ereport(ERROR,
+					(errcode(ERRCODE_INTERNAL_ERROR),
+					 errmsg("linear_regression: GPU mode requires GPU backend but backend or linreg_train is NULL"),
+					 errdetail("backend=%p, linreg_train=%p",
+							   (void *) backend,
+							   backend ? (void *) backend->linreg_train : NULL),
+					 errhint("GPU mode requires GPU to be available - cannot fall back to CPU")));
+		}
+
 		if (backend == NULL || backend->linreg_train == NULL)
 		{
 			elog(DEBUG1,
@@ -622,6 +776,12 @@ lr_fallback:;
 		/* Defensive: Validate all parameters before calling GPU function */
 		if (feature_matrix == NULL)
 		{
+			if (NDB_REQUIRE_GPU())
+			{
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+						 errmsg("linear_regression: feature_matrix is NULL - GPU mode requires valid input")));
+			}
 			if (errstr)
 				*errstr = pstrdup("linear_regression: feature_matrix is NULL");
 			elog(WARNING,
@@ -631,6 +791,12 @@ lr_fallback:;
 
 		if (label_vector == NULL)
 		{
+			if (NDB_REQUIRE_GPU())
+			{
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+						 errmsg("linear_regression: label_vector is NULL - GPU mode requires valid input")));
+			}
 			if (errstr)
 				*errstr = pstrdup("linear_regression: label_vector is NULL");
 			elog(WARNING,
@@ -640,6 +806,13 @@ lr_fallback:;
 
 		if (sample_count <= 0 || sample_count > 10000000)
 		{
+			if (NDB_REQUIRE_GPU())
+			{
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+						 errmsg("linear_regression: invalid sample_count %d - GPU mode requires valid input",
+								sample_count)));
+			}
 			if (errstr)
 				*errstr = psprintf("linear_regression: invalid sample_count %d",
 								   sample_count);
@@ -651,6 +824,13 @@ lr_fallback:;
 
 		if (feature_dim <= 0 || feature_dim > 10000)
 		{
+			if (NDB_REQUIRE_GPU())
+			{
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+						 errmsg("linear_regression: invalid feature_dim %d - GPU mode requires valid input",
+								feature_dim)));
+			}
 			if (errstr)
 				*errstr = psprintf("linear_regression: invalid feature_dim %d",
 								   feature_dim);
@@ -766,12 +946,10 @@ lr_fallback:;
 
 				if (metadata != NULL)
 				{
-					/*
-					 * Skip metrics copying to avoid DirectFunctionCall1
-					 * issues
-					 */
-					result->spec.metrics = NULL;
-					result->metadata = NULL;
+					/* Set metrics for model registration */
+					result->spec.metrics = metadata;
+					result->metadata = metadata;
+					/* Don't transfer ownership - metadata will be copied during registration */
 
 					meta_txt = DatumGetCString(
 											   DirectFunctionCall1(jsonb_out,
@@ -785,10 +963,13 @@ lr_fallback:;
 				}
 				else
 				{
-					elog(WARNING,
-						 "gpu_model_bridge: linear_regression direct path "
-						 "metadata is NULL!");
-					result->spec.metrics = NULL;
+					/* metadata is NULL - pack_model should have created it */
+					/* This is a critical error in GPU mode - model cannot be registered without metrics */
+					ereport(ERROR,
+							(errcode(ERRCODE_INTERNAL_ERROR),
+							 errmsg("neurondb: GPU training succeeded but metrics JSON is NULL"),
+							 errdetail("pack_model should have created metrics JSON with training_backend=1 and storage=gpu"),
+							 errhint("This indicates a bug in ndb_cuda_linreg_pack_model - metrics JSON creation failed or was skipped")));
 				}
 			}
 			else
@@ -823,13 +1004,26 @@ lr_fallback:;
 			/* CPU mode: never error, just return false for fallback */
 			if (!NDB_COMPUTE_MODE_IS_CPU() && NDB_REQUIRE_GPU())
 			{
+				/* Capture error message before raising ERROR */
+				char *error_detail = NULL;
+				if (errstr && *errstr)
+				{
+					error_detail = pstrdup(*errstr);
+				}
+				else
+				{
+					error_detail = pstrdup("GPU training failed - no error details available");
+				}
 				ereport(ERROR,
 						(errcode(ERRCODE_INTERNAL_ERROR),
 						 errmsg("linear_regression: GPU training failed - GPU mode requires GPU to be available"),
-						 errdetail("GPU attempt elapsed %.3f ms (%s)",
+						 errdetail("GPU attempt elapsed %.3f ms. Error: %s",
 								   elapsed_ms,
-								   (errstr && *errstr) ? *errstr : "no error"),
+								   error_detail),
 						 errhint("Set compute_mode='auto' for automatic CPU fallback.")));
+				/* Should not reach here, but included for safety */
+				if (error_detail)
+					pfree(error_detail);
 			}
 			/* AUTO mode: fall back to CPU */
 			ndb_gpu_stats_record(false, 0.0, elapsed_ms, true);
@@ -1128,7 +1322,22 @@ linreg_fallback:;
 	ereport(DEBUG2, (errmsg("gpu_model_bridge: about to check if (!trained), trained=%d", trained)));
 	if (!trained)
 	{
-		ereport(DEBUG1, (errmsg("gpu_model_bridge: trained is false, cleaning up")));
+		ereport(DEBUG1, (errmsg("gpu_model_bridge: trained is false, cleaning up. algorithm=%s, errstr=%s",
+								algorithm ? algorithm : "NULL",
+								(errstr && *errstr) ? *errstr : "NULL")));
+		/* If errstr is not set and we have an algorithm, set a default error message */
+		/* This ensures the caller always gets a meaningful error message */
+		if (errstr && *errstr == NULL)
+		{
+			if (algorithm != NULL)
+			{
+				*errstr = psprintf("ndb_gpu_try_train_model: GPU training failed for algorithm '%s' - no specific error available. Check GPU availability, backend registration, and that the algorithm is supported.", algorithm);
+			}
+			else
+			{
+				*errstr = pstrdup("ndb_gpu_try_train_model: GPU training failed - no specific error available. Check GPU availability and backend registration.");
+			}
+		}
 		nfree(payload);
 		nfree(metadata);
 		if (result != NULL)
diff --git a/NeuronDB/src/gpu/common/ml_gpu_buffer.c b/NeuronDB/src/gpu/common/ml_gpu_buffer.c
index 4a1db6b..c13f77e 100644
--- a/NeuronDB/src/gpu/common/ml_gpu_buffer.c
+++ b/NeuronDB/src/gpu/common/ml_gpu_buffer.c
@@ -95,8 +95,13 @@ ml_gpu_buffer_init_owner(MLGpuBuffer *buf,
 
 	if (total_bytes > 0)
 	{
-		buf->host_ptr =
-			zero ? palloc0(total_bytes) : palloc(total_bytes);
+		{
+			char *tmp = NULL;
+			nalloc(tmp, char, total_bytes);
+			buf->host_ptr = tmp;
+			if (zero)
+				MemSet(buf->host_ptr, 0, total_bytes);
+		}
 	}
 
 	MemoryContextSwitchTo(oldcx);
diff --git a/NeuronDB/src/gpu/cuda/gpu_backend_cuda.c b/NeuronDB/src/gpu/cuda/gpu_backend_cuda.c
index 16f485e..6969b39 100644
--- a/NeuronDB/src/gpu/cuda/gpu_backend_cuda.c
+++ b/NeuronDB/src/gpu/cuda/gpu_backend_cuda.c
@@ -694,8 +694,9 @@ ndb_cuda_launch_kmeans_update(const float *vectors,
 		|| centroids == NULL)
 		return -1;
 
-	assign32 = (int32_t *) palloc(sizeof(int32_t) * num_vectors);
-	counts = (int32_t *) palloc0(sizeof(int32_t) * k);
+	nalloc(assign32, int32_t, num_vectors);
+	nalloc(counts, int32_t, k);
+	MemSet(counts, 0, sizeof(int32_t) * k);
 
 	for (i = 0; i < num_vectors; i++)
 		assign32[i] = (int32_t) assignments[i];
diff --git a/NeuronDB/src/gpu/cuda/gpu_hf_cuda.c b/NeuronDB/src/gpu/cuda/gpu_hf_cuda.c
index 7eec75e..bd85e33 100644
--- a/NeuronDB/src/gpu/cuda/gpu_hf_cuda.c
+++ b/NeuronDB/src/gpu/cuda/gpu_hf_cuda.c
@@ -204,7 +204,7 @@ ndb_cuda_hf_load_model_weights(const char *model_name,
 								(size_t)config->embed_dim * config->hidden_dim * 2 * sizeof(float) +
 								(size_t)config->vocab_size * config->embed_dim * sizeof(float);
 
-			host_weights_buffer = palloc(total_weights_size);
+			nalloc(host_weights_buffer, char, total_weights_size);
 			if (!host_weights_buffer)
 			{
 				neurondb_onnx_unload_model(onnx_session);
@@ -221,7 +221,7 @@ ndb_cuda_hf_load_model_weights(const char *model_name,
 			cuda_err = cudaMalloc(&device_weights_buffer, total_weights_size);
 			if (cuda_err != cudaSuccess)
 			{
-				pfree(host_weights_buffer);
+				nfree(host_weights_buffer);
 				neurondb_onnx_unload_model(onnx_session);
 				if (errstr)
 					*errstr = psprintf("failed to allocate GPU memory for model weights: %s",
@@ -235,7 +235,7 @@ ndb_cuda_hf_load_model_weights(const char *model_name,
 			if (cuda_err != cudaSuccess)
 			{
 				cudaFree(device_weights_buffer);
-				pfree(host_weights_buffer);
+				nfree(host_weights_buffer);
 				neurondb_onnx_unload_model(onnx_session);
 				if (errstr)
 					*errstr = psprintf("failed to copy model weights to GPU: %s",
@@ -327,11 +327,11 @@ ndb_cuda_hf_tokenize_text(const char *text,
 			}
 
 			*seq_len = onnx_seq_len;
-			pfree(onnx_token_ids);
+			nfree(onnx_token_ids);
 			return 0;
 		}
 		if (onnx_token_ids)
-			pfree(onnx_token_ids);
+			nfree(onnx_token_ids);
 	}
 	PG_CATCH();
 	{
@@ -2890,7 +2890,8 @@ ndb_cuda_hf_generate_batch(const char *model_name,
 	num_streams = (num_prompts < max_streams) ? num_prompts : max_streams;
 
 	/* Create CUDA streams for parallel processing */
-	streams = (cudaStream_t *) palloc0(num_streams * sizeof(cudaStream_t));
+		nalloc(streams, cudaStream_t, num_streams);
+		MemSet(streams, 0, sizeof(cudaStream_t) * num_streams);
 	for (i = 0; i < num_streams; i++)
 	{
 		cudaError_t cuda_err = cudaStreamCreate(&streams[i]);
diff --git a/NeuronDB/src/gpu/cuda/gpu_knn_cuda.c b/NeuronDB/src/gpu/cuda/gpu_knn_cuda.c
index 7518c83..ba9e604 100644
--- a/NeuronDB/src/gpu/cuda/gpu_knn_cuda.c
+++ b/NeuronDB/src/gpu/cuda/gpu_knn_cuda.c
@@ -55,6 +55,7 @@ ndb_cuda_knn_pack(const struct KNNModel *model,
 	size_t		labels_bytes;
 	bytea	   *blob = NULL;
 	char	   *base = NULL;
+	char	   *tmp = NULL;
 	NdbCudaKnnModelHeader *hdr = NULL;
 	float	   *features_dest = NULL;
 	double	   *labels_dest = NULL;
@@ -119,7 +120,8 @@ ndb_cuda_knn_pack(const struct KNNModel *model,
 		return -1;
 	}
 
-	blob = (bytea *) palloc(VARHDRSZ + payload_bytes);
+	nalloc(tmp, char, VARHDRSZ + payload_bytes);
+	blob = (bytea *) tmp;
 	if (blob == NULL)
 	{
 		if (errstr)
@@ -399,7 +401,7 @@ ndb_cuda_knn_train(const float *features,
 			*errstr = pstrdup("CUDA KNN train: feature array size exceeds MaxAllocSize");
 		return -1;
 	}
-	features_copy = (float *) palloc(sizeof(float) * (size_t) n_samples * (size_t) feature_dim);
+	nalloc(features_copy, float, (size_t) n_samples * (size_t) feature_dim);
 	if (features_copy == NULL)
 	{
 		if (errstr)
@@ -407,7 +409,7 @@ ndb_cuda_knn_train(const float *features,
 		return -1;
 	}
 
-	labels_copy = (double *) palloc(sizeof(double) * (size_t) n_samples);
+	nalloc(labels_copy, double, (size_t) n_samples);
 	if (labels_copy == NULL)
 	{
 		if (errstr)
@@ -574,7 +576,7 @@ ndb_cuda_knn_predict(const bytea * model_data,
 			*errstr = pstrdup("CUDA KNN predict: distances array size exceeds MaxAllocSize");
 		return -1;
 	}
-	distances = (float *) palloc(sizeof(float) * hdr->n_samples);
+	nalloc(distances, float, hdr->n_samples);
 	if (distances == NULL)
 	{
 		if (errstr)
@@ -815,7 +817,7 @@ ndb_cuda_knn_evaluate_batch(const bytea * model_data,
 	}
 
 	/* Allocate predictions array */
-	predictions = (int *) palloc(sizeof(int) * (size_t) n_samples);
+	nalloc(predictions, int, (size_t) n_samples);
 	if (predictions == NULL)
 	{
 		if (errstr)
diff --git a/NeuronDB/src/gpu/cuda/gpu_linreg_cuda.c b/NeuronDB/src/gpu/cuda/gpu_linreg_cuda.c
index bed6bcd..c4b1b3e 100644
--- a/NeuronDB/src/gpu/cuda/gpu_linreg_cuda.c
+++ b/NeuronDB/src/gpu/cuda/gpu_linreg_cuda.c
@@ -27,14 +27,32 @@
 #include "lib/stringinfo.h"
 #include "utils/builtins.h"
 #include "utils/jsonb.h"
+#include "utils/elog.h"
 
 #include "ml_linear_regression_internal.h"
 #include "neurondb_cuda_linreg.h"
 #include "neurondb_validation.h"
-#include "neurondb_safe_memory.h"
+/* Note: neurondb_safe_memory.h not needed - we override nalloc/nfree below */
 #include "neurondb_macros.h"
 #include "neurondb_guc.h"
 #include "neurondb_constants.h"
+#include "neurondb_json.h"
+
+/* TEMPORARY DEBUGGING: Replace nalloc/nfree with direct PostgreSQL allocators */
+/* This isolates whether the wrappers are causing memory corruption */
+#undef nalloc
+#undef nfree
+#define nalloc(var, type, count) \
+	do { \
+		(var) = (type *) MemoryContextAlloc(CurrentMemoryContext, sizeof(type) * (Size) (count)); \
+		memset((var), 0, sizeof(type) * (Size) (count)); \
+	} while (0)
+#define nfree(ptr) \
+	do { \
+		if ((ptr) != NULL) { \
+			pfree((ptr)); \
+		} \
+	} while (0)
 
 extern cudaError_t launch_build_X_matrix_kernel(const float *features,
 												float *X_with_intercept,
@@ -71,6 +89,29 @@ ndb_cuda_linreg_pack_model(const LinRegModel *model,
 	bytea *blob = NULL;
 	char *blob_raw = NULL;
 
+
+	/* GPU mode: ensure we're NOT in CPU mode */
+	if (NDB_COMPUTE_MODE_IS_CPU())
+	{
+		if (errstr)
+			*errstr = pstrdup("CUDA linreg_pack_model: CPU mode detected - should not be called");
+		ereport(ERROR,
+				(errcode(ERRCODE_INTERNAL_ERROR),
+				 errmsg("neurondb: CUDA pack_model called in CPU mode - this is a bug")));
+		return -1;
+	}
+
+	/* Ensure GPU mode is required */
+	if (!NDB_REQUIRE_GPU())
+	{
+		if (errstr)
+			*errstr = pstrdup("CUDA linreg_pack_model: not in GPU mode");
+		ereport(ERROR,
+				(errcode(ERRCODE_INTERNAL_ERROR),
+				 errmsg("neurondb: CUDA pack_model called but GPU mode not required - this is a bug")));
+		return -1;
+	}
+
 	if (errstr)
 		*errstr = NULL;
 	if (model == NULL || model_data == NULL)
@@ -82,12 +123,13 @@ ndb_cuda_linreg_pack_model(const LinRegModel *model,
 
 	payload_bytes = sizeof(NdbCudaLinRegModelHeader)
 		+ sizeof(float) * (size_t) model->n_features;
+
 	nalloc(blob_raw, char, VARHDRSZ + payload_bytes);
 	blob = (bytea *) blob_raw;
 	SET_VARSIZE(blob, VARHDRSZ + payload_bytes);
 	base = VARDATA(blob);
-
 	hdr = (NdbCudaLinRegModelHeader *) base;
+	
 	hdr->feature_dim = model->n_features;
 	hdr->n_samples = model->n_samples;
 	hdr->intercept = (float) model->intercept;
@@ -96,23 +138,36 @@ ndb_cuda_linreg_pack_model(const LinRegModel *model,
 	hdr->mae = model->mae;
 
 	coef_dest = (float *) (base + sizeof(NdbCudaLinRegModelHeader));
+	
 	if (model->coefficients != NULL)
 	{
 		int			i;
 
 		for (i = 0; i < model->n_features; i++)
+		{
 			coef_dest[i] = (float) model->coefficients[i];
+		}
 	}
 
 	if (metrics != NULL)
 	{
 		StringInfoData buf;
-		Jsonb *metrics_json = NULL;
+		Jsonb	   *metrics_json = NULL;
+		double		r_squared;
+		double		mse;
+		double		mae;
 
+		/* Compute safe metric values */
+		r_squared = isfinite(model->r_squared) ? model->r_squared : 0.0;
+		mse = isfinite(model->mse) ? model->mse : 0.0;
+		mae = isfinite(model->mae) ? model->mae : 0.0;
+
+		/* Create metrics JSON string - MUST include training_backend=1 and storage=gpu */
 		initStringInfo(&buf);
 		appendStringInfo(&buf,
 						 "{\"algorithm\":\"linear_regression\","
 						 "\"training_backend\":1,"
+						 "\"storage\":\"gpu\","
 						 "\"n_features\":%d,"
 						 "\"n_samples\":%d,"
 						 "\"r_squared\":%.6f,"
@@ -120,13 +175,82 @@ ndb_cuda_linreg_pack_model(const LinRegModel *model,
 						 "\"mae\":%.6f}",
 						 model->n_features,
 						 model->n_samples,
-						 model->r_squared,
-						 model->mse,
-						 model->mae);
+						 r_squared,
+						 mse,
+						 mae);
+
+		/* Create JSON in TopMemoryContext to isolate from potentially corrupted context */
+		/* This prevents the corrupted freelist from affecting JSON creation */
+		{
+			MemoryContext oldcontext;
+			Jsonb	   *temp_json = NULL;
+
+			oldcontext = MemoryContextSwitchTo(TopMemoryContext);
+
+			PG_TRY();
+			{
+				temp_json = ndb_jsonb_in_cstring(buf.data);
+			}
+			PG_CATCH();
+			{
+				MemoryContextSwitchTo(oldcontext);
+				FlushErrorState();
+				ereport(ERROR,
+						(errcode(ERRCODE_INTERNAL_ERROR),
+						 errmsg("neurondb: CUDA linreg_pack_model: Failed to create metrics JSONB in TopMemoryContext"),
+						 errdetail("Even TopMemoryContext failed - this indicates a more serious issue"),
+						 errhint("Check PostgreSQL server logs for additional errors")));
+			}
+			PG_END_TRY();
+
+			if (temp_json == NULL)
+			{
+				MemoryContextSwitchTo(oldcontext);
+				ereport(ERROR,
+						(errcode(ERRCODE_INTERNAL_ERROR),
+						 errmsg("neurondb: CUDA linreg_pack_model: Failed to create metrics JSONB - ndb_jsonb_in_cstring returned NULL")));
+			}
+
+			/* Switch back to original context and copy JSON to it */
+			MemoryContextSwitchTo(oldcontext);
+
+			/* Copy JSONB to current context using palloc and memcpy */
+			/* JSONB is a varlena type, so we need to copy the entire structure */
+			{
+				size_t		jsonb_size = VARSIZE(temp_json);
+
+				metrics_json = (Jsonb *) MemoryContextAlloc(CurrentMemoryContext, jsonb_size);
+				memcpy(metrics_json, temp_json, jsonb_size);
+
+				/* Free the temporary JSON from TopMemoryContext */
+				pfree(temp_json);
+			}
+		}
+		
+		if (metrics_json == NULL)
+		{
+			ereport(ERROR,
+					(errcode(ERRCODE_INTERNAL_ERROR),
+					 errmsg("neurondb: CUDA linreg_pack_model: Failed to create metrics JSONB - ndb_jsonb_in_cstring returned NULL"),
+					 errdetail("Memory context %p may be corrupted. JSON string length=%d",
+							   (void *) CurrentMemoryContext, buf.len),
+					 errhint("This indicates memory corruption occurred earlier in the training process, likely during matrix operations or memory allocation.")));
+		}
+
+		/* Free StringInfo data - use pfree directly to avoid nfree wrapper issues */
+		if (buf.data != NULL)
+		{
+			pfree(buf.data);
+			buf.data = NULL;
+		}
+
+		if (metrics_json == NULL)
+		{
+			ereport(ERROR,
+					(errcode(ERRCODE_INTERNAL_ERROR),
+					 errmsg("neurondb: CUDA linreg_pack_model: metrics JSONB is NULL after creation - GPU mode requires valid metrics")));
+		}
 
-		metrics_json = DatumGetJsonbP(DirectFunctionCall1(
-														  jsonb_in, CStringGetTextDatum(buf.data)));
-		nfree(buf.data);
 		*metrics = metrics_json;
 	}
 
@@ -167,14 +291,34 @@ ndb_cuda_linreg_train(const float *features,
 				k;
 	int			rc = -1;
 
-	/* CPU mode: never execute GPU code */
+	/* Memory context check at start to catch early corruption */
+#ifdef MEMORY_CONTEXT_CHECKING
+	MemoryContextCheck(CurrentMemoryContext);
+#endif
+
+	/* GPU mode: ensure we're NOT in CPU mode - strict check */
 	if (NDB_COMPUTE_MODE_IS_CPU())
 	{
 		if (errstr)
-			*errstr = pstrdup("CUDA linreg_train: CPU mode - GPU code should not be called");
+			*errstr = pstrdup("CUDA linreg_train: CPU mode detected - should not be called");
+		ereport(ERROR,
+				(errcode(ERRCODE_INTERNAL_ERROR),
+				 errmsg("neurondb: CUDA linreg_train called in CPU mode - this is a bug")));
+		return -1;
+	}
+
+	/* Ensure GPU mode is required - no fallback allowed */
+	if (!NDB_REQUIRE_GPU())
+	{
+		if (errstr)
+			*errstr = pstrdup("CUDA linreg_train: not in GPU mode");
+		ereport(ERROR,
+				(errcode(ERRCODE_INTERNAL_ERROR),
+				 errmsg("neurondb: CUDA linreg_train called but GPU mode not required - this is a bug")));
 		return -1;
 	}
 
+
 	if (errstr)
 		*errstr = NULL;
 
@@ -361,10 +505,27 @@ ndb_cuda_linreg_train(const float *features,
 				goto gpu_cleanup;
 		}
 
-		cudaFree(d_features);
-		cudaFree(d_targets);
-		cudaFree(d_XtX);
-		cudaFree(d_Xty);
+		/* Free CUDA memory - set pointers to NULL to prevent double-free in gpu_cleanup */
+		if (d_features)
+		{
+			cudaFree(d_features);
+			d_features = NULL;
+		}
+		if (d_targets)
+		{
+			cudaFree(d_targets);
+			d_targets = NULL;
+		}
+		if (d_XtX)
+		{
+			cudaFree(d_XtX);
+			d_XtX = NULL;
+		}
+		if (d_Xty)
+		{
+			cudaFree(d_Xty);
+			d_Xty = NULL;
+		}
 #endif
 
 gpu_cleanup:
@@ -521,7 +682,7 @@ cpu_fallback:
 		}
 
 		/* Allocate Cholesky factor L (lower triangular, stored row-major) */
-		L = (double *) palloc0(sizeof(double) * dim_with_intercept * dim_with_intercept);
+		nalloc(L, double, dim_with_intercept * dim_with_intercept);
 
 		/* Cholesky decomposition: X'X_work = L * L^T */
 		/* Use working copy (XtX_work) so we don't modify original matrix */
@@ -625,38 +786,68 @@ cpu_fallback:
 
 		/* Compute (X'X)^(-1) for metrics calculation using Cholesky */
 		/* Solve L * L^T * X = I column by column */
+		/* Initialize h_XtX_inv to zero before use */
+		memset(h_XtX_inv, 0, sizeof(double) * dim_with_intercept * dim_with_intercept);
+		
 		for (col = 0; col < dim_with_intercept; col++)
 		{
-			double	   *col_vec = (double *) palloc0(sizeof(double) * dim_with_intercept);
+			double *col_vec = NULL;
+			double *inv_work = NULL;
 
+			nalloc(col_vec, double, dim_with_intercept);
+
+			/* Initialize all entries to zero, then set unit vector */
+			memset(col_vec, 0, sizeof(double) * dim_with_intercept);
 			col_vec[col] = 1.0;
 
 			/* Forward substitution: L * y = e_col */
+			/* Use a separate work array for inverse computation to avoid reusing y_work */
+			nalloc(inv_work, double, dim_with_intercept);
+			
 			for (row = 0; row < dim_with_intercept; row++)
 			{
 				double		sum = col_vec[row];
 
 				for (k_local = 0; k_local < row; k_local++)
 				{
-					sum -= L[row * dim_with_intercept + k_local] * y_work[k_local];
+					sum -= L[row * dim_with_intercept + k_local] * inv_work[k_local];
 				}
-				y_work[row] = sum / L[row * dim_with_intercept + row];
+				inv_work[row] = sum / L[row * dim_with_intercept + row];
 			}
 
 			/* Backward substitution: L^T * x = y */
 			for (row = dim_with_intercept - 1; row >= 0; row--)
 			{
-				double		sum = y_work[row];
+				double		sum = inv_work[row];
+				int			idx;
 
 				for (k_local = row + 1; k_local < dim_with_intercept; k_local++)
 				{
+					/* Bounds check to prevent buffer overrun */
+					idx = k_local * dim_with_intercept + col;
+					if (idx < 0 || idx >= dim_with_intercept * dim_with_intercept)
+					{
+						nfree(inv_work);
+						nfree(col_vec);
+						elog(ERROR, "neurondb: CUDA linreg_train: buffer overrun detected: idx=%d, max=%d", 
+							 idx, dim_with_intercept * dim_with_intercept);
+					}
 					sum -= L[k_local * dim_with_intercept + row]
-						* h_XtX_inv[k_local * dim_with_intercept + col];
+						* h_XtX_inv[idx];
+				}
+				/* Bounds check before write */
+				idx = row * dim_with_intercept + col;
+				if (idx < 0 || idx >= dim_with_intercept * dim_with_intercept)
+				{
+					nfree(inv_work);
+					nfree(col_vec);
+					elog(ERROR, "neurondb: CUDA linreg_train: buffer overrun detected on write: idx=%d, max=%d", 
+						 idx, dim_with_intercept * dim_with_intercept);
 				}
-				h_XtX_inv[row * dim_with_intercept + col] =
-					sum / L[row * dim_with_intercept + row];
+				h_XtX_inv[idx] = sum / L[row * dim_with_intercept + row];
 			}
-
+			
+			nfree(inv_work);
 			nfree(col_vec);
 		}
 
@@ -683,7 +874,9 @@ cpu_fallback:
 		nalloc(model_coefficients, double, feature_dim);
 		model.coefficients = model_coefficients;
 		for (i = 0; i < feature_dim; i++)
+		{
 			model.coefficients[i] = h_beta[i + 1];
+		}
 
 		/* Compute metrics */
 		for (i = 0; i < n_samples; i++)
@@ -692,11 +885,14 @@ cpu_fallback:
 
 		for (i = 0; i < n_samples; i++)
 		{
-			const float *row = features + (i * feature_dim);
-			double		y_pred = model.intercept;
+			const float *row;
+			double		y_pred;
 			double		error;
 			int			j_local;
 
+			row = features + (i * feature_dim);
+			y_pred = model.intercept;
+
 			for (j_local = 0; j_local < feature_dim; j_local++)
 				y_pred += model.coefficients[j_local] * row[j_local];
 
@@ -1120,6 +1316,7 @@ ndb_cuda_linreg_evaluate(const bytea * model_data,
 
 	if (cuda_err != cudaSuccess)
 	{
+		const char *cuda_err_str = cudaGetErrorString(cuda_err);
 		cudaFree(d_features);
 		cudaFree(d_targets);
 		cudaFree(d_coefficients);
@@ -1127,8 +1324,15 @@ ndb_cuda_linreg_evaluate(const bytea * model_data,
 		cudaFree(d_sae);
 		cudaFree(d_count);
 		if (errstr)
-			*errstr = psprintf("neurondb: ndb_cuda_linreg_evaluate: evaluation kernel failed: %s",
-							   cudaGetErrorString(cuda_err));
+		{
+			*errstr = psprintf("neurondb: ndb_cuda_linreg_evaluate: evaluation kernel launch failed: CUDA error %d (%s). "
+							   "Parameters: n_samples=%d, feature_dim=%d, model_feature_dim=%d",
+							   (int) cuda_err,
+							   cuda_err_str ? cuda_err_str : "unknown",
+							   n_samples,
+							   feature_dim,
+							   hdr->feature_dim);
+		}
 		return -1;
 	}
 
diff --git a/NeuronDB/src/gpu/cuda/gpu_lr_cuda.c b/NeuronDB/src/gpu/cuda/gpu_lr_cuda.c
index cbedd05..1e8d163 100644
--- a/NeuronDB/src/gpu/cuda/gpu_lr_cuda.c
+++ b/NeuronDB/src/gpu/cuda/gpu_lr_cuda.c
@@ -35,6 +35,7 @@
 #include "neurondb_macros.h"
 #include "neurondb_guc.h"
 #include "neurondb_constants.h"
+#include "neurondb_json.h"
 
 int
 ndb_cuda_lr_pack_model(const LRModel *model,
@@ -92,6 +93,7 @@ ndb_cuda_lr_pack_model(const LRModel *model,
 		appendStringInfo(&buf,
 						 "{\"algorithm\":\"logistic_regression\","
 						 "\"training_backend\":1,"
+						 "\"storage\":\"gpu\","
 						 "\"n_features\":%d,"
 						 "\"n_samples\":%d,"
 						 "\"max_iters\":%d,"
@@ -107,10 +109,71 @@ ndb_cuda_lr_pack_model(const LRModel *model,
 						 model->final_loss,
 						 model->accuracy);
 
-		metrics_json = DatumGetJsonbP(DirectFunctionCall1(
-														  jsonb_in,
-														  CStringGetDatum(buf.data)));
-		nfree(buf.data);
+		/* Create JSON in TopMemoryContext to isolate from potentially corrupted context */
+		/* This prevents the corrupted freelist from affecting JSON creation */
+		{
+			MemoryContext oldcontext;
+			Jsonb	   *temp_json = NULL;
+
+			oldcontext = MemoryContextSwitchTo(TopMemoryContext);
+
+			PG_TRY();
+			{
+				temp_json = ndb_jsonb_in_cstring(buf.data);
+			}
+			PG_CATCH();
+			{
+				MemoryContextSwitchTo(oldcontext);
+				FlushErrorState();
+				ereport(ERROR,
+						(errcode(ERRCODE_INTERNAL_ERROR),
+						 errmsg("neurondb: CUDA lr_pack_model: Failed to create metrics JSONB in TopMemoryContext"),
+						 errdetail("Even TopMemoryContext failed - this indicates a more serious issue"),
+						 errhint("Check PostgreSQL server logs for additional errors")));
+			}
+			PG_END_TRY();
+
+			if (temp_json == NULL)
+			{
+				MemoryContextSwitchTo(oldcontext);
+				ereport(ERROR,
+						(errcode(ERRCODE_INTERNAL_ERROR),
+						 errmsg("neurondb: CUDA lr_pack_model: Failed to create metrics JSONB - ndb_jsonb_in_cstring returned NULL")));
+			}
+
+			/* Switch back to original context and copy JSON to it */
+			MemoryContextSwitchTo(oldcontext);
+
+			/* Copy JSONB to current context using MemoryContextAlloc and memcpy */
+			/* JSONB is a varlena type, so we need to copy the entire structure */
+			{
+				size_t		jsonb_size = VARSIZE(temp_json);
+
+				metrics_json = (Jsonb *) MemoryContextAlloc(CurrentMemoryContext, jsonb_size);
+				memcpy(metrics_json, temp_json, jsonb_size);
+
+				/* Free the temporary JSON from TopMemoryContext */
+				pfree(temp_json);
+			}
+		}
+
+		if (metrics_json == NULL)
+		{
+			ereport(ERROR,
+					(errcode(ERRCODE_INTERNAL_ERROR),
+					 errmsg("neurondb: CUDA lr_pack_model: Failed to create metrics JSONB - ndb_jsonb_in_cstring returned NULL"),
+					 errdetail("Memory context %p may be corrupted. JSON string length=%d",
+							   (void *) CurrentMemoryContext, buf.len),
+					 errhint("This indicates memory corruption occurred earlier in the training process, likely during matrix operations or memory allocation.")));
+		}
+
+		/* Free StringInfo data - use pfree directly to avoid nfree wrapper issues */
+		if (buf.data != NULL)
+		{
+			pfree(buf.data);
+			buf.data = NULL;
+		}
+
 		*metrics = metrics_json;
 	}
 
@@ -171,6 +234,8 @@ ndb_cuda_lr_train(const float *features,
 	int			i;
 	int			rc = -1;
 
+	elog(WARNING, "ndb_cuda_lr_train: ENTRY - n_samples=%d, feature_dim=%d", n_samples, feature_dim);
+
 	/* CPU mode: never execute GPU code */
 	if (NDB_COMPUTE_MODE_IS_CPU())
 	{
@@ -181,6 +246,8 @@ ndb_cuda_lr_train(const float *features,
 
 	if (errstr)
 		*errstr = NULL;
+	
+	elog(WARNING, "ndb_cuda_lr_train: After initial checks");
 
 	/* Defensive: Comprehensive parameter validation before proceeding */
 	if (model_data == NULL)
@@ -278,6 +345,8 @@ ndb_cuda_lr_train(const float *features,
 		}
 	}
 
+	elog(WARNING, "ndb_cuda_lr_train: After validation checks - skipping hyperparams parsing (use defaults)");
+
 	elog(DEBUG1,
 		 "ndb_cuda_lr_train: entry: model_data=%p, features=%p, "
 		 "labels=%p, n_samples=%d, feature_dim=%d",
@@ -293,109 +362,18 @@ ndb_cuda_lr_train(const float *features,
 		 n_samples,
 		 feature_dim);
 
-	/* Extract and validate hyperparameters from JSONB */
-	if (hyperparams != NULL)
-	{
-		Datum		max_iters_datum;
-		Datum		lr_datum;
-		Datum		lambda_datum;
-		Datum		numeric_datum;
-		Numeric		num;
-		int			parsed_max_iters;
-		double		parsed_lr;
-		double		parsed_lambda;
-
-		max_iters_datum = DirectFunctionCall2(jsonb_object_field,
-											  JsonbPGetDatum(hyperparams),
-											  CStringGetTextDatum("max_iters"));
-		if (DatumGetPointer(max_iters_datum) != NULL)
-		{
-			numeric_datum = DirectFunctionCall1(jsonb_numeric, max_iters_datum);
-			if (DatumGetPointer(numeric_datum) != NULL)
-			{
-				num = DatumGetNumeric(numeric_datum);
-				parsed_max_iters = DatumGetInt32(DirectFunctionCall1(
-																	 numeric_int4, NumericGetDatum(num)));
-				/* Defensive: Validate max_iters range */
-				if (parsed_max_iters > 0 && parsed_max_iters <= 1000000)
-				{
-					max_iters = parsed_max_iters;
-				}
-				else if (parsed_max_iters > 1000000)
-				{
-					elog(WARNING,
-						 "ndb_cuda_lr_train: max_iters %d exceeds maximum 1000000, using default %d",
-						 parsed_max_iters, default_max_iters);
-					max_iters = default_max_iters;
-				}
-				else
-				{
-					elog(WARNING,
-						 "ndb_cuda_lr_train: invalid max_iters %d, using default %d",
-						 parsed_max_iters, default_max_iters);
-					max_iters = default_max_iters;
-				}
-			}
-		}
-
-		lr_datum = DirectFunctionCall2(jsonb_object_field,
-									   JsonbPGetDatum(hyperparams),
-									   CStringGetTextDatum("learning_rate"));
-		if (DatumGetPointer(lr_datum) != NULL)
-		{
-			numeric_datum = DirectFunctionCall1(jsonb_numeric, lr_datum);
-			if (DatumGetPointer(numeric_datum) != NULL)
-			{
-				num = DatumGetNumeric(numeric_datum);
-				parsed_lr = DatumGetFloat8(
-										   DirectFunctionCall1(numeric_float8, NumericGetDatum(num)));
-
-				/*
-				 * Defensive: Validate learning_rate range and check for
-				 * NaN/Inf
-				 */
-				if (isfinite(parsed_lr) && parsed_lr > 0.0 && parsed_lr <= 10.0)
-				{
-					learning_rate = parsed_lr;
-				}
-				else
-				{
-					elog(WARNING,
-						 "ndb_cuda_lr_train: invalid learning_rate %f (must be finite, > 0, <= 10.0), using default %f",
-						 parsed_lr, default_learning_rate);
-					learning_rate = default_learning_rate;
-				}
-			}
-		}
+	/* 
+	 * Skip hyperparameter parsing - DirectFunctionCall causes crashes in GPU context
+	 * Just use defaults like linear regression does
+	 */
+	(void) hyperparams;  /* Suppress unused parameter warning */
+	max_iters = default_max_iters;
+	learning_rate = default_learning_rate;
+	lambda = default_lambda;
 
-		lambda_datum = DirectFunctionCall2(jsonb_object_field,
-										   JsonbPGetDatum(hyperparams),
-										   CStringGetTextDatum("lambda"));
-		if (DatumGetPointer(lambda_datum) != NULL)
-		{
-			numeric_datum = DirectFunctionCall1(jsonb_numeric, lambda_datum);
-			if (DatumGetPointer(numeric_datum) != NULL)
-			{
-				num = DatumGetNumeric(numeric_datum);
-				parsed_lambda = DatumGetFloat8(
-											   DirectFunctionCall1(numeric_float8, NumericGetDatum(num)));
-				/* Defensive: Validate lambda range and check for NaN/Inf */
-				if (isfinite(parsed_lambda) && parsed_lambda >= 0.0 && parsed_lambda <= 1000.0)
-				{
-					lambda = parsed_lambda;
-				}
-				else
-				{
-					elog(WARNING,
-						 "ndb_cuda_lr_train: invalid lambda %f (must be finite, >= 0, <= 1000.0), using default %f",
-						 parsed_lambda, default_lambda);
-					lambda = default_lambda;
-				}
-			}
-		}
-	}
+	/* Hyperparameter parsing disabled - DirectFunctionCall causes crashes in GPU context */
 
-	/* Defensive: Final validation of hyperparameters */
+	/* Defensive: Final validation of hyperparameters (using defaults) */
 	if (max_iters <= 0 || max_iters > 1000000)
 	{
 		elog(WARNING,
@@ -418,7 +396,7 @@ ndb_cuda_lr_train(const float *features,
 		lambda = default_lambda;
 	}
 
-	/* Allocate host memory with defensive checks */
+	/* Allocate host memory for weights and gradients */
 	weights = (double *) palloc0(sizeof(double) * (size_t) feature_dim);
 	if (weights == NULL)
 	{
@@ -426,10 +404,10 @@ ndb_cuda_lr_train(const float *features,
 			*errstr = pstrdup("ndb_cuda_lr_train: palloc0 weights failed");
 		return -1;
 	}
-	nalloc(grad_weights, double, feature_dim);
+	grad_weights = (double *) palloc0(sizeof(double) * (size_t) feature_dim);
 	if (grad_weights == NULL)
 	{
-		nfree(weights);
+		pfree(weights);
 		if (errstr)
 			*errstr = pstrdup("ndb_cuda_lr_train: palloc grad_weights failed");
 		return -1;
@@ -442,8 +420,8 @@ ndb_cuda_lr_train(const float *features,
 	z_bytes = sizeof(double) * (size_t) n_samples;
 	weight_bytes_gpu = sizeof(float) * (size_t) feature_dim;
 
-	elog(DEBUG1,
-		 "ndb_cuda_lr_train: allocating GPU memory: feature_bytes=%zu "
+	elog(WARNING,
+		 "ndb_cuda_lr_train: About to allocate GPU memory: feature_bytes=%zu "
 		 "(%.2f MB), label_bytes=%zu",
 		 feature_bytes,
 		 feature_bytes / (1024.0 * 1024.0),
diff --git a/NeuronDB/src/gpu/rocm/gpu_backend_rocm.c b/NeuronDB/src/gpu/rocm/gpu_backend_rocm.c
index cd58461..77dee34 100644
--- a/NeuronDB/src/gpu/rocm/gpu_backend_rocm.c
+++ b/NeuronDB/src/gpu/rocm/gpu_backend_rocm.c
@@ -678,8 +678,9 @@ rocm_backend_dbscan_impl(const float *vectors,
 	 */
 	Assert(vectors && cluster_ids && num_vectors > 0 && dim > 0);
 
-	bool *visited = (bool *)palloc0(num_vectors * sizeof(bool));
-	int *neighbors = (int *)palloc(num_vectors * sizeof(int));
+	bool *visited = NULL;
+	nalloc(visited, bool, num_vectors);
+	nalloc(neighbors, int, num_vectors);
 	int cluster_id = -1;
 
 	/* Initialize all points as unclassified */
@@ -920,7 +921,7 @@ ndb_rocm_launch_kmeans_assign(const float *vectors,
 		|| assignments == NULL)
 		return -1;
 
-	assign32 = (int32_t *)palloc(sizeof(int32_t) * num_vectors);
+	nalloc(assign32, int32_t, num_vectors);
 
 	rc = gpu_kmeans_assign_hip(vectors,
 		centroids,
@@ -962,8 +963,8 @@ ndb_rocm_launch_kmeans_update(const float *vectors,
 		|| centroids == NULL)
 		return -1;
 
-	assign32 = (int32_t *)palloc(sizeof(int32_t) * num_vectors);
-	counts = (int32_t *)palloc0(sizeof(int32_t) * k);
+	nalloc(assign32, int32_t, num_vectors);
+		nalloc(counts, int32_t, k);
 
 	for (i = 0; i < num_vectors; i++)
 		assign32[i] = (int32_t)assignments[i];
diff --git a/NeuronDB/src/gpu/rocm/gpu_hf_rocm.c b/NeuronDB/src/gpu/rocm/gpu_hf_rocm.c
index baf600d..5bc7979 100644
--- a/NeuronDB/src/gpu/rocm/gpu_hf_rocm.c
+++ b/NeuronDB/src/gpu/rocm/gpu_hf_rocm.c
@@ -199,7 +199,7 @@ ndb_rocm_hf_load_model_weights(const char *model_name,
 								(size_t)config->embed_dim * config->hidden_dim * 2 * sizeof(float) +
 								(size_t)config->vocab_size * config->embed_dim * sizeof(float);
 
-			host_weights_buffer = palloc(total_weights_size);
+			nalloc(host_weights_buffer, char, total_weights_size);
 			if (!host_weights_buffer)
 			{
 				neurondb_onnx_unload_model(onnx_session);
@@ -216,7 +216,7 @@ ndb_rocm_hf_load_model_weights(const char *model_name,
 			hip_err = hipMalloc(&device_weights_buffer, total_weights_size);
 			if (hip_err != hipSuccess)
 			{
-				pfree(host_weights_buffer);
+				nfree(host_weights_buffer);
 				neurondb_onnx_unload_model(onnx_session);
 				if (errstr)
 					*errstr = psprintf("failed to allocate GPU memory for model weights: %s",
@@ -230,7 +230,7 @@ ndb_rocm_hf_load_model_weights(const char *model_name,
 			if (hip_err != hipSuccess)
 			{
 				hipFree(device_weights_buffer);
-				pfree(host_weights_buffer);
+				nfree(host_weights_buffer);
 				neurondb_onnx_unload_model(onnx_session);
 				if (errstr)
 					*errstr = psprintf("failed to copy model weights to GPU: %s",
@@ -322,11 +322,11 @@ ndb_rocm_hf_tokenize_text(const char *text,
 			}
 
 			*seq_len = onnx_seq_len;
-			pfree(onnx_token_ids);
+			nfree(onnx_token_ids);
 			return 0;
 		}
 		if (onnx_token_ids)
-			pfree(onnx_token_ids);
+			nfree(onnx_token_ids);
 	}
 	PG_CATCH();
 	{
@@ -2782,7 +2782,8 @@ ndb_rocm_hf_generate_batch(const char *model_name,
 	num_streams = (num_prompts < max_streams) ? num_prompts : max_streams;
 
 	/* Create ROCm streams for parallel processing */
-	streams = (hipStream_t *)palloc0(num_streams * sizeof(hipStream_t));
+		nalloc(streams, hipStream_t, num_streams);
+		MemSet(streams, 0, sizeof(hipStream_t) * num_streams);
 	for (i = 0; i < num_streams; i++)
 	{
 		hipError_t cuda_err = hipStreamCreate(&streams[i]);
diff --git a/NeuronDB/src/gpu/rocm/gpu_knn_rocm.c b/NeuronDB/src/gpu/rocm/gpu_knn_rocm.c
index 1b05219..11cc017 100644
--- a/NeuronDB/src/gpu/rocm/gpu_knn_rocm.c
+++ b/NeuronDB/src/gpu/rocm/gpu_knn_rocm.c
@@ -117,7 +117,9 @@ ndb_rocm_knn_pack(const struct KNNModel *model,
 		return -1;
 	}
 
-	blob = (bytea *) palloc(VARHDRSZ + payload_bytes);
+	char *tmp = NULL;
+	nalloc(tmp, char, VARHDRSZ + payload_bytes);
+	blob = (bytea *) tmp;
 	if (blob == NULL)
 	{
 		if (errstr)
@@ -389,7 +391,7 @@ ndb_rocm_knn_train(const float *features,
 			*errstr = pstrdup("HIP KNN train: feature array size exceeds MaxAllocSize");
 		return -1;
 	}
-	features_copy = (float *) palloc(sizeof(float) * (size_t) n_samples * (size_t) feature_dim);
+	nalloc(features_copy, float, (size_t) n_samples * (size_t) feature_dim);
 	if (features_copy == NULL)
 	{
 		if (errstr)
@@ -397,7 +399,7 @@ ndb_rocm_knn_train(const float *features,
 		return -1;
 	}
 
-	labels_copy = (double *) palloc(sizeof(double) * (size_t) n_samples);
+	nalloc(labels_copy, double, (size_t) n_samples);
 	if (labels_copy == NULL)
 	{
 		if (errstr)
@@ -564,7 +566,7 @@ ndb_rocm_knn_predict(const bytea * model_data,
 			*errstr = pstrdup("HIP KNN predict: distances array size exceeds MaxAllocSize");
 		return -1;
 	}
-	distances = (float *) palloc(sizeof(float) * hdr->n_samples);
+	nalloc(distances, float, hdr->n_samples);
 	if (distances == NULL)
 	{
 		if (errstr)
@@ -805,7 +807,7 @@ ndb_rocm_knn_evaluate_batch(const bytea * model_data,
 	}
 
 	/* Allocate predictions array */
-	predictions = (int *) palloc(sizeof(int) * (size_t) n_samples);
+	nalloc(predictions, int, (size_t) n_samples);
 	if (predictions == NULL)
 	{
 		if (errstr)
diff --git a/NeuronDB/src/gpu/rocm/gpu_svm_rocm.c b/NeuronDB/src/gpu/rocm/gpu_svm_rocm.c
index 1cafb37..d4f8ba4 100644
--- a/NeuronDB/src/gpu/rocm/gpu_svm_rocm.c
+++ b/NeuronDB/src/gpu/rocm/gpu_svm_rocm.c
@@ -648,7 +648,7 @@ ndb_rocm_svm_predict(const bytea * model_data,
 	{
 		if (errstr)
 			*errstr = pstrdup("HIP SVM predict: model_data too small for expected layout");
-		nfree((void *) detoasted);
+		pfree((void *) detoasted);
 		return -1;
 	}
 
@@ -659,7 +659,7 @@ ndb_rocm_svm_predict(const bytea * model_data,
 							   "has %d, input has %d",
 							   hdr->feature_dim,
 							   feature_dim);
-		nfree((void *) detoasted);
+		pfree((void *) detoasted);
 		return -1;
 	}
 
@@ -689,7 +689,7 @@ ndb_rocm_svm_predict(const bytea * model_data,
 		*confidence_out = fabs(prediction);
 
 	/* Free detoasted copy */
-	nfree((void *) detoasted);
+	pfree((void *) detoasted);
 
 	return 0;
 }
@@ -732,7 +732,7 @@ ndb_rocm_svm_predict_batch(const bytea * model_data,
 	{
 		if (errstr)
 			*errstr = pstrdup("HIP SVM batch predict: model_data too small");
-		nfree((void *) detoasted);
+		pfree((void *) detoasted);
 		return -1;
 	}
 
@@ -749,7 +749,7 @@ ndb_rocm_svm_predict_batch(const bytea * model_data,
 	{
 		if (errstr)
 			*errstr = pstrdup("HIP SVM batch predict: model_data too small for expected layout");
-		nfree((void *) detoasted);
+		pfree((void *) detoasted);
 		return -1;
 	}
 
@@ -759,7 +759,7 @@ ndb_rocm_svm_predict_batch(const bytea * model_data,
 		if (errstr)
 			*errstr = psprintf("HIP SVM batch predict: feature dimension mismatch (expected %d, got %d)",
 							   hdr->feature_dim, feature_dim);
-		nfree((void *) detoasted);
+		pfree((void *) detoasted);
 		return -1;
 	}
 
@@ -788,7 +788,7 @@ ndb_rocm_svm_predict_batch(const bytea * model_data,
 	}
 
 	/* Free detoasted copy */
-	nfree((void *) detoasted);
+	pfree((void *) detoasted);
 
 	return 0;
 }
diff --git a/NeuronDB/src/index/hnsw_am.c b/NeuronDB/src/index/hnsw_am.c
index 5e8a96c..f09f932 100644
--- a/NeuronDB/src/index/hnsw_am.c
+++ b/NeuronDB/src/index/hnsw_am.c
@@ -350,7 +350,6 @@ hnswbuild(Relation heap, Relation index, IndexInfo * indexInfo)
 {
 	HnswBuildState buildstate = {0};
 	Buffer		metaBuffer;
-	Page		metaPage = NULL;  /* Suppress unused variable warning */
 	HnswOptions *options = NULL;
 	IndexBuildResult *result = NULL;
 	int			m,
@@ -370,7 +369,6 @@ hnswbuild(Relation heap, Relation index, IndexInfo * indexInfo)
 	/* Initialize metadata page on block 0 */
 	metaBuffer = ReadBuffer(index, P_NEW);
 	LockBuffer(metaBuffer, BUFFER_LOCK_EXCLUSIVE);
-	metaPage = BufferGetPage(metaBuffer);
 
 	options = (HnswOptions *) indexInfo->ii_AmCache;
 	if (options == NULL)
@@ -446,7 +444,6 @@ static void
 hnswbuildempty(Relation index)
 {
 	Buffer		metaBuffer;
-	Page		metaPage;
 	HnswOptions opts;
 
 	/* Load options from relation to match CREATE INDEX reloptions */
@@ -455,7 +452,7 @@ hnswbuildempty(Relation index)
 	/* Initialize metadata page on block 0 */
 	metaBuffer = ReadBuffer(index, P_NEW);
 	LockBuffer(metaBuffer, BUFFER_LOCK_EXCLUSIVE);
-	metaPage = BufferGetPage(metaBuffer);
+	(void) BufferGetPage(metaBuffer);  /* Ensure page is valid */
 
 	hnswInitMetaPage(metaBuffer,
 					 opts.m,
@@ -544,7 +541,6 @@ hnswbulkdelete(IndexVacuumInfo * info,
 	if (stats == NULL)
 	{
 		nalloc(new_stats, IndexBulkDeleteResult, 1);
-		memset(new_stats, 0, sizeof(IndexBulkDeleteResult));
 		stats = new_stats;
 	}
 
@@ -685,9 +681,9 @@ hnswbulkdelete(IndexVacuumInfo * info,
 				}
 
 				{
-					ItemId		itemId = PageGetItemId(nodePage, FirstOffsetNumber);
+					ItemId		itemId2 = PageGetItemId(nodePage, FirstOffsetNumber);
 
-					if (!ItemIdIsValid(itemId))
+					if (!ItemIdIsValid(itemId2))
 					{
 						UnlockReleaseBuffer(nodeBuf);
 						nodeBuf = InvalidBuffer;
@@ -700,7 +696,7 @@ hnswbulkdelete(IndexVacuumInfo * info,
 						continue;
 					}
 
-					node = (HnswNode) PageGetItem(nodePage, itemId);
+					node = (HnswNode) PageGetItem(nodePage, itemId2);
 					if (node == NULL)
 					{
 						UnlockReleaseBuffer(nodeBuf);
@@ -815,7 +811,6 @@ hnswvacuumcleanup(IndexVacuumInfo * info, IndexBulkDeleteResult * stats)
 	if (stats == NULL)
 	{
 		nalloc(new_stats, IndexBulkDeleteResult, 1);
-		memset(new_stats, 0, sizeof(IndexBulkDeleteResult));
 		stats = new_stats;
 	}
 	return stats;
@@ -922,10 +917,14 @@ hnswoptions(Datum reloptions, bool validate)
 					
 					if (eq_pos != NULL)
 					{
+						char *param_name;
+						char *param_value_str;
+						int param_value;
+						
 						*eq_pos = '\0';
-						char *param_name = elem_str;
-						char *param_value_str = eq_pos + 1;
-						int param_value = atoi(param_value_str);
+						param_name = elem_str;
+						param_value_str = eq_pos + 1;
+						param_value = atoi(param_value_str);
 						
 						if (strcmp(param_name, "m") == 0)
 						{
@@ -943,7 +942,7 @@ hnswoptions(Datum reloptions, bool validate)
 							found_ef_search = true;
 						}
 					}
-					pfree(elem_str);
+					nfree(elem_str);
 				}
 			}
 			
@@ -1624,14 +1623,14 @@ hnswExtractVectorData(Datum value, Oid typeOid, int *out_dim, MemoryContext ctx)
 		VectorF16  *hv = (VectorF16 *) PG_DETOAST_DATUM(value);
 		bool		needsFree = false;
 
-		if (hv != DatumGetPointer(value))
+		if (hv != (VectorF16 *) DatumGetPointer(value))
 			needsFree = true;
 
 		NDB_CHECK_NULL(hv, "halfvec");
 		if (hv->dim <= 0 || hv->dim > 32767)
 		{
 			if (needsFree)
-				pfree(hv);
+				nfree(hv);
 			ereport(ERROR,
 					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
 					 errmsg("hnsw: invalid halfvec dimension %d", hv->dim)));
@@ -1651,14 +1650,14 @@ hnswExtractVectorData(Datum value, Oid typeOid, int *out_dim, MemoryContext ctx)
 		VectorMap  *sv = (VectorMap *) PG_DETOAST_DATUM(value);
 		bool		needsFree = false;
 
-		if (sv != DatumGetPointer(value))
+		if (sv != (VectorMap *) DatumGetPointer(value))
 			needsFree = true;
 
 		NDB_CHECK_NULL(sv, "sparsevec");
 		if (sv->total_dim <= 0 || sv->total_dim > 32767)
 		{
 			if (needsFree)
-				pfree(sv);
+				nfree(sv);
 			ereport(ERROR,
 					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
 					 errmsg("hnsw: invalid sparsevec total_dim %d", sv->total_dim)));
@@ -1691,7 +1690,7 @@ hnswExtractVectorData(Datum value, Oid typeOid, int *out_dim, MemoryContext ctx)
 		VarBit	   *bit_vec = (VarBit *) PG_DETOAST_DATUM(value);
 		bool		needsFree = false;
 
-		if (bit_vec != DatumGetPointer(value))
+		if (bit_vec != (VarBit *) DatumGetPointer(value))
 			needsFree = true;
 
 		NDB_CHECK_NULL(bit_vec, "bit vector");
@@ -1703,7 +1702,7 @@ hnswExtractVectorData(Datum value, Oid typeOid, int *out_dim, MemoryContext ctx)
 			if (nbits <= 0 || nbits > 32767)
 			{
 				if (needsFree)
-					pfree(bit_vec);
+					nfree(bit_vec);
 				ereport(ERROR,
 						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
 						 errmsg("hnsw: invalid bit vector length %d", nbits)));
@@ -1807,6 +1806,7 @@ hnswSearch(Relation index,
 				worstIdx;
 	float4		worstDist,
 				minDist;
+	BlockNumber numBlocks;	/* Computed once and reused throughout function */
 
 	/* Defensive: no vectors yet */
 	if (metaPage->entryPoint == InvalidBlockNumber)
@@ -1817,8 +1817,6 @@ hnswSearch(Relation index,
 		return;
 	}
 
-	BlockNumber numBlocks;	/* Computed once and reused throughout function */
-
 	/* Enforce k <= efSearch: effective k is min(k, efSearch) */
 	if (k > efSearch)
 		k = efSearch;
@@ -1848,7 +1846,6 @@ hnswSearch(Relation index,
 								numBlocks)));
 			}
 			nalloc(visitedSet, bool, numBlocks);
-			memset(visitedSet, 0, visitedSetSize);
 		}
 		visitedCount = 0;
 
@@ -1905,6 +1902,9 @@ hnswSearch(Relation index,
 
 				if (node->level >= level)
 				{
+					BlockNumber *neighborBlocks = NULL;
+					int			validNeighborCount = 0;
+					
 					neighbors = HnswGetNeighborsSafe(node, level, metaPage->m);
 					neighborCount = node->neighborCount[level];
 
@@ -1912,8 +1912,6 @@ hnswSearch(Relation index,
 					neighborCount = hnswValidateNeighborCount(neighborCount, metaPage->m, level);
 
 					/* Copy neighbor block numbers to avoid deadlock - unlock nodeBuf first */
-					BlockNumber *neighborBlocks = NULL;
-					int			validNeighborCount = 0;
 
 					if (neighborCount > 0)
 					{
@@ -2073,6 +2071,8 @@ hnswSearch(Relation index,
 		for (i = 0; i < candidateCount && candidateCount < efSearch; i++)
 		{
 			BlockNumber candidate = candidates[i];
+			BlockNumber *neighborBlocks = NULL;
+			int			validNeighborCount = 0;
 
 			if (!hnswValidateBlockNumber(candidate, index))
 			{
@@ -2111,9 +2111,6 @@ hnswSearch(Relation index,
 			neighborCount = hnswValidateNeighborCount(neighborCount, metaPage->m, 0);
 
 			/* Copy neighbor block numbers to avoid deadlock - unlock nodeBuf first */
-			BlockNumber *neighborBlocks = NULL;
-			int			validNeighborCount = 0;
-
 			if (neighborCount > 0)
 			{
 				nalloc(neighborBlocks, BlockNumber, neighborCount);
diff --git a/NeuronDB/src/index/index_hnsw_tenant.c b/NeuronDB/src/index/index_hnsw_tenant.c
index 161ba66..4aaf8b4 100644
--- a/NeuronDB/src/index/index_hnsw_tenant.c
+++ b/NeuronDB/src/index/index_hnsw_tenant.c
@@ -135,7 +135,7 @@ hnsw_tenant_search(PG_FUNCTION_ARGS)
 			tenant_id_long = strtol(tenant_id_str, &endptr, 10);
 			if (errno != 0 || *endptr != '\0' || tenant_id_long < 0 || tenant_id_long > INT_MAX)
 			{
-				pfree(tenant_id_str);
+				nfree(tenant_id_str);
 				ereport(ERROR,
 						(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),
 						 errmsg("tenant_id must be a non-negative integer, got: %s", tenant_id_str)));
@@ -410,7 +410,7 @@ get_hnsw_tenant_table(const char *table, const char *col, int32 tenant_id)
 	char *buf = NULL;
 
 	n = strlen(table) + strlen(col) + 48;
-	buf = (char *) palloc(n);
+	nalloc(buf, char, n);
 	snprintf(buf, n, "%s%s_%s_%d", HNSW_INDEX_PREFIX, table, col, tenant_id);
 
 	return buf;
diff --git a/NeuronDB/src/index/index_rerank.c b/NeuronDB/src/index/index_rerank.c
index 8fdc3a3..3231920 100644
--- a/NeuronDB/src/index/index_rerank.c
+++ b/NeuronDB/src/index/index_rerank.c
@@ -49,7 +49,7 @@ get_rerank_cache_table(const char *tbl, const char *col)
 
 	Assert(tbl != NULL && col != NULL);
 
-	buf = (char *) palloc(strlen(tbl) + strlen(col) + 32);
+	nalloc(buf, char, strlen(tbl) + strlen(col) + 32);
 
 	snprintf(buf,
 			 strlen(tbl) + strlen(col) + 32,
diff --git a/NeuronDB/src/index/index_temporal.c b/NeuronDB/src/index/index_temporal.c
index 92abb3c..9a85331 100644
--- a/NeuronDB/src/index/index_temporal.c
+++ b/NeuronDB/src/index/index_temporal.c
@@ -47,7 +47,7 @@ get_temporal_index_table(const char *table, const char *col)
 {
 	char *buf = NULL;
 
-	buf = palloc(strlen(table) + strlen(col) + 32);
+	nalloc(buf, char, strlen(table) + strlen(col) + 32);
 	snprintf(buf,
 			 strlen(table) + strlen(col) + 32,
 			 "__tvx_index_%s_%s",
@@ -341,7 +341,7 @@ VectorToLiteral(Vector *v)
 	char *buf = NULL;
 
 	out = vector_out_internal(v);
-	buf = palloc(strlen(out) + 4);
+	nalloc(buf, char, strlen(out) + 4);
 	snprintf(buf, strlen(out) + 4, "'%s'", out);
 
 	return buf;
diff --git a/NeuronDB/src/index/index_validator.c b/NeuronDB/src/index/index_validator.c
index ccf4a32..fbce83c 100644
--- a/NeuronDB/src/index/index_validator.c
+++ b/NeuronDB/src/index/index_validator.c
@@ -644,7 +644,7 @@ check_hnsw_connectivity(Relation index, ValidateResult * result)
 			int			queueTail = 0;
 			int			queueSize = totalNodes;
 
-			queue = (BlockNumber *) palloc(queueSize * sizeof(BlockNumber));
+			nalloc(queue, BlockNumber, queueSize);
 			queue[queueTail++] = entryPoint;
 			visited[entryPoint] = true;
 
@@ -1144,7 +1144,7 @@ neurondb_rebuild_index(PG_FUNCTION_ARGS)
 			session = ndb_spi_session_begin(CurrentMemoryContext, false);
 			if (session == NULL)
 			{
-				pfree(indexInfo);
+				nfree(indexInfo);
 				nfree(inner_indexName);
 				nfree(rebuildCmd);
 				ereport(ERROR,
diff --git a/NeuronDB/src/llm/llm_router.c b/NeuronDB/src/llm/llm_router.c
index cadbb82..e3e8e30 100644
--- a/NeuronDB/src/llm/llm_router.c
+++ b/NeuronDB/src/llm/llm_router.c
@@ -941,9 +941,7 @@ ndb_llm_route_embed(const NdbLLMConfig *cfg,
 					}
 					else
 					{
-						input_data = (float *) palloc(
-													  token_length
-													  * sizeof(float));
+						nalloc(input_data, float, token_length);
 						for (i = 0; i < token_length;
 							 i++)
 							input_data[i] = (float)
@@ -994,10 +992,7 @@ ndb_llm_route_embed(const NdbLLMConfig *cfg,
 							*dim_out =
 								output_tensor
 								->size;
-							*vec_out = (float *) palloc(
-														*dim_out
-														* sizeof(
-																 float));
+							nalloc(*vec_out, float, *dim_out);
 							memcpy(*vec_out,
 								   output_tensor
 								   ->data,
@@ -1312,10 +1307,14 @@ ndb_llm_route_complete_batch(const NdbLLMConfig *cfg,
 	/* Initialize output */
 	out->num_items = num_prompts;
 	out->num_success = 0;
-	out->texts = (char **) palloc0(num_prompts * sizeof(char *));
-	out->tokens_in = (int *) palloc0(num_prompts * sizeof(int));
-	out->tokens_out = (int *) palloc0(num_prompts * sizeof(int));
-	out->http_status = (int *) palloc0(num_prompts * sizeof(int));
+		nalloc(out->texts, char *, num_prompts);
+		MemSet(out->texts, 0, sizeof(char *) * num_prompts);
+		nalloc(out->tokens_in, int, num_prompts);
+		MemSet(out->tokens_in, 0, sizeof(int) * num_prompts);
+		nalloc(out->tokens_out, int, num_prompts);
+		MemSet(out->tokens_out, 0, sizeof(int) * num_prompts);
+		nalloc(out->http_status, int, num_prompts);
+		MemSet(out->http_status, 0, sizeof(int) * num_prompts);
 
 	if (cfg->provider == NULL || provider_is(cfg->provider, "huggingface")
 		|| provider_is(cfg->provider, "hf-http")
@@ -1333,8 +1332,8 @@ ndb_llm_route_complete_batch(const NdbLLMConfig *cfg,
 			char	   *gpu_err = NULL;
 
 
-			batch_results = (NdbCudaHfBatchResult *) palloc0(
-															 num_prompts * sizeof(NdbCudaHfBatchResult));
+			nalloc(batch_results, NdbCudaHfBatchResult, num_prompts);
+			MemSet(batch_results, 0, sizeof(NdbCudaHfBatchResult) * num_prompts);
 			rc = neurondb_gpu_hf_complete_batch(cfg->model,
 												prompts,
 												num_prompts,
@@ -1552,8 +1551,10 @@ ndb_llm_route_rerank_batch(const NdbLLMConfig *cfg,
 	}
 
 	/* Initialize output */
-	*scores_out = (float **) palloc0(num_queries * sizeof(float *));
-	*nscores_out = (int *) palloc0(num_queries * sizeof(int));
+		nalloc(*scores_out, float *, num_queries);
+		MemSet(*scores_out, 0, sizeof(float *) * num_queries);
+		nalloc(*nscores_out, int, num_queries);
+		MemSet(*nscores_out, 0, sizeof(int) * num_queries);
 
 	/* For now, process sequentially */
 	for (i = 0; i < num_queries; i++)
@@ -1638,8 +1639,10 @@ ndb_llm_route_embed_batch(const NdbLLMConfig *cfg,
 	}
 
 	/* Initialize output */
-	vecs = (float **) palloc0(num_texts * sizeof(float *));
-	dims = (int *) palloc0(num_texts * sizeof(int));
+		nalloc(vecs, float *, num_texts);
+		MemSet(vecs, 0, sizeof(float *) * num_texts);
+		nalloc(dims, int, num_texts);
+		MemSet(dims, 0, sizeof(int) * num_texts);
 
 	if (cfg->provider == NULL || provider_is(cfg->provider, "huggingface")
 		|| provider_is(cfg->provider, "hf-http"))
diff --git a/NeuronDB/src/llm/llm_runtime.c b/NeuronDB/src/llm/llm_runtime.c
index 4bc872e..4c49825 100644
--- a/NeuronDB/src/llm/llm_runtime.c
+++ b/NeuronDB/src/llm/llm_runtime.c
@@ -578,7 +578,7 @@ ndb_llm_complete(PG_FUNCTION_ARGS)
 			if (len >= 2 && raw_model[0] == '"' && raw_model[len - 1] == '"')
 			{
 				/* Allocate new string without quotes */
-				model_from_params = palloc(len - 1);
+				nalloc(model_from_params, char, len - 1);
 				memcpy(model_from_params, raw_model + 1, len - 2);
 				model_from_params[len - 2] = '\0';
 				nfree(raw_model);
@@ -839,7 +839,8 @@ ndb_llm_complete(PG_FUNCTION_ARGS)
 					if (len >= 2 && error_text[0] == '"' && error_text[len - 1] == '"')
 					{
 						/* Remove quotes */
-						char	   *unquoted = palloc(len - 1);
+						char *unquoted = NULL;
+						nalloc(unquoted, char, len - 1);
 						
 						memcpy(unquoted, error_text + 1, len - 2);
 						unquoted[len - 2] = '\0';
@@ -1506,7 +1507,7 @@ ndb_llm_rerank(PG_FUNCTION_ARGS)
 
 		funcctx = SRF_FIRSTCALL_INIT();
 		old = MemoryContextSwitchTo(funcctx->multi_call_memory_ctx);
-		new_ctx = palloc0(sizeof(Ctx));
+		nalloc(new_ctx, Ctx, 1);
 		arr = PG_GETARG_ARRAYTYPE_P(1);
 		new_ctx->ndocs = ArrayGetNItems(ARR_NDIM(arr), ARR_DIMS(arr));
 		new_ctx->i = 0;
@@ -1925,7 +1926,7 @@ ndb_llm_complete_batch(PG_FUNCTION_ARGS)
 								num_prompts,
 								nprompt_elems)));
 
-			prompts = (char **) palloc0(num_prompts * sizeof(char *));
+			nalloc(prompts, char *, num_prompts);
 			for (i = 0; i < num_prompts; i++)
 			{
 				if (prompt_nulls[i])
@@ -1967,14 +1968,12 @@ ndb_llm_complete_batch(PG_FUNCTION_ARGS)
 
 		/* Store state */
 		{
-			BatchCompleteState *state =
-				(BatchCompleteState *) palloc0(
-											   sizeof(BatchCompleteState));
+			BatchCompleteState *state = NULL;
+			nalloc(state, BatchCompleteState, 1);
 
 			state->num_prompts = num_prompts;
 			state->current_idx = 0;
-			state->batch_resp = (NdbLLMBatchResp *) palloc0(
-															sizeof(NdbLLMBatchResp));
+			nalloc(state->batch_resp, NdbLLMBatchResp, 1);
 			/* Deep copy batch_resp structure */
 			state->batch_resp->num_items = batch_resp.num_items;
 			state->batch_resp->num_success = batch_resp.num_success;
@@ -1982,8 +1981,7 @@ ndb_llm_complete_batch(PG_FUNCTION_ARGS)
 			{
 				int			j;
 
-				state->batch_resp->texts = (char **) palloc0(
-															 batch_resp.num_items * sizeof(char *));
+				nalloc(state->batch_resp->texts, char *, batch_resp.num_items);
 				for (j = 0; j < batch_resp.num_items; j++)
 				{
 					if (batch_resp.texts[j])
@@ -2243,7 +2241,7 @@ ndb_llm_rerank_batch(PG_FUNCTION_ARGS)
 								num_queries,
 								nquery_elems)));
 
-			queries = (char **) palloc0(num_queries * sizeof(char *));
+			nalloc(queries, char *, num_queries);
 			for (i = 0; i < num_queries; i++)
 			{
 				if (query_nulls[i])
@@ -2254,9 +2252,8 @@ ndb_llm_rerank_batch(PG_FUNCTION_ARGS)
 		}
 
 		/* Extract documents from 2D array */
-		ndocs_array = (int *) palloc0(num_queries * sizeof(int));
-		docs_array =
-			(const char ***) palloc0(num_queries * sizeof(char **));
+		nalloc(ndocs_array, int, num_queries);
+		nalloc(docs_array, const char **, num_queries);
 
 		/*
 		 * Extract documents from array (handles both 1D array of arrays and
@@ -2335,7 +2332,8 @@ ndb_llm_rerank_batch(PG_FUNCTION_ARGS)
 									  &ndoc_elems);
 
 					ndocs_array[i] = ndoc_elems;
-					docs = (char **) palloc0(ndoc_elems * sizeof(char *));
+					nalloc(docs, char *, ndoc_elems);
+					MemSet(docs, 0, sizeof(char *) * ndoc_elems);
 					for (j = 0; j < ndoc_elems; j++)
 					{
 						if (doc_nulls[j])
@@ -2478,7 +2476,8 @@ ndb_llm_rerank_batch(PG_FUNCTION_ARGS)
 									  &ndoc_elems);
 
 					ndocs_array[i] = ndoc_elems;
-					docs = (char **) palloc0(ndoc_elems * sizeof(char *));
+					nalloc(docs, char *, ndoc_elems);
+					MemSet(docs, 0, sizeof(char *) * ndoc_elems);
 					for (j = 0; j < ndoc_elems; j++)
 					{
 						if (doc_nulls[j])
@@ -2544,8 +2543,9 @@ ndb_llm_rerank_batch(PG_FUNCTION_ARGS)
 
 		/* Store state */
 		{
-			BatchRerankState *state = (BatchRerankState *) palloc0(
-																   sizeof(BatchRerankState));
+			BatchRerankState *state = NULL;
+			nalloc(state, BatchRerankState, 1);
+			MemSet(state, 0, sizeof(BatchRerankState));
 
 			if (!state)
 			{
diff --git a/NeuronDB/src/ml/analytics.c b/NeuronDB/src/ml/analytics.c
index a19520d..515e0fd 100644
--- a/NeuronDB/src/ml/analytics.c
+++ b/NeuronDB/src/ml/analytics.c
@@ -51,6 +51,12 @@ feedback_loop_integrate(PG_FUNCTION_ARGS)
 	text	   *query;
 	text	   *result;
 	float4		user_rating;
+	char *query_str = NULL;
+	char *result_str = NULL;
+	const char *tbl_def;
+	int			ret;
+	NdbSpiSession *spi_session = NULL;
+	MemoryContext oldcontext;
 
 	/* Validate argument count */
 	if (PG_NARGS() < 3)
@@ -61,13 +67,6 @@ feedback_loop_integrate(PG_FUNCTION_ARGS)
 	query = PG_GETARG_TEXT_PP(0);
 	result = PG_GETARG_TEXT_PP(1);
 	user_rating = PG_GETARG_FLOAT4(2);
-	char *query_str = NULL;
-	char *result_str = NULL;
-	const char *tbl_def;
-	int			ret;
-
-	NdbSpiSession *spi_session = NULL;
-	MemoryContext oldcontext;
 
 	query_str = text_to_cstring(query);
 	result_str = text_to_cstring(result);
diff --git a/NeuronDB/src/ml/ml_catalog.c b/NeuronDB/src/ml/ml_catalog.c
index 12d8747..894408b 100644
--- a/NeuronDB/src/ml/ml_catalog.c
+++ b/NeuronDB/src/ml/ml_catalog.c
@@ -67,7 +67,7 @@ ml_catalog_default_project(const char *algorithm, const char *training_table)
 	 * Sanitize table name: replace spaces and other problematic chars with
 	 * underscores
 	 */
-	sanitized_table = palloc(strlen(training_table) + 1);
+	nalloc(sanitized_table, char, strlen(training_table) + 1);
 	j = 0;
 	for (i = 0; training_table[i] != '\0'; i++)
 	{
diff --git a/NeuronDB/src/ml/ml_deeplearning.c b/NeuronDB/src/ml/ml_deeplearning.c
index fff2d25..39eaf15 100644
--- a/NeuronDB/src/ml/ml_deeplearning.c
+++ b/NeuronDB/src/ml/ml_deeplearning.c
@@ -483,12 +483,13 @@ dl_predict(PG_FUNCTION_ARGS)
 					else
 					{
 						/* Create input tensor from input_data array */
-						input_tensor = (ONNXTensor *) palloc0(sizeof(ONNXTensor));
+						nalloc(input_tensor, ONNXTensor, 1);
+						MemSet(input_tensor, 0, sizeof(ONNXTensor));
 						input_tensor->ndim = 1;
-						input_tensor->shape = (int64 *) palloc(sizeof(int64) * 1);
+						nalloc(input_tensor->shape, int64, 1);
 						input_tensor->shape[0] = n_inputs;
 						input_tensor->size = n_inputs;
-						input_tensor->data = (float *) palloc(sizeof(float) * n_inputs);
+						nalloc(input_tensor->data, float, n_inputs);
 
 						/* Copy input data */
 						for (d = 0; d < n_inputs; d++)
@@ -579,7 +580,7 @@ dl_predict(PG_FUNCTION_ARGS)
 	}
 
 	/* Generate predictions */
-	predictions = (float *) palloc(n_outputs * sizeof(float));
+	nalloc(predictions, float, n_outputs);
 #ifdef HAVE_ONNX_RUNTIME
 	/* If we have ONNX output, use it */
 	if (output_tensor != NULL && output_tensor->data != NULL)
@@ -636,7 +637,7 @@ dl_predict(PG_FUNCTION_ARGS)
 	}
 
 	/* Build result array */
-	elems = (Datum *) palloc(n_outputs * sizeof(Datum));
+	nalloc(elems, Datum, n_outputs);
 	for (i = 0; i < n_outputs; i++)
 		elems[i] = Float4GetDatum(predictions[i]);
 
diff --git a/NeuronDB/src/ml/ml_feature_store.c b/NeuronDB/src/ml/ml_feature_store.c
index 098dee6..baf7543 100644
--- a/NeuronDB/src/ml/ml_feature_store.c
+++ b/NeuronDB/src/ml/ml_feature_store.c
@@ -340,7 +340,7 @@ neurondb_get_features(PG_FUNCTION_ARGS)
 						  &feat_nulls,
 						  &num_features);
 
-		feature_names = (char **) palloc(sizeof(char *) * num_features);
+		nalloc(feature_names, char *, num_features);
 		for (i = 0; i < num_features; ++i)
 			feature_names[i] = TextDatumGetCString(feat_datums[i]);
 	}
@@ -406,7 +406,7 @@ neurondb_get_features(PG_FUNCTION_ARGS)
 			tuple = SPI_tuptable->vals[0];
 			tupdesc = SPI_tuptable->tupdesc;
 			nf = ndefs;
-			obj_pairs = (JsonbPair *) palloc(sizeof(JsonbPair) * nf);
+			nalloc(obj_pairs, JsonbPair, nf);
 
 			for (j = 0; j < ndefs; ++j)
 			{
@@ -492,8 +492,8 @@ neurondb_get_features(PG_FUNCTION_ARGS)
 		{
 			/* entity row not found: return nulls for all features */
 			int			nf = ndefs;
-			JsonbPair  *obj_pairs =
-				(JsonbPair *) palloc(sizeof(JsonbPair) * nf);
+			JsonbPair *obj_pairs = NULL;
+			nalloc(obj_pairs, JsonbPair, nf);
 
 			for (j = 0; j < ndefs; ++j)
 			{
diff --git a/NeuronDB/src/ml/ml_histogram.c b/NeuronDB/src/ml/ml_histogram.c
index d9955ce..2691ef8 100644
--- a/NeuronDB/src/ml/ml_histogram.c
+++ b/NeuronDB/src/ml/ml_histogram.c
@@ -122,7 +122,7 @@ similarity_histogram(PG_FUNCTION_ARGS)
 				 errmsg("Need at least 2 vectors")));
 
 	/* Sample random pairs and compute distances */
-	distances = (double *) palloc(sizeof(double) * num_samples);
+	nalloc(distances, double, num_samples);
 
 	for (i = 0; i < num_samples; i++)
 	{
diff --git a/NeuronDB/src/ml/ml_hybrid_search.c b/NeuronDB/src/ml/ml_hybrid_search.c
index ea3ba25..60ed768 100644
--- a/NeuronDB/src/ml/ml_hybrid_search.c
+++ b/NeuronDB/src/ml/ml_hybrid_search.c
@@ -163,7 +163,7 @@ hybrid_search_fusion(PG_FUNCTION_ARGS)
 		 num_docs,
 		 semantic_weight);
 
-	scores = (HybridScore *) palloc(sizeof(HybridScore) * num_docs);
+	nalloc(scores, HybridScore, num_docs);
 
 	/* Initialize scores */
 	for (i = 0; i < num_docs; i++)
@@ -234,7 +234,7 @@ hybrid_search_fusion(PG_FUNCTION_ARGS)
 	/* Sort by hybrid score */
 	qsort(scores, num_docs, sizeof(HybridScore), hybrid_score_cmp);
 
-	result_datums = (Datum *) palloc(sizeof(Datum) * num_docs);
+	nalloc(result_datums, Datum, num_docs);
 	for (i = 0; i < num_docs; i++)
 		result_datums[i] = Int32GetDatum(scores[i].doc_id);
 
diff --git a/NeuronDB/src/ml/ml_lightgbm.c b/NeuronDB/src/ml/ml_lightgbm.c
index 735677f..4e368de 100644
--- a/NeuronDB/src/ml/ml_lightgbm.c
+++ b/NeuronDB/src/ml/ml_lightgbm.c
@@ -138,7 +138,9 @@ lightgbm_model_serialize_to_bytea(int n_estimators, int max_depth, float learnin
 	appendBinaryStringInfo(&buf, boosting_type, type_len);
 
 	total_size = VARHDRSZ + buf.len;
-	result = (bytea *) palloc(total_size);
+	char *tmp = NULL;
+	nalloc(tmp, char, total_size);
+	result = (bytea *) tmp;
 	SET_VARSIZE(result, total_size);
 	memcpy(VARDATA(result), buf.data, buf.len);
 	nfree(buf.data);
@@ -261,7 +263,8 @@ lightgbm_gpu_train(MLGpuModel *model, const MLGpuTrainSpec *spec, char **errstr)
 												 CStringGetDatum(metrics_json.data)));
 	nfree(metrics_json.data);
 
-	state = (LightGBMGpuModelState *) palloc0(sizeof(LightGBMGpuModelState));
+		nalloc(state, LightGBMGpuModelState, 1);
+		MemSet(state, 0, sizeof(LightGBMGpuModelState));
 	state->model_blob = model_data;
 	state->metrics = metrics;
 	state->n_estimators = n_estimators;
@@ -402,7 +405,9 @@ lightgbm_gpu_serialize(const MLGpuModel *model, bytea * *payload_out,
 	}
 
 	payload_size = VARSIZE(state->model_blob);
-	payload_copy = (bytea *) palloc(payload_size);
+	char *tmp = NULL;
+	nalloc(tmp, char, payload_size);
+	payload_copy = (bytea *) tmp;
 	memcpy(payload_copy, state->model_blob, payload_size);
 
 	if (payload_out != NULL)
@@ -443,7 +448,9 @@ lightgbm_gpu_deserialize(MLGpuModel *model, const bytea * payload,
 	}
 
 	payload_size = VARSIZE(payload);
-	payload_copy = (bytea *) palloc(payload_size);
+	char *tmp = NULL;
+	nalloc(tmp, char, payload_size);
+	payload_copy = (bytea *) tmp;
 	memcpy(payload_copy, payload, payload_size);
 
 	if (lightgbm_model_deserialize_from_bytea(payload_copy, &n_estimators, &max_depth, &learning_rate, &n_features, boosting_type, sizeof(boosting_type)) != 0)
@@ -454,7 +461,8 @@ lightgbm_gpu_deserialize(MLGpuModel *model, const bytea * payload,
 		return false;
 	}
 
-	state = (LightGBMGpuModelState *) palloc0(sizeof(LightGBMGpuModelState));
+		nalloc(state, LightGBMGpuModelState, 1);
+		MemSet(state, 0, sizeof(LightGBMGpuModelState));
 	state->model_blob = payload_copy;
 	state->n_estimators = n_estimators;
 	state->max_depth = max_depth;
@@ -466,7 +474,9 @@ lightgbm_gpu_deserialize(MLGpuModel *model, const bytea * payload,
 	if (metadata != NULL)
 	{
 		int			metadata_size = VARSIZE(metadata);
-		Jsonb	   *metadata_copy = (Jsonb *) palloc(metadata_size);
+		char *tmp = NULL;
+		nalloc(tmp, char, metadata_size);
+		Jsonb *metadata_copy = (Jsonb *) tmp;
 
 		memcpy(metadata_copy, metadata, metadata_size);
 		state->metrics = metadata_copy;
diff --git a/NeuronDB/src/ml/ml_linear_regression.c b/NeuronDB/src/ml/ml_linear_regression.c
index 30aad47..8d57f77 100644
--- a/NeuronDB/src/ml/ml_linear_regression.c
+++ b/NeuronDB/src/ml/ml_linear_regression.c
@@ -42,6 +42,7 @@
 #include "neurondb_spi_safe.h"
 #include "neurondb_sql.h"
 #include "neurondb_constants.h"
+#include "neurondb_guc.h"
 #include "neurondb_json.h"
 #include "utils/lsyscache.h"
 
@@ -964,15 +965,18 @@ linreg_try_gpu_predict_catalog(int32 model_id,
 	if (payload == NULL)
 		goto cleanup;
 
-	/* Check if this is a GPU model using training_backend */
+	/* Check if this is a GPU model using training_backend or payload format */
 	{
 		bool		is_gpu_model = false;
-		JsonbIterator *it = NULL;
-		JsonbValue	v;
-		JsonbIteratorToken r;
+		uint32		payload_size;
 
+		/* First check metrics for training_backend */
 		if (metrics != NULL)
 		{
+			JsonbIterator *it = NULL;
+			JsonbValue	v;
+			JsonbIteratorToken r;
+
 			it = JsonbIteratorInit((JsonbContainer *) & metrics->root);
 			while ((r = JsonbIteratorNext(&it, &v, true)) != WJB_DONE)
 			{
@@ -995,6 +999,48 @@ linreg_try_gpu_predict_catalog(int32 model_id,
 			}
 		}
 
+		/* If metrics check didn't find GPU indicator, check payload format */
+		/* GPU models start with NdbCudaLinRegModelHeader, CPU models start with uint8 training_backend */
+		if (!is_gpu_model)
+		{
+			payload_size = VARSIZE(payload) - VARHDRSZ;
+			
+			/* CPU format: first byte is training_backend (uint8), then n_features (int32) */
+			/* GPU format: first field is feature_dim (int32) */
+			/* Check if payload looks like GPU format (starts with int32, not uint8) */
+			if (payload_size >= sizeof(int32))
+			{
+				const int32 *first_int = (const int32 *) VARDATA(payload);
+				int32		first_value = *first_int;
+				
+				/* If first 4 bytes look like a reasonable feature_dim, check for GPU format */
+				if (first_value > 0 && first_value <= 100000)
+				{
+					/* Check if payload size matches GPU format */
+					if (payload_size >= sizeof(NdbCudaLinRegModelHeader))
+					{
+						const NdbCudaLinRegModelHeader *hdr = (const NdbCudaLinRegModelHeader *) VARDATA(payload);
+						
+						/* Validate header fields match the first int32 */
+						if (hdr->feature_dim == first_value &&
+							hdr->n_samples >= 0 && hdr->n_samples <= 1000000000)
+						{
+							size_t		expected_gpu_size = sizeof(NdbCudaLinRegModelHeader) +
+								sizeof(float) * (size_t) hdr->feature_dim;
+							
+							/* Size matches GPU format - likely a GPU model */
+							if (payload_size >= expected_gpu_size && payload_size < expected_gpu_size + 1000)
+							{
+								is_gpu_model = true;
+								elog(DEBUG1, "linreg_try_gpu_predict_catalog: detected GPU model by payload format (feature_dim=%d, n_samples=%d, payload_size=%u)",
+									 hdr->feature_dim, hdr->n_samples, payload_size);
+							}
+						}
+					}
+				}
+			}
+		}
+
 		if (!is_gpu_model)
 			goto cleanup;
 	}
@@ -1043,10 +1089,12 @@ linreg_load_model_from_catalog(int32 model_id, LinRegModel **out)
 	}
 
 	/* Skip GPU models - they should be handled by GPU prediction */
-	/* Inline the GPU check to avoid return statement crash */
+	/* Check both metrics and payload format to determine if this is a GPU model */
 	{
 		bool		is_gpu_model = false;
+		uint32		payload_size;
 
+		/* First check metrics for training_backend */
 		if (metrics != NULL)
 		{
 			JsonbIterator *it = NULL;
@@ -1075,8 +1123,51 @@ linreg_load_model_from_catalog(int32 model_id, LinRegModel **out)
 			}
 		}
 
+		/* If metrics check didn't find GPU indicator, check payload format */
+		/* GPU models start with NdbCudaLinRegModelHeader, CPU models start with uint8 training_backend */
+		if (!is_gpu_model)
+		{
+			payload_size = VARSIZE(payload) - VARHDRSZ;
+			
+			/* CPU format: first byte is training_backend (uint8), then n_features (int32) */
+			/* GPU format: first field is feature_dim (int32) */
+			/* Check if payload looks like GPU format (starts with int32, not uint8) */
+			if (payload_size >= sizeof(int32))
+			{
+				const int32 *first_int = (const int32 *) VARDATA(payload);
+				int32		first_value = *first_int;
+				
+				/* If first 4 bytes look like a reasonable feature_dim, check for GPU format */
+				if (first_value > 0 && first_value <= 100000)
+				{
+					/* Check if payload size matches GPU format */
+					if (payload_size >= sizeof(NdbCudaLinRegModelHeader))
+					{
+						const NdbCudaLinRegModelHeader *hdr = (const NdbCudaLinRegModelHeader *) VARDATA(payload);
+						
+						/* Validate header fields match the first int32 */
+						if (hdr->feature_dim == first_value &&
+							hdr->n_samples >= 0 && hdr->n_samples <= 1000000000)
+						{
+							size_t		expected_gpu_size = sizeof(NdbCudaLinRegModelHeader) +
+								sizeof(float) * (size_t) hdr->feature_dim;
+							
+							/* Size matches GPU format - likely a GPU model */
+							if (payload_size >= expected_gpu_size && payload_size < expected_gpu_size + 1000)
+							{
+								is_gpu_model = true;
+								elog(DEBUG1, "linreg_load_model_from_catalog: detected GPU model by payload format (feature_dim=%d, n_samples=%d, payload_size=%u)",
+									 hdr->feature_dim, hdr->n_samples, payload_size);
+							}
+						}
+					}
+				}
+			}
+		}
+
 		if (is_gpu_model)
 		{
+			elog(DEBUG1, "linreg_load_model_from_catalog: skipping GPU model (model_id=%d), use GPU prediction path", model_id);
 			nfree(payload);
 			nfree(metrics);
 
@@ -1084,18 +1175,8 @@ linreg_load_model_from_catalog(int32 model_id, LinRegModel **out)
 		}
 	}
 
-	/* Try to deserialize CPU model, but handle failures gracefully */
-	PG_TRY();
-	{
-		*out = linreg_model_deserialize(payload, NULL);
-	}
-	PG_CATCH();
-	{
-		FlushErrorState();
-		*out = NULL;
-		ereport(WARNING, (errmsg("linreg_load_model_from_catalog: CPU model deserialization failed, assuming GPU model")));
-	}
-	PG_END_TRY();
+	/* Try to deserialize CPU model */
+	*out = linreg_model_deserialize(payload, NULL);
 
 	nfree(payload);
 	nfree(metrics);
@@ -1474,11 +1555,20 @@ train_linear_regression(PG_FUNCTION_ARGS)
 					linreg_model.coefficients = NULL;
 				}
 
-				/* Update metrics to use training_backend=1 */
+				/* ALWAYS create metrics with training_backend=1 for GPU training */
+				/* Use metrics from GPU if available, otherwise create minimal metrics */
 				if (gpu_result.spec.metrics != NULL)
 				{
+					/* GPU provided metrics - use them (they already have training_backend=1) */
+					updated_metrics = gpu_result.spec.metrics;
+					elog(INFO, "neurondb: linear_regression: Using GPU-provided metrics with training_backend=1");
+				}
+				else
+				{
+					/* GPU didn't provide metrics (likely crashed during creation) - create minimal metrics */
 					StringInfoData metrics_buf;
 
+					elog(WARNING, "neurondb: linear_regression: GPU metrics is NULL, creating minimal metrics with training_backend=1");
 					initStringInfo(&metrics_buf);
 					appendStringInfo(&metrics_buf,
 									 "{\"algorithm\":\"linear_regression\","
@@ -1495,7 +1585,7 @@ train_linear_regression(PG_FUNCTION_ARGS)
 						nfree(metrics_buf.data);
 						ereport(ERROR,
 								(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),
-								 errmsg("neurondb: failed to parse metrics JSON")));
+								 errmsg("neurondb: failed to parse metrics JSON for GPU training")));
 					}
 					nfree(metrics_buf.data);
 				}
@@ -2508,27 +2598,32 @@ evaluate_linear_regression_by_model_id_jsonb(int32 model_id, text * table_name,
 	Jsonb *gpu_metrics = NULL;
 	bool		is_gpu_model = false;
 
-	MemoryContext currctx;
-
-	double *model_coefficients_ptr = NULL;
 	NdbSpiSession *eval_jsonb_spi_session = NULL;
 
-#ifdef NDB_GPU_CUDA
-	float *h_features = NULL;
-	double *h_targets = NULL;
-	int			valid_rows = 0;
-#endif
-
 
 	/* Load model from catalog - try CPU first, then GPU */
 	if (!linreg_load_model_from_catalog(model_id, &model))
 	{
 		/*
-		 * CPU model load failed - this might indicate a GPU-trained model or
-		 * corrupted data
+		 * CPU model load failed - this might indicate a GPU-trained model.
+		 * Try loading GPU payload instead.
 		 */
-		/* Return NULL instead of trying to create JSONB to avoid corruption */
-		return NULL;
+		if (!ml_catalog_fetch_model_payload(model_id, &gpu_payload, NULL, &gpu_metrics))
+		{
+			/* Neither CPU nor GPU model found */
+			return NULL;
+		}
+		
+		if (gpu_payload != NULL)
+		{
+			/* Mark this as a GPU model for later processing */
+			is_gpu_model = true;
+		}
+		else
+		{
+			/* No model data found */
+			return NULL;
+		}
 	}
 
 	tbl_str = text_to_cstring(table_name);
@@ -2605,18 +2700,71 @@ evaluate_linear_regression_by_model_id_jsonb(int32 model_id, text * table_name,
 			feat_is_array = true;
 	}
 
-	/*
-	 * GPU batch evaluation path for GPU models - uses optimized evaluation
-	 * kernel
-	 */
-	if (is_gpu_model && neurondb_gpu_is_available())
+	/* Unified evaluation: Determine predict function based on compute mode */
+	/* All metrics calculation is the same - only difference is predict function */
 	{
-#ifdef NDB_GPU_CUDA
-		const NdbCudaLinRegModelHeader *gpu_hdr;
+		bool		use_gpu_predict = false;
 		int			feat_dim = 0;
+#ifdef NDB_GPU_CUDA
+		const NdbCudaLinRegModelHeader *gpu_hdr = NULL;
+		const float *gpu_coefficients = NULL;
+#endif
 
-		/* Load GPU model header */
-		if (VARSIZE(gpu_payload) - VARHDRSZ < sizeof(NdbCudaLinRegModelHeader))
+		/* Determine if we should use GPU predict or CPU predict */
+		if (is_gpu_model && neurondb_gpu_is_available() && !NDB_COMPUTE_MODE_IS_CPU())
+		{
+			/* GPU model and GPU mode: use GPU predict */
+#ifdef NDB_GPU_CUDA
+			if (gpu_payload != NULL && VARSIZE(gpu_payload) - VARHDRSZ >= sizeof(NdbCudaLinRegModelHeader))
+			{
+				gpu_hdr = (const NdbCudaLinRegModelHeader *) VARDATA(gpu_payload);
+				feat_dim = gpu_hdr->feature_dim;
+				gpu_coefficients = (const float *) ((const char *) gpu_hdr + sizeof(NdbCudaLinRegModelHeader));
+				use_gpu_predict = true;
+			}
+#endif
+		}
+		else if (model != NULL)
+		{
+			/* CPU model or CPU mode: use CPU predict */
+			feat_dim = model->n_features;
+			use_gpu_predict = false;
+		}
+		else if (is_gpu_model && gpu_payload != NULL)
+		{
+			/* GPU model but CPU mode: convert to CPU format for CPU predict */
+#ifdef NDB_GPU_CUDA
+			if (VARSIZE(gpu_payload) - VARHDRSZ >= sizeof(NdbCudaLinRegModelHeader))
+			{
+				gpu_hdr = (const NdbCudaLinRegModelHeader *) VARDATA(gpu_payload);
+				gpu_coefficients = (const float *) ((const char *) gpu_hdr + sizeof(NdbCudaLinRegModelHeader));
+				feat_dim = gpu_hdr->feature_dim;
+
+				/* Convert GPU model to CPU format */
+				{
+					double *cpu_coefficients = NULL;
+					double		cpu_intercept = 0.0;
+					int			coef_idx;
+
+					cpu_intercept = gpu_hdr->intercept;
+					nalloc(cpu_coefficients, double, feat_dim);
+
+					for (coef_idx = 0; coef_idx < feat_dim; coef_idx++)
+						cpu_coefficients[coef_idx] = (double) gpu_coefficients[coef_idx];
+
+					/* Create temporary CPU model structure */
+					nalloc(model, LinRegModel, 1);
+					model->n_features = feat_dim;
+					model->intercept = cpu_intercept;
+					model->coefficients = cpu_coefficients;
+				}
+				use_gpu_predict = false;
+			}
+#endif
+		}
+
+		/* Ensure we have a valid model or GPU payload */
+		if (model == NULL && !use_gpu_predict)
 		{
 			NDB_SPI_SESSION_END(eval_jsonb_spi_session);
 			nfree(gpu_payload);
@@ -2626,452 +2774,250 @@ evaluate_linear_regression_by_model_id_jsonb(int32 model_id, text * table_name,
 			nfree(targ_str);
 			ereport(ERROR,
 					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-					 errmsg("neurondb: evaluate_linear_regression_by_model_id_jsonb: GPU payload too small"),
-					 errdetail("Payload size is %u bytes, minimum required is %zu bytes", VARSIZE(gpu_payload) - VARHDRSZ, sizeof(NdbCudaLinRegModelHeader)),
-					 errhint("Model data may be corrupted. Verify the model was stored correctly.")));
+					 errmsg("neurondb: evaluate_linear_regression_by_model_id_jsonb: no valid model found"),
+					 errdetail("Neither CPU model nor GPU payload is available"),
+					 errhint("Verify the model exists in the catalog.")));
 		}
 
-		gpu_hdr = (const NdbCudaLinRegModelHeader *) VARDATA(gpu_payload);
-		feat_dim = gpu_hdr->feature_dim;
+		if (feat_dim <= 0)
+		{
+			NDB_SPI_SESSION_END(eval_jsonb_spi_session);
+			if (model != NULL)
+			{
+				nfree(model->coefficients);
+				nfree(model);
+			}
+			nfree(gpu_payload);
+			nfree(gpu_metrics);
+			nfree(tbl_str);
+			nfree(feat_str);
+			nfree(targ_str);
+			ereport(ERROR,
+					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+					 errmsg("neurondb: evaluate_linear_regression_by_model_id_jsonb: invalid feature dimension %d",
+							feat_dim)));
+		}
+
+		/* First pass: compute mean of y (common for both CPU and GPU) */
+		for (i = 0; i < nvec; i++)
+		{
+			HeapTuple	tuple = SPI_tuptable->vals[i];
+			TupleDesc	tupdesc = SPI_tuptable->tupdesc;
+			Datum		targ_datum;
+			bool		targ_null;
 
-		/* Allocate host buffers for features and targets */
-		nalloc(h_features, float, (size_t) nvec * (size_t) feat_dim);
-		nalloc(h_targets, double, nvec);
+			targ_datum = SPI_getbinval(tuple, tupdesc, 2, &targ_null);
+			if (!targ_null)
+				y_mean += DatumGetFloat8(targ_datum);
+		}
+		y_mean /= nvec;
 
-		/*
-		 * Extract features and targets from SPI results - optimized batch
-		 * extraction
-		 */
+		/* Second pass: unified evaluation loop - only difference is predict function */
+		for (i = 0; i < nvec; i++)
 		{
+			HeapTuple	tuple = SPI_tuptable->vals[i];
 			TupleDesc	tupdesc = SPI_tuptable->tupdesc;
+			Datum		feat_datum;
+			Datum		targ_datum;
+			bool		feat_null;
+			bool		targ_null;
+			double		y_true;
+			double		y_pred = 0.0;
+			double		error;
+			int			j;
+			int			actual_dim;
+
+			feat_datum = SPI_getbinval(tuple, tupdesc, 1, &feat_null);
+			targ_datum = SPI_getbinval(tuple, tupdesc, 2, &targ_null);
+
+			if (feat_null || targ_null)
+				continue;
+
+			y_true = DatumGetFloat8(targ_datum);
 
-			for (i = 0; i < nvec; i++)
+			/* Extract features and determine dimension */
+			if (feat_is_array)
 			{
-				HeapTuple	tuple = SPI_tuptable->vals[i];
-				Datum		feat_datum;
-				Datum		targ_datum;
-				bool		feat_null;
-				bool		targ_null;
-				Vector *vec = NULL;
-				ArrayType *arr = NULL;
-				float *feat_row = NULL;
-
-				feat_datum = SPI_getbinval(tuple, tupdesc, 1, &feat_null);
-				targ_datum = SPI_getbinval(tuple, tupdesc, 2, &targ_null);
-
-				if (feat_null || targ_null)
+				ArrayType *arr = DatumGetArrayTypeP(feat_datum);
+				if (ARR_NDIM(arr) != 1)
 					continue;
+				actual_dim = ARR_DIMS(arr)[0];
+			}
+			else
+			{
+				Vector *vec = DatumGetVector(feat_datum);
+				actual_dim = vec->dim;
+			}
 
-				feat_row = h_features + (valid_rows * feat_dim);
-				h_targets[valid_rows] = DatumGetFloat8(targ_datum);
-				y_mean += h_targets[valid_rows];
+			/* Validate feature dimension */
+			if (actual_dim != feat_dim)
+				continue;
 
-				/* Extract feature vector - optimized paths */
+			/* Call appropriate predict function based on compute mode */
+			if (use_gpu_predict)
+			{
+				/* GPU predict path */
+#ifdef NDB_GPU_CUDA
+				float	   *feat_row = NULL;
+				int			predict_rc;
+
+				/* Extract features to float array for GPU predict */
+				nalloc(feat_row, float, feat_dim);
 				if (feat_is_array)
 				{
-					arr = DatumGetArrayTypeP(feat_datum);
-					if (ARR_NDIM(arr) != 1 || ARR_DIMS(arr)[0] != feat_dim)
-						continue;
+					ArrayType *arr = DatumGetArrayTypeP(feat_datum);
 					if (feat_type_oid == FLOAT8ARRAYOID)
 					{
 						float8	   *data = (float8 *) ARR_DATA_PTR(arr);
-						int			j;
-						int			j_remain = feat_dim % 4;
-						int			j_end = feat_dim - j_remain;
-
-						for (j = 0; j < j_end; j += 4)
-						{
-							feat_row[j] = (float) data[j];
-							feat_row[j + 1] = (float) data[j + 1];
-							feat_row[j + 2] = (float) data[j + 2];
-							feat_row[j + 3] = (float) data[j + 3];
-						}
-						for (j = j_end; j < feat_dim; j++)
+						for (j = 0; j < feat_dim; j++)
 							feat_row[j] = (float) data[j];
 					}
 					else
 					{
 						float4	   *data = (float4 *) ARR_DATA_PTR(arr);
-
 						memcpy(feat_row, data, sizeof(float) * feat_dim);
 					}
 				}
 				else
 				{
-					vec = DatumGetVector(feat_datum);
-					if (vec->dim != feat_dim)
-						continue;
+					Vector *vec = DatumGetVector(feat_datum);
 					memcpy(feat_row, vec->data, sizeof(float) * feat_dim);
 				}
 
-				valid_rows++;
+				/* Use GPU predict (which works) */
+				predict_rc = ndb_cuda_linreg_predict(gpu_payload,
+													  feat_row,
+													  feat_dim,
+													  &y_pred,
+													  NULL);
+				if (predict_rc != 0)
+				{
+					/* GPU predict failed - fall back to CPU prediction using GPU coefficients */
+					y_pred = gpu_hdr->intercept;
+					for (j = 0; j < feat_dim; j++)
+						y_pred += (double) gpu_coefficients[j] * (double) feat_row[j];
+				}
+				nfree(feat_row);
+#endif
 			}
-		}
-
-		if (valid_rows < 2)
-		{
-			nfree(h_features);
-			nfree(h_targets);
-			nfree(gpu_payload);
-			nfree(gpu_metrics);
-			nfree(tbl_str);
-			nfree(feat_str);
-			nfree(targ_str);
-			NDB_SPI_SESSION_END(eval_jsonb_spi_session);
-			ereport(ERROR,
-					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-					 errmsg("neurondb: evaluate_linear_regression_by_model_id_jsonb: need at least 2 valid samples, got %d",
-							valid_rows),
-					 errdetail("Processed %d valid rows, minimum required is 2", valid_rows),
-					 errhint("Ensure the evaluation table contains at least 2 valid feature-target pairs.")));
-		}
-
-		y_mean /= valid_rows;
-
-		/* Use optimized GPU evaluation kernel */
-		{
-			double		gpu_mse = 0.0;
-			double		gpu_mae = 0.0;
-			double		gpu_rmse = 0.0;
-			double		gpu_r_squared = 0.0;
-
-			char *gpu_err = NULL;
-			int			eval_rc;
-
-			eval_rc = ndb_cuda_linreg_evaluate(gpu_payload,
-											   h_features,
-											   h_targets,
-											   valid_rows,
-											   feat_dim,
-											   &gpu_mse,
-											   &gpu_mae,
-											   &gpu_rmse,
-											   &gpu_r_squared,
-											   &gpu_err);
-
-			if (eval_rc == 0)
+			else
 			{
-				/* GPU evaluation succeeded */
-				mse = gpu_mse;
-				mae = gpu_mae;
-				rmse = gpu_rmse;
-				r_squared = gpu_r_squared;
-				nvec = valid_rows;
-
-				nfree(h_features);
-				nfree(h_targets);
-				nfree(gpu_payload);
-				nfree(gpu_metrics);
-				nfree(tbl_str);
-				nfree(feat_str);
-				nfree(targ_str);
+				/* CPU predict path - compute prediction using model coefficients */
+				y_pred = model->intercept;
 
-				NDB_SPI_SESSION_END(eval_jsonb_spi_session);
-
-				/* Build result JSON */
-				MemoryContextSwitchTo(oldcontext);
+				if (feat_is_array)
 				{
-					initStringInfo(&jsonbuf);
-					appendStringInfo(&jsonbuf,
-									 "{\"mse\":%.6f,\"mae\":%.6f,\"rmse\":%.6f,\"r_squared\":%.6f,\"n_samples\":%d}",
-									 mse, mae, rmse, r_squared, nvec);
-
-					/* Use ndb_jsonb_in_cstring to parse JSON string to JSONB */
-					result = ndb_jsonb_in_cstring(jsonbuf.data);
-					nfree(jsonbuf.data);
-					jsonbuf.data = NULL;
-
-					if (result == NULL)
+					ArrayType *arr = DatumGetArrayTypeP(feat_datum);
+					if (feat_type_oid == FLOAT8ARRAYOID)
 					{
-						ereport(ERROR,
-								(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),
-								 errmsg("neurondb: failed to parse GPU evaluation metrics JSON")));
+						double	   *feat_data = (double *) ARR_DATA_PTR(arr);
+						for (j = 0; j < model->n_features; j++)
+							y_pred += model->coefficients[j] * feat_data[j];
+					}
+					else
+					{
+						float	   *feat_data = (float *) ARR_DATA_PTR(arr);
+						for (j = 0; j < model->n_features; j++)
+							y_pred += model->coefficients[j] * (double) feat_data[j];
 					}
-
-					return result;
+				}
+				else
+				{
+					Vector *vec = DatumGetVector(feat_datum);
+					for (j = 0; j < model->n_features && j < vec->dim; j++)
+						y_pred += model->coefficients[j] * vec->data[j];
 				}
 			}
-			else
-			{
-				/* GPU evaluation failed, fall back to CPU */
-				elog(WARNING,
-					 "neurondb: evaluate_linear_regression_by_model_id_jsonb: GPU evaluation kernel failed: %s, falling back to CPU",
-					 gpu_err ? gpu_err : "unknown error");
-				nfree(gpu_err);
-				nfree(h_features);
-				nfree(h_targets);
 
-				/* Fall through to CPU path */
-			}
+			/* Compute errors (same for both CPU and GPU) */
+			error = y_true - y_pred;
+			mse += error * error;
+			mae += fabs(error);
+			ss_res += error * error;
+			ss_tot += (y_true - y_mean) * (y_true - y_mean);
 		}
-#endif							/* NDB_GPU_CUDA */
-	}
-
-	/* CPU evaluation path */
-	/* Handle GPU model fallback - convert GPU model to CPU format */
-	if (is_gpu_model && gpu_payload != NULL && model == NULL)
-	{
-		const NdbCudaLinRegModelHeader *gpu_hdr;
-		const float *gpu_coefficients;
-
-		double *cpu_coefficients = NULL;
-		double		cpu_intercept = 0.0;
-		int			feat_dim;
 
-		if (VARSIZE(gpu_payload) - VARHDRSZ < sizeof(NdbCudaLinRegModelHeader))
+		/* Normalize metrics (same for both CPU and GPU) */
+		if (nvec > 0)
 		{
-			NDB_SPI_SESSION_END(eval_jsonb_spi_session);
-			nfree(gpu_payload);
-			nfree(gpu_metrics);
-			nfree(tbl_str);
-			nfree(feat_str);
-			nfree(targ_str);
-			ereport(ERROR,
-					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-					 errmsg("neurondb: evaluate_linear_regression_by_model_id_jsonb: GPU payload too small for CPU fallback"),
-					 errdetail("Payload size is %u bytes, minimum required is %zu bytes", VARSIZE(gpu_payload) - VARHDRSZ, sizeof(NdbCudaLinRegModelHeader)),
-					 errhint("Model data may be corrupted. Verify the model was stored correctly.")));
+			mse /= nvec;
+			mae /= nvec;
 		}
+		rmse = sqrt(mse);
 
-		gpu_hdr = (const NdbCudaLinRegModelHeader *) VARDATA(gpu_payload);
-		gpu_coefficients = (const float *) ((const char *) gpu_hdr + sizeof(NdbCudaLinRegModelHeader));
-		feat_dim = gpu_hdr->feature_dim;
-		cpu_intercept = gpu_hdr->intercept;
-
-		/* Convert coefficients from float to double */
-		nalloc(cpu_coefficients, double, feat_dim);
-
-		for (i = 0; i < feat_dim; i++)
-			cpu_coefficients[i] = (double) gpu_coefficients[i];
-
-		/* Create temporary CPU model structure */
-		nalloc(model, LinRegModel, 1);
-
-		model->n_features = feat_dim;
-		model->intercept = cpu_intercept;
-		model->coefficients = cpu_coefficients;
-	}
+		/* Compute R-squared (same for both CPU and GPU) */
+		if (ss_tot > 1e-10)
+			r_squared = 1.0 - (ss_res / ss_tot);
+		else
+			r_squared = 0.0;
 
-	/* Ensure model is available for CPU evaluation */
-	if (model == NULL)
-	{
-		NDB_SPI_SESSION_END(eval_jsonb_spi_session);
+		/* Cleanup */
+		if (model != NULL)
+		{
+			nfree(model->coefficients);
+			nfree(model);
+		}
 		nfree(gpu_payload);
 		nfree(gpu_metrics);
 		nfree(tbl_str);
 		nfree(feat_str);
 		nfree(targ_str);
-		ereport(ERROR,
-				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-				 errmsg("neurondb: evaluate_linear_regression_by_model_id_jsonb: CPU model evaluation requires model data"),
-				 errdetail("Model structure is NULL after loading from catalog"),
-				 errhint("Verify the model exists and can be deserialized correctly.")));
-	}
-
-	/* Store coefficients pointer while model is known to be valid */
-	if (model != NULL)
-		model_coefficients_ptr = model->coefficients;
-
-	/* First pass: compute mean of y */
-	for (i = 0; i < nvec; i++)
-	{
-		HeapTuple	tuple = SPI_tuptable->vals[i];
-		TupleDesc	tupdesc = SPI_tuptable->tupdesc;
-		Datum		targ_datum;
-		bool		targ_null;
-
-		targ_datum = SPI_getbinval(tuple, tupdesc, 2, &targ_null);
-		if (!targ_null)
-			y_mean += DatumGetFloat8(targ_datum);
-	}
-	y_mean /= nvec;
 
-	/* Second pass: compute predictions and metrics */
-	for (i = 0; i < nvec; i++)
-	{
-		HeapTuple	tuple = SPI_tuptable->vals[i];
-		TupleDesc	tupdesc = SPI_tuptable->tupdesc;
-		Datum		feat_datum;
-		Datum		targ_datum;
-		bool		feat_null;
-		bool		targ_null;
-		Vector *vec = NULL;
-		ArrayType *arr = NULL;
-		double		y_true;
-		double		y_pred;
-		double		error;
-		int			j;
-		int			actual_dim;
-
-		feat_datum = SPI_getbinval(tuple, tupdesc, 1, &feat_null);
-		targ_datum = SPI_getbinval(tuple, tupdesc, 2, &targ_null);
+		NDB_SPI_SESSION_END(eval_jsonb_spi_session);
 
-		if (feat_null || targ_null)
-			continue;
+		/* Build result JSON (same for both CPU and GPU) */
+		MemoryContextSwitchTo(oldcontext);
+		{
+			Datum		d;
 
-		y_true = DatumGetFloat8(targ_datum);
+			initStringInfo(&jsonbuf);
+			appendStringInfo(&jsonbuf,
+							 "{\"mse\":%.6f,\"mae\":%.6f,\"rmse\":%.6f,\"r_squared\":%.6f,\"n_samples\":%d}",
+							 mse, mae, rmse, r_squared, nvec);
 
-		/* Extract features and determine dimension */
-		if (feat_is_array)
-		{
-			arr = DatumGetArrayTypeP(feat_datum);
-			if (ARR_NDIM(arr) != 1)
+			/* Use inline DirectFunctionCall1 to bypass ndb_jsonb_in_cstring helper */
+			PG_TRY();
 			{
-				NDB_SPI_SESSION_END(eval_jsonb_spi_session);
-				if (model != NULL)
-				{
-					nfree(model->coefficients);
-					nfree(model);
-				}
-				nfree(gpu_payload);
-				nfree(gpu_metrics);
-				nfree(tbl_str);
-				nfree(feat_str);
-				nfree(targ_str);
-				ereport(ERROR,
-						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-						 errmsg("neurondb: features array must be 1-D"),
-						 errdetail("Array has %d dimensions, expected 1", ARR_NDIM(arr)),
-						 errhint("Ensure feature column contains 1-dimensional arrays only.")));
+				d = DirectFunctionCall1(jsonb_in, CStringGetDatum(jsonbuf.data));
+				result = DatumGetJsonbP(d);
 			}
-			actual_dim = ARR_DIMS(arr)[0];
-		}
-		else
-		{
-			vec = DatumGetVector(feat_datum);
-			actual_dim = vec->dim;
-		}
-
-		/* Validate feature dimension */
-		if (actual_dim != model->n_features)
-		{
-			NDB_SPI_SESSION_END(eval_jsonb_spi_session);
-			if (model != NULL)
+			PG_CATCH();
 			{
-				nfree(model->coefficients);
-				nfree(model);
+				FlushErrorState();
+				result = NULL;
 			}
-			nfree(gpu_payload);
-			nfree(gpu_metrics);
-			nfree(tbl_str);
-			nfree(feat_str);
-			nfree(targ_str);
-			ereport(ERROR,
-					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-					 errmsg("neurondb: feature dimension mismatch (expected %d, got %d)",
-							model->n_features,
-							actual_dim),
-					 errdetail("Model was trained with %d features, but input has %d features", model->n_features, actual_dim),
-					 errhint("Ensure the feature column has the same dimension as the training data.")));
-		}
+			PG_END_TRY();
+			nfree(jsonbuf.data);
+			jsonbuf.data = NULL;
 
-		/* Compute prediction using model */
-		y_pred = model->intercept;
-
-		if (feat_is_array)
-		{
-			if (feat_type_oid == FLOAT8ARRAYOID)
+			if (result == NULL)
 			{
-				double	   *feat_data = (double *) ARR_DATA_PTR(arr);
-
-				for (j = 0; j < model->n_features; j++)
-					y_pred += model->coefficients[j] * feat_data[j];
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),
+						 errmsg("neurondb: failed to parse evaluation metrics JSON")));
 			}
-			else
-			{
-				float	   *feat_data = (float *) ARR_DATA_PTR(arr);
 
-				for (j = 0; j < model->n_features; j++)
-					y_pred += model->coefficients[j] * (double) feat_data[j];
-			}
+			return result;
 		}
-		else
-		{
-			for (j = 0; j < model->n_features && j < vec->dim; j++)
-				y_pred += model->coefficients[j] * vec->data[j];
-		}
-
-		/* Compute errors */
-		error = y_true - y_pred;
-		mse += error * error;
-		mae += fabs(error);
-		ss_res += error * error;
-		ss_tot += (y_true - y_mean) * (y_true - y_mean);
 	}
 
-	mse /= nvec;
-	mae /= nvec;
-	rmse = sqrt(mse);
-
-	/*
-	 * Handle RÂ² calculation - if ss_tot is zero (no variance in y), RÂ² is
-	 * undefined
-	 */
-	if (ss_tot == 0.0)
-		r_squared = 0.0;		/* Convention: set to 0 when there's no
-								 * variance to explain */
-	else
-		r_squared = 1.0 - (ss_res / ss_tot);
-
-	/* End SPI session BEFORE creating JSONB to avoid memory context issues */
+	/* Should not reach here - unified evaluation handles both CPU and GPU */
 	NDB_SPI_SESSION_END(eval_jsonb_spi_session);
-
-	/* Switch to oldcontext to create result */
-	MemoryContextSwitchTo(oldcontext);
-
-	/* Build result JSON string */
-	initStringInfo(&jsonbuf);
-	appendStringInfo(&jsonbuf,
-					 "{\"mse\":%.6f,\"mae\":%.6f,\"rmse\":%.6f,\"r_squared\":%.6f,\"n_samples\":%d}",
-					 mse, mae, rmse, r_squared, nvec);
-
-	/* Use ndb_jsonb_in_cstring */
-	result = ndb_jsonb_in_cstring(jsonbuf.data);
-	nfree(jsonbuf.data);
-	jsonbuf.data = NULL;
-
-	if (result == NULL)
-	{
-		ereport(ERROR,
-				(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),
-				 errmsg("neurondb: failed to parse metrics JSON")));
-	}
-
-	if (result == NULL)
+	if (model != NULL)
 	{
-		nfree(jsonbuf.data);
-		jsonbuf.data = NULL;
-		nfree(model_coefficients_ptr);
+		nfree(model->coefficients);
 		nfree(model);
-		nfree(gpu_payload);
-		nfree(gpu_metrics);
-		nfree(tbl_str);
-		nfree(feat_str);
-		nfree(targ_str);
-		ereport(ERROR,
-				(errcode(ERRCODE_INTERNAL_ERROR),
-				 errmsg("neurondb: evaluate_linear_regression_by_model_id_jsonb: JSONB result is NULL")));
 	}
-
-	nfree(jsonbuf.data);
-	jsonbuf.data = NULL;
-
-	/*
-	 * Bulletproof cleanup: ensure correct context and pointer integrity
-	 * before freeing
-	 */
-	currctx = CurrentMemoryContext;
-	if (oldcontext && currctx != oldcontext)
-		MemoryContextSwitchTo(oldcontext);
-
-	/* Cleanup - nfree handles NULL checks */
-	nfree(model_coefficients_ptr);
-	nfree(model);
 	nfree(gpu_payload);
 	nfree(gpu_metrics);
 	nfree(tbl_str);
 	nfree(feat_str);
 	nfree(targ_str);
+	ereport(ERROR,
+			(errcode(ERRCODE_INTERNAL_ERROR),
+			 errmsg("neurondb: evaluate_linear_regression_by_model_id_jsonb: unexpected code path"),
+			 errdetail("Unified evaluation should have handled all cases")));
+	return NULL;
 
 	return result;
 }
diff --git a/NeuronDB/src/ml/ml_logistic_regression.c b/NeuronDB/src/ml/ml_logistic_regression.c
index e525354..a995863 100644
--- a/NeuronDB/src/ml/ml_logistic_regression.c
+++ b/NeuronDB/src/ml/ml_logistic_regression.c
@@ -45,6 +45,7 @@
 #include "neurondb_json.h"
 #include "utils/jsonb.h"
 #include "utils/elog.h"
+#include "neurondb_guc.h"
 #ifdef NDB_GPU_CUDA
 #include "neurondb_cuda_lr.h"
 #include "neurondb_cuda_runtime.h"
@@ -485,7 +486,9 @@ train_logistic_regression(PG_FUNCTION_ARGS)
 													 * Create empty JSONB as
 													 * fallback
 													 */
-													gpu_hyperparams = (Jsonb *) palloc(VARHDRSZ + sizeof(uint32));
+													char *tmp = NULL;
+													nalloc(tmp, char, VARHDRSZ + sizeof(uint32));
+													gpu_hyperparams = (Jsonb *) tmp;
 													SET_VARSIZE(gpu_hyperparams, VARHDRSZ + sizeof(uint32));
 													*((uint32 *) VARDATA(gpu_hyperparams)) = JB_CMASK;	/* Empty object header */
 													if (hyperbuf.data != NULL)
@@ -539,10 +542,10 @@ train_logistic_regression(PG_FUNCTION_ARGS)
 															nfree(hyperbuf.data);
 															lr_dataset_free(&dataset);
 														}
-														else if (!neurondb_gpu_is_available() || 1) /* Force CPU training */
+														else if (!neurondb_gpu_is_available())
 														{
 															elog(DEBUG1,
-																 "neurondb: logistic_regression: GPU training disabled, using CPU");
+																 "neurondb: logistic_regression: GPU not available, using CPU");
 															nfree(gpu_hyperparams);
 															nfree(hyperbuf.data);
 															lr_dataset_free(&dataset);
@@ -1753,25 +1756,37 @@ evaluate_logistic_regression_by_model_id(PG_FUNCTION_ARGS)
 	feat_str = text_to_cstring(feature_col);
 	targ_str = text_to_cstring(label_col);
 
-	/* Load model from catalog - check GPU model directly */
+	/* Load model from catalog - try CPU first, then GPU */
+	LRModel *model = NULL;
+	if (!lr_load_model_from_catalog(model_id, &model))
 	{
-		if (ml_catalog_fetch_model_payload(model_id, &gpu_payload, NULL, &gpu_metrics))
+		/*
+		 * CPU model load failed - this might indicate a GPU-trained model.
+		 * Try loading GPU payload instead.
+		 */
+		if (!ml_catalog_fetch_model_payload(model_id, &gpu_payload, NULL, &gpu_metrics))
 		{
-			is_gpu_model = lr_metadata_is_gpu(gpu_metrics);
-			if (!is_gpu_model)
-			{
-				/*
-				 * CPU model - would need to load here if we had CPU model
-				 * loading
-				 */
-				nfree(gpu_payload);
-				nfree(gpu_metrics);
-				gpu_payload = NULL;
-				gpu_metrics = NULL;
-			}
+			/* Neither CPU nor GPU model found */
+			nfree(tbl_str);
+			nfree(feat_str);
+			nfree(targ_str);
+			ereport(ERROR,
+					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+					 errmsg(NDB_ERR_MSG("evaluate_logistic_regression_by_model_id: model %d not found"),
+							model_id)));
+		}
+		
+		if (gpu_payload != NULL)
+		{
+			/* Mark this as a GPU model for later processing */
+			is_gpu_model = true;
 		}
 		else
 		{
+			/* No model data found */
+			nfree(tbl_str);
+			nfree(feat_str);
+			nfree(targ_str);
 			ereport(ERROR,
 					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
 					 errmsg(NDB_ERR_MSG("evaluate_logistic_regression_by_model_id: model %d not found"),
@@ -1823,461 +1838,101 @@ evaluate_logistic_regression_by_model_id(PG_FUNCTION_ARGS)
 	if (feat_type_oid == FLOAT8ARRAYOID || feat_type_oid == FLOAT4ARRAYOID)
 		feat_is_array = true;
 
-	/*
-	 * GPU batch evaluation path for GPU models - uses optimized evaluation
-	 * kernel
-	 */
-	if (is_gpu_model && neurondb_gpu_is_available())
+	/* Unified evaluation: Determine predict function based on compute mode */
+	/* All metrics calculation is the same - only difference is predict function */
 	{
-#ifdef NDB_GPU_CUDA
-		const NdbCudaLrModelHeader *gpu_hdr;
-
-		double *h_labels = NULL;
-		float *h_features = NULL;
+		bool		use_gpu_predict = false;
 		int			feat_dim = 0;
-		int			valid_rows = 0;
-		size_t		payload_size;
-
-		/* Defensive check: validate payload size */
-		payload_size = VARSIZE(gpu_payload) - VARHDRSZ;
-		if (payload_size < sizeof(NdbCudaLrModelHeader))
-		{
-			elog(DEBUG1,
-				 "evaluate_logistic_regression_by_model_id: GPU payload too small (%zu bytes), falling back to CPU",
-				 payload_size);
-			goto cpu_evaluation_path;
-		}
-
-		/* Load GPU model header with defensive checks */
-		gpu_hdr = (const NdbCudaLrModelHeader *) VARDATA(gpu_payload);
-		if (gpu_hdr == NULL)
-		{
-			elog(DEBUG1,
-				 "evaluate_logistic_regression_by_model_id: NULL GPU header, falling back to CPU");
-			goto cpu_evaluation_path;
-		}
-
-		feat_dim = gpu_hdr->feature_dim;
-		if (feat_dim <= 0 || feat_dim > 100000)
-		{
-			elog(DEBUG1,
-				 "evaluate_logistic_regression_by_model_id: invalid feature_dim (%d), falling back to CPU",
-				 feat_dim);
-			goto cpu_evaluation_path;
-		}
+		int			tp = 0,
+					tn = 0,
+					fp = 0,
+					fn = 0;
+		int			total_predictions = 0;
 
-		/* Allocate host buffers for features and labels with size checks */
+		/* Determine if we should use GPU predict or CPU predict */
+		if (is_gpu_model && neurondb_gpu_is_available() && !NDB_COMPUTE_MODE_IS_CPU())
 		{
-			size_t		features_size = sizeof(float) * (size_t) nvec * (size_t) feat_dim;
-			size_t		labels_size = sizeof(double) * (size_t) nvec;
-
-			if (features_size > MaxAllocSize || labels_size > MaxAllocSize)
+			/* GPU model and GPU mode: use GPU predict */
+			if (gpu_payload != NULL && VARSIZE(gpu_payload) - VARHDRSZ >= sizeof(NdbCudaLrModelHeader))
 			{
-				elog(DEBUG1,
-					 "evaluate_logistic_regression_by_model_id: allocation size too large (features=%zu, labels=%zu), falling back to CPU",
-					 features_size, labels_size);
-				goto cpu_evaluation_path;
+				const NdbCudaLrModelHeader *gpu_hdr = (const NdbCudaLrModelHeader *) VARDATA(gpu_payload);
+				feat_dim = gpu_hdr->feature_dim;
+				use_gpu_predict = true;
 			}
-
-			nalloc(h_features, float, (size_t) nvec * (size_t) feat_dim);
-			nalloc(h_labels, double, (size_t) nvec);
 		}
-
-		/*
-		 * Extract features and labels from SPI results - optimized batch
-		 * extraction
-		 */
-		/* Cache TupleDesc to avoid repeated lookups */
+		else if (model != NULL)
 		{
-			TupleDesc	tupdesc = SPI_tuptable->tupdesc;
-
-			if (tupdesc == NULL)
-			{
-				elog(DEBUG1,
-					 "evaluate_logistic_regression_by_model_id: NULL TupleDesc, falling back to CPU");
-				nfree(h_features);
-				h_features = NULL;
-				nfree(h_labels);
-				h_labels = NULL;
-				goto cpu_evaluation_path;
-			}
-
-			for (i = 0; i < nvec; i++)
-			{
-				HeapTuple	tuple;
-				Datum		feat_datum;
-				Datum		targ_datum;
-				bool		feat_null;
-				bool		targ_null;
-				Vector	   *vec = NULL;
-				ArrayType  *arr = NULL;
-				float	   *feat_row = NULL;
-
-				if (SPI_tuptable == NULL || SPI_tuptable->vals == NULL || i >= SPI_processed)
-					break;
-
-				tuple = SPI_tuptable->vals[i];
-				if (tuple == NULL)
-					continue;
-
-				feat_datum = SPI_getbinval(tuple, tupdesc, 1, &feat_null);
-				targ_datum = SPI_getbinval(tuple, tupdesc, 2, &targ_null);
-
-				if (feat_null || targ_null)
-					continue;
-
-				/* Bounds check */
-				if (valid_rows >= nvec)
-				{
-					elog(DEBUG1,
-						 "evaluate_logistic_regression_by_model_id: valid_rows overflow, breaking");
-					break;
-				}
-
-				feat_row = h_features + (valid_rows * feat_dim);
-				if (feat_row == NULL || feat_row < h_features || feat_row >= h_features + (nvec * feat_dim))
-				{
-					elog(DEBUG1,
-						 "evaluate_logistic_regression_by_model_id: feat_row out of bounds, skipping row");
-					continue;
-				}
-
-				h_labels[valid_rows] = DatumGetFloat8(targ_datum);
-
-				/* Extract feature vector - optimized paths */
-				if (feat_is_array)
-				{
-					arr = DatumGetArrayTypeP(feat_datum);
-					if (ARR_NDIM(arr) != 1 || ARR_DIMS(arr)[0] != feat_dim)
-						continue;
-					if (feat_type_oid == FLOAT8ARRAYOID)
-					{
-						/* Optimized: bulk conversion with loop unrolling hint */
-						float8	   *data = (float8 *) ARR_DATA_PTR(arr);
-						int			j;
-						int			j_remain = feat_dim % 4;
-						int			j_end = feat_dim - j_remain;
-
-						/*
-						 * Process 4 elements at a time for better cache
-						 * locality
-						 */
-						for (j = 0; j < j_end; j += 4)
-						{
-							feat_row[j] = (float) data[j];
-							feat_row[j + 1] = (float) data[j + 1];
-							feat_row[j + 2] = (float) data[j + 2];
-							feat_row[j + 3] = (float) data[j + 3];
-						}
-						/* Handle remaining elements */
-						for (j = j_end; j < feat_dim; j++)
-							feat_row[j] = (float) data[j];
-					}
-					else
-					{
-						/* FLOAT4ARRAYOID: direct memcpy (already optimal) */
-						float4	   *data = (float4 *) ARR_DATA_PTR(arr);
-
-						memcpy(feat_row, data, sizeof(float) * feat_dim);
-					}
-				}
-				else
-				{
-					/* Vector type: direct memcpy (already optimal) */
-					vec = DatumGetVector(feat_datum);
-					if (vec->dim != feat_dim)
-						continue;
-					memcpy(feat_row, vec->data, sizeof(float) * feat_dim);
-				}
-
-				valid_rows++;
-			}
+			/* CPU model or CPU mode: use CPU predict */
+			feat_dim = model->n_features;
+			use_gpu_predict = false;
 		}
-
-		if (valid_rows == 0)
+		else if (is_gpu_model && gpu_payload != NULL)
 		{
-			nfree(h_features);
-			h_features = NULL;
-			nfree(h_labels);
-			h_labels = NULL;
-			if (gpu_payload)
+			/* GPU model but CPU mode: convert to CPU format for CPU predict */
+			if (VARSIZE(gpu_payload) - VARHDRSZ >= sizeof(NdbCudaLrModelHeader))
 			{
-				nfree(gpu_payload);
-				gpu_payload = NULL;
-			}
-			if (gpu_metrics)
-			{
-				nfree(gpu_metrics);
-				gpu_metrics = NULL;
-			}
-			nfree(tbl_str);
-			tbl_str = NULL;
-			nfree(feat_str);
-			feat_str = NULL;
-			nfree(targ_str);
-			targ_str = NULL;
-			ndb_spi_stringinfo_free(spi_session, &query);
-			NDB_SPI_SESSION_END(spi_session);
-			ereport(ERROR,
-					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-					 errmsg(NDB_ERR_MSG("evaluate_logistic_regression_by_model_id: no valid rows found"))));
-		}
+				const NdbCudaLrModelHeader *gpu_hdr = (const NdbCudaLrModelHeader *) VARDATA(gpu_payload);
+				const float *gpu_weights = (const float *) ((const char *) gpu_hdr + sizeof(NdbCudaLrModelHeader));
+				feat_dim = gpu_hdr->feature_dim;
 
-		/* Use optimized GPU batch evaluation */
-		{
-			int			rc;
-
-			char *gpu_errstr = NULL;
-
-			/* Defensive checks before GPU call */
-			if (h_features == NULL || h_labels == NULL || valid_rows <= 0 || feat_dim <= 0)
-			{
-				elog(DEBUG1,
-					 "evaluate_logistic_regression_by_model_id: invalid inputs for GPU evaluation (features=%p, labels=%p, rows=%d, dim=%d), falling back to CPU",
-					 (void *) h_features, (void *) h_labels, valid_rows, feat_dim);
-				nfree(h_features);
-				h_features = NULL;
-				nfree(h_labels);
-				h_labels = NULL;
-				goto cpu_evaluation_path;
-			}
-
-			PG_TRY();
-			{
-				rc = ndb_cuda_lr_evaluate(gpu_payload,
-										  h_features,
-										  h_labels,
-										  valid_rows,
-										  feat_dim,
-										  threshold,
-										  &accuracy,
-										  &precision,
-										  &recall,
-										  &f1_score,
-										  &log_loss,
-										  &gpu_errstr);
-
-				if (rc == 0)
+				/* Convert GPU model to CPU format */
 				{
-					/* Success - build result and return */
-					nvec = valid_rows;
-
-					/* Free arrays and cleanup before ending SPI */
-					nfree(h_features);
-					h_features = NULL;
-					nfree(h_labels);
-					h_labels = NULL;
-					if (gpu_payload)
-					{
-						nfree(gpu_payload);
-						gpu_payload = NULL;
-					}
-					if (gpu_metrics)
-					{
-						nfree(gpu_metrics);
-						gpu_metrics = NULL;
-					}
-					if (gpu_errstr)
-					{
-						nfree(gpu_errstr);
-						gpu_errstr = NULL;
-					}
-					nfree(tbl_str);
-					tbl_str = NULL;
-					nfree(feat_str);
-					feat_str = NULL;
-					nfree(targ_str);
-					targ_str = NULL;
-					ndb_spi_stringinfo_free(spi_session, &query);
-
-					/* End SPI session BEFORE creating JSONB */
-					NDB_SPI_SESSION_END(spi_session);
-
-					/* Switch to oldcontext to create JSONB */
-					MemoryContextSwitchTo(oldcontext);
-
-					/* Build result JSON string */
-					initStringInfo(&jsonbuf);
-					appendStringInfo(&jsonbuf,
-									 "{\"accuracy\":%.6f,\"precision\":%.6f,\"recall\":%.6f,\"f1_score\":%.6f,\"log_loss\":%.6f,\"n_samples\":%d}",
-									 accuracy,
-									 precision,
-									 recall,
-									 f1_score,
-									 log_loss,
-									 nvec);
-
-					/* Create JSONB in oldcontext using ndb_jsonb_in_cstring */
-					result_jsonb = ndb_jsonb_in_cstring(jsonbuf.data);
-					nfree(jsonbuf.data);
-					jsonbuf.data = NULL;
-
-					if (result_jsonb == NULL)
-					{
-						ereport(ERROR,
-								(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),
-								 errmsg("neurondb: evaluate_logistic_regression_by_model_id: failed to parse metrics JSON")));
-					}
+					double *cpu_weights = NULL;
+					double		cpu_bias = 0.0;
+					int			coef_idx;
 
-					PG_RETURN_JSONB_P(result_jsonb);
-				}
-				else
-				{
-					/* GPU evaluation failed - fall back to CPU */
-					elog(DEBUG1,
-						 "evaluate_logistic_regression_by_model_id: GPU batch evaluation failed: %s, falling back to CPU",
-						 gpu_errstr ? gpu_errstr : "unknown error");
-					if (gpu_errstr)
-					{
-						nfree(gpu_errstr);
-						gpu_errstr = NULL;
-					}
-					nfree(h_features);
-					h_features = NULL;
-					nfree(h_labels);
-					h_labels = NULL;
-					goto cpu_evaluation_path;
-				}
-			}
-			PG_CATCH();
-			{
-				elog(DEBUG1,
-					 "evaluate_logistic_regression_by_model_id: exception during GPU evaluation, falling back to CPU");
-				if (h_features)
-				{
-					nfree(h_features);
-					h_features = NULL;
-				}
-				if (h_labels)
-				{
-					nfree(h_labels);
-					h_labels = NULL;
-				}
-				goto cpu_evaluation_path;
-			}
-			PG_END_TRY();
-		}
-#endif							/* NDB_GPU_CUDA */
-	}
-#ifndef NDB_GPU_CUDA
-	/* When CUDA is not available, always use CPU path */
-	/* Fall through to CPU path */
-#endif
+					cpu_bias = gpu_hdr->bias;
+					nalloc(cpu_weights, double, feat_dim);
 
-cpu_evaluation_path:
-	/* CPU evaluation path */
-	{
-		LRModel *model = NULL;
-		int			tp = 0,
-					tn = 0,
-					fp = 0,
-					fn = 0;
-		int			correct_predictions = 0;
-		int			total_predictions = 0;
+					for (coef_idx = 0; coef_idx < feat_dim; coef_idx++)
+						cpu_weights[coef_idx] = (double) gpu_weights[coef_idx];
 
-		/* Load model from catalog - try CPU first, then GPU if CPU fails */
-		if (!lr_load_model_from_catalog(model_id, &model))
-		{
-			/*
-			 * If CPU model loading failed, try to load GPU model for CPU
-			 * evaluation
-			 */
-			bytea *gpu_model_payload = NULL;
-			Jsonb *gpu_model_metrics = NULL;
-
-			if (ml_catalog_fetch_model_payload(model_id, &gpu_model_payload, NULL, &gpu_model_metrics))
-			{
-				/* Unified format: any model can be evaluated on CPU or GPU */
-				/*
-				 * training_backend flag is informational only, doesn't
-				 * restrict evaluation
-				 */
-				if (gpu_model_payload)
-				{
-					nfree(gpu_model_payload);
-					gpu_model_payload = NULL;
-				}
-				if (gpu_model_metrics)
-				{
-					nfree(gpu_model_metrics);
-					gpu_model_metrics = NULL;
+					/* Create temporary CPU model structure */
+					nalloc(model, LRModel, 1);
+					model->n_features = feat_dim;
+					model->bias = cpu_bias;
+					model->weights = cpu_weights;
 				}
+				use_gpu_predict = false;
 			}
-
-			/* If we get here, the model doesn't exist at all */
-			if (gpu_payload)
-			{
-				nfree(gpu_payload);
-				gpu_payload = NULL;
-			}
-			if (gpu_metrics)
-			{
-				nfree(gpu_metrics);
-				gpu_metrics = NULL;
-			}
-			nfree(tbl_str);
-			nfree(feat_str);
-			nfree(targ_str);
-			ndb_spi_stringinfo_free(spi_session, &query);
-			NDB_SPI_SESSION_END(spi_session);
-			ereport(ERROR,
-					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-					 errmsg(NDB_ERR_MSG("evaluate_logistic_regression_by_model_id: model %d not found"),
-							model_id)));
 		}
 
-		/* Re-execute query for CPU evaluation */
-		ret = ndb_spi_execute(spi_session, query.data, true, 0);
-		if (ret != SPI_OK_SELECT)
+		/* Ensure we have a valid model or GPU payload */
+		if (model == NULL && !use_gpu_predict)
 		{
-			nfree(model->weights);
-			nfree(model);
+			NDB_SPI_SESSION_END(spi_session);
 			nfree(gpu_payload);
 			nfree(gpu_metrics);
 			nfree(tbl_str);
 			nfree(feat_str);
 			nfree(targ_str);
-			ndb_spi_stringinfo_free(spi_session, &query);
-			NDB_SPI_SESSION_END(spi_session);
 			ereport(ERROR,
-					(errcode(ERRCODE_INTERNAL_ERROR),
-					 errmsg("neurondb: evaluate_logistic_regression_by_model_id: query failed for CPU evaluation")));
+					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+					 errmsg("neurondb: evaluate_logistic_regression_by_model_id: no valid model found"),
+					 errdetail("Neither CPU model nor GPU payload is available"),
+					 errhint("Verify the model exists in the catalog.")));
 		}
 
-		nvec = SPI_processed;
-		if (nvec < 2)
+		if (feat_dim <= 0)
 		{
-			if (model->weights)
+			NDB_SPI_SESSION_END(spi_session);
+			if (model != NULL)
 			{
 				nfree(model->weights);
-				model->weights = NULL;
-			}
-			nfree(model);
-			model = NULL;
-			if (gpu_payload)
-			{
-				nfree(gpu_payload);
-				gpu_payload = NULL;
-			}
-			if (gpu_metrics)
-			{
-				nfree(gpu_metrics);
-				gpu_metrics = NULL;
+				nfree(model);
 			}
+			nfree(gpu_payload);
+			nfree(gpu_metrics);
 			nfree(tbl_str);
-			tbl_str = NULL;
 			nfree(feat_str);
-			feat_str = NULL;
 			nfree(targ_str);
-			ndb_spi_stringinfo_free(spi_session, &query);
-			NDB_SPI_SESSION_END(spi_session);
 			ereport(ERROR,
 					(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-					 errmsg("neurondb: evaluate_logistic_regression_by_model_id: need at least 2 samples for CPU evaluation, got %d",
-							nvec)));
+					 errmsg("neurondb: evaluate_logistic_regression_by_model_id: invalid feature dimension %d",
+							feat_dim)));
 		}
 
-		/* Process each row for CPU evaluation */
+		/* Unified evaluation loop - only difference is predict function */
 		for (i = 0; i < nvec; i++)
 		{
 			HeapTuple	tuple = SPI_tuptable->vals[i];
@@ -2286,15 +1941,12 @@ cpu_evaluation_path:
 			Datum		targ_datum;
 			bool		feat_null;
 			bool		targ_null;
-			Vector	   *vec = NULL;
-			ArrayType  *arr = NULL;
-			int			actual_dim;
 			double		y_true;
-			double		y_pred_prob;
+			double		y_pred_prob = 0.0;
 			int			y_pred_class;
 			int			y_true_class;
-			double		z;
-			int			eval_j;
+			int			actual_dim;
+			int			j;
 
 			feat_datum = SPI_getbinval(tuple, tupdesc, 1, &feat_null);
 			targ_datum = SPI_getbinval(tuple, tupdesc, 2, &targ_null);
@@ -2307,77 +1959,148 @@ cpu_evaluation_path:
 			/* Extract features and determine dimension */
 			if (feat_is_array)
 			{
-				arr = DatumGetArrayTypeP(feat_datum);
+				ArrayType *arr = DatumGetArrayTypeP(feat_datum);
 				if (ARR_NDIM(arr) != 1)
-				{
-					nfree(model->weights);
-					nfree(model);
-					nfree(gpu_payload);
-					nfree(gpu_metrics);
-					nfree(tbl_str);
-					nfree(feat_str);
-					nfree(targ_str);
-					ndb_spi_stringinfo_free(spi_session, &query);
-					NDB_SPI_SESSION_END(spi_session);
-					ereport(ERROR,
-							(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-							 errmsg("neurondb: evaluate_logistic_regression_by_model_id: features array must be 1-D")));
-				}
+					continue;
 				actual_dim = ARR_DIMS(arr)[0];
 			}
 			else
 			{
-				vec = DatumGetVector(feat_datum);
+				Vector *vec = DatumGetVector(feat_datum);
 				actual_dim = vec->dim;
 			}
 
 			/* Validate feature dimension */
-			if (actual_dim != model->n_features)
-			{
-				nfree(model->weights);
-				nfree(model);
-				nfree(gpu_payload);
-				nfree(gpu_metrics);
-				nfree(tbl_str);
-				nfree(feat_str);
-				nfree(targ_str);
-				ndb_spi_stringinfo_free(spi_session, &query);
-				NDB_SPI_SESSION_END(spi_session);
-				ereport(ERROR,
-						(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-						 errmsg("neurondb: evaluate_logistic_regression_by_model_id: feature dimension mismatch (expected %d, got %d)",
-								model->n_features,
-								actual_dim)));
-			}
-
-			/* Compute prediction using model */
-			z = model->bias;
+			if (actual_dim != feat_dim)
+				continue;
 
-			if (feat_is_array)
+			/* Call appropriate predict function based on compute mode */
+			if (use_gpu_predict)
 			{
-				if (feat_type_oid == FLOAT8ARRAYOID)
-				{
-					double	   *feat_data = (double *) ARR_DATA_PTR(arr);
+				/* GPU predict path - use lr_try_gpu_predict_catalog */
+				Vector *feat_vec = NULL;
 
-					for (eval_j = 0; eval_j < model->n_features; eval_j++)
-						z += model->weights[eval_j] * feat_data[eval_j];
+				/* Create Vector structure for GPU predict */
+				if (feat_is_array)
+				{
+					ArrayType *arr = DatumGetArrayTypeP(feat_datum);
+					float *feat_data = NULL;
+					nalloc(feat_data, float, feat_dim);
+					if (feat_type_oid == FLOAT8ARRAYOID)
+					{
+						float8 *data = (float8 *) ARR_DATA_PTR(arr);
+						for (j = 0; j < feat_dim; j++)
+							feat_data[j] = (float) data[j];
+					}
+					else
+					{
+						float4 *data = (float4 *) ARR_DATA_PTR(arr);
+						memcpy(feat_data, data, sizeof(float) * feat_dim);
+					}
+					/* Create temporary Vector structure */
+					{
+						size_t		vec_size = VARHDRSZ + sizeof(int16) * 2 + sizeof(float) * feat_dim;
+						feat_vec = (Vector *) palloc0(vec_size);
+						SET_VARSIZE(feat_vec, vec_size);
+						feat_vec->dim = feat_dim;
+						memcpy(feat_vec->data, feat_data, sizeof(float) * feat_dim);
+					}
+					nfree(feat_data);
 				}
 				else
 				{
-					float	   *feat_data = (float *) ARR_DATA_PTR(arr);
+					feat_vec = DatumGetVector(feat_datum);
+				}
 
-					for (eval_j = 0; eval_j < model->n_features; eval_j++)
-						z += model->weights[eval_j] * (double) feat_data[eval_j];
+				/* Use GPU predict */
+				if (!lr_try_gpu_predict_catalog(model_id, feat_vec, &y_pred_prob))
+				{
+					/* GPU predict failed - in GPU mode, this is an error */
+					if (NDB_REQUIRE_GPU())
+					{
+						if (feat_is_array)
+							pfree(feat_vec);
+						NDB_SPI_SESSION_END(spi_session);
+						if (model != NULL)
+						{
+							nfree(model->weights);
+							nfree(model);
+						}
+						nfree(gpu_payload);
+						nfree(gpu_metrics);
+						nfree(tbl_str);
+						nfree(feat_str);
+						nfree(targ_str);
+						ereport(ERROR,
+								(errcode(ERRCODE_INTERNAL_ERROR),
+								 errmsg("neurondb: evaluate_logistic_regression_by_model_id: GPU prediction failed"),
+								 errdetail("GPU mode is required but GPU prediction failed"),
+								 errhint("Check GPU availability and model compatibility.")));
+					}
+					/* AUTO mode: fall back to CPU prediction using GPU coefficients */
+					{
+						const NdbCudaLrModelHeader *gpu_hdr = (const NdbCudaLrModelHeader *) VARDATA(gpu_payload);
+						const float *gpu_weights = (const float *) ((const char *) gpu_hdr + sizeof(NdbCudaLrModelHeader));
+						double z = gpu_hdr->bias;
+						if (feat_is_array)
+						{
+							ArrayType *arr = DatumGetArrayTypeP(feat_datum);
+							if (feat_type_oid == FLOAT8ARRAYOID)
+							{
+								float8 *data = (float8 *) ARR_DATA_PTR(arr);
+								for (j = 0; j < feat_dim; j++)
+									z += (double) gpu_weights[j] * (double) data[j];
+							}
+							else
+							{
+								float4 *data = (float4 *) ARR_DATA_PTR(arr);
+								for (j = 0; j < feat_dim; j++)
+									z += (double) gpu_weights[j] * (double) data[j];
+							}
+						}
+						else
+						{
+							Vector *vec = DatumGetVector(feat_datum);
+							for (j = 0; j < feat_dim && j < vec->dim; j++)
+								z += (double) gpu_weights[j] * (double) vec->data[j];
+						}
+						y_pred_prob = sigmoid(z);
+					}
 				}
+				if (feat_is_array)
+					pfree(feat_vec);
 			}
 			else
 			{
-				for (eval_j = 0; eval_j < model->n_features && eval_j < vec->dim; eval_j++)
-					z += model->weights[eval_j] * vec->data[eval_j];
-			}
+				/* CPU predict path - compute prediction using model coefficients */
+				double z = model->bias;
 
-			/* Apply sigmoid to get probability */
-			y_pred_prob = 1.0 / (1.0 + exp(-z));
+				if (feat_is_array)
+				{
+					ArrayType *arr = DatumGetArrayTypeP(feat_datum);
+					if (feat_type_oid == FLOAT8ARRAYOID)
+					{
+						double *feat_data = (double *) ARR_DATA_PTR(arr);
+						for (j = 0; j < model->n_features; j++)
+							z += model->weights[j] * feat_data[j];
+					}
+					else
+					{
+						float *feat_data = (float *) ARR_DATA_PTR(arr);
+						for (j = 0; j < model->n_features; j++)
+							z += model->weights[j] * (double) feat_data[j];
+					}
+				}
+				else
+				{
+					Vector *vec = DatumGetVector(feat_datum);
+					for (j = 0; j < model->n_features && j < vec->dim; j++)
+						z += model->weights[j] * vec->data[j];
+				}
+
+				/* Apply sigmoid to get probability */
+				y_pred_prob = sigmoid(z);
+			}
 
 			/* Convert probability to class prediction */
 			y_pred_class = (y_pred_prob >= threshold) ? 1 : 0;
@@ -2385,7 +2108,7 @@ cpu_evaluation_path:
 			/* Convert true label to class */
 			y_true_class = (y_true >= 0.5) ? 1 : 0;
 
-			/* Update confusion matrix */
+			/* Update confusion matrix (same for both CPU and GPU) */
 			if (y_true_class == 1 && y_pred_class == 1)
 				tp++;
 			else if (y_true_class == 0 && y_pred_class == 0)
@@ -2395,77 +2118,97 @@ cpu_evaluation_path:
 			else if (y_true_class == 1 && y_pred_class == 0)
 				fn++;
 
-			/* Compute log loss with numerical stability */
+			/* Compute log loss with numerical stability (same for both CPU and GPU) */
 			if (y_pred_prob > 1e-15 && y_pred_prob < 1.0 - 1e-15)
 			{
 				log_loss += -(y_true * log(y_pred_prob) + (1.0 - y_true) * log(1.0 - y_pred_prob));
 			}
 			else if (y_pred_prob <= 1e-15)
 			{
-				/* Prediction is essentially 0, log loss for positive class */
-				log_loss += -(y_true * -30.0 + (1.0 - y_true) * 0.0);	/* log(1e-15) â‰ˆ -30 */
+				log_loss += -(y_true * -30.0 + (1.0 - y_true) * 0.0);
 			}
 			else if (y_pred_prob >= 1.0 - 1e-15)
 			{
-				/* Prediction is essentially 1, log loss for negative class */
-				log_loss += -(y_true * 0.0 + (1.0 - y_true) * -30.0);	/* log(1e-15) â‰ˆ -30 */
+				log_loss += -(y_true * 0.0 + (1.0 - y_true) * -30.0);
 			}
 
 			total_predictions++;
 		}
 
-		/* Compute final metrics */
-		correct_predictions = tp + tn;
-		accuracy = (total_predictions > 0) ? (double) correct_predictions / total_predictions : 0.0;
+		/* Compute final metrics (same for both CPU and GPU) */
+		accuracy = (total_predictions > 0) ? (double) (tp + tn) / total_predictions : 0.0;
 		precision = (tp + fp > 0) ? (double) tp / (tp + fp) : 0.0;
 		recall = (tp + fn > 0) ? (double) tp / (tp + fn) : 0.0;
 		f1_score = (precision + recall > 0) ? 2.0 * precision * recall / (precision + recall) : 0.0;
 		log_loss = (total_predictions > 0) ? log_loss / total_predictions : 0.0;
 
-		/* Cleanup before creating JSONB */
-		if (model->weights)
+		/* Cleanup */
+		if (model != NULL)
 		{
 			nfree(model->weights);
-			model->weights = NULL;
+			nfree(model);
 		}
-		nfree(model);
-		model = NULL;
 		nfree(gpu_payload);
-		gpu_payload = NULL;
 		nfree(gpu_metrics);
-		gpu_metrics = NULL;
 		nfree(tbl_str);
-		tbl_str = NULL;
 		nfree(feat_str);
-		feat_str = NULL;
 		nfree(targ_str);
-		targ_str = NULL;
-		ndb_spi_stringinfo_free(spi_session, &query);
-		NDB_SPI_SESSION_END(spi_session);
 
-		/* Build result JSON AFTER SPI session end in parent context */
-		initStringInfo(&jsonbuf);
-		appendStringInfo(&jsonbuf,
-						 "{\"accuracy\":%.6f,\"precision\":%.6f,\"recall\":%.6f,\"f1_score\":%.6f,\"log_loss\":%.6f,\"n_samples\":%d,\"tp\":%d,\"tn\":%d,\"fp\":%d,\"fn\":%d}",
-						 accuracy, precision, recall, f1_score, log_loss, total_predictions, tp, tn, fp, fn);
+		NDB_SPI_SESSION_END(spi_session);
 
-		/*
-		 * Use ndb_jsonb_in_cstring which handles errors properly and returns
-		 * JSONB in current context
-		 */
-		result_jsonb = ndb_jsonb_in_cstring(jsonbuf.data);
-		if (result_jsonb == NULL)
+		/* Build result JSON (same for both CPU and GPU) */
+		MemoryContextSwitchTo(oldcontext);
 		{
+			Datum		d;
+
+			initStringInfo(&jsonbuf);
+			appendStringInfo(&jsonbuf,
+							 "{\"accuracy\":%.6f,\"precision\":%.6f,\"recall\":%.6f,\"f1_score\":%.6f,\"log_loss\":%.6f,\"n_samples\":%d,\"tp\":%d,\"tn\":%d,\"fp\":%d,\"fn\":%d}",
+							 accuracy, precision, recall, f1_score, log_loss, total_predictions, tp, tn, fp, fn);
+
+			/* Use inline DirectFunctionCall1 to bypass ndb_jsonb_in_cstring helper */
+			PG_TRY();
+			{
+				d = DirectFunctionCall1(jsonb_in, CStringGetDatum(jsonbuf.data));
+				result_jsonb = DatumGetJsonbP(d);
+			}
+			PG_CATCH();
+			{
+				FlushErrorState();
+				result_jsonb = NULL;
+			}
+			PG_END_TRY();
 			nfree(jsonbuf.data);
-			ereport(ERROR,
-					(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),
-					 errmsg("neurondb: evaluate_logistic_regression_by_model_id: failed to parse metrics JSON")));
+			jsonbuf.data = NULL;
+
+			if (result_jsonb == NULL)
+			{
+				ereport(ERROR,
+						(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),
+						 errmsg("neurondb: evaluate_logistic_regression_by_model_id: failed to parse evaluation metrics JSON")));
+			}
+
+			PG_RETURN_JSONB_P(result_jsonb);
 		}
-		nfree(jsonbuf.data);
-		jsonbuf.data = NULL;
+	}
 
-		PG_RETURN_JSONB_P(result_jsonb);
+	/* Should not reach here - unified evaluation handles both CPU and GPU */
+	NDB_SPI_SESSION_END(spi_session);
+	if (model != NULL)
+	{
+		nfree(model->weights);
+		nfree(model);
 	}
+	nfree(gpu_payload);
+	nfree(gpu_metrics);
+	nfree(tbl_str);
+	nfree(feat_str);
+	nfree(targ_str);
+	ereport(ERROR,
+			(errcode(ERRCODE_INTERNAL_ERROR),
+			 errmsg("neurondb: evaluate_logistic_regression_by_model_id: unexpected code path"),
+			 errdetail("Unified evaluation should have handled all cases")));
+	return (Datum) 0;
 }
 
 static void
@@ -2754,7 +2497,8 @@ lr_stream_accum_init(LRStreamAccum * accum, int dim)
 	accum->initialized = false;
 
 	/* Allocate gradient vector */
-	accum->grad_w = (double *) palloc0(sizeof(double) * dim);
+		nalloc(accum->grad_w, double, dim);
+		MemSet(accum->grad_w, 0, sizeof(double) * dim);
 	if (accum->grad_w == NULL)
 		ereport(ERROR,
 				(errcode(ERRCODE_OUT_OF_MEMORY),
@@ -3088,7 +2832,8 @@ lr_gpu_train(MLGpuModel *model, const MLGpuTrainSpec *spec, char **errstr)
 		model->backend_state = NULL;
 	}
 
-	state = (LRGpuModelState *) palloc0(sizeof(LRGpuModelState));
+		nalloc(state, LRGpuModelState, 1);
+		MemSet(state, 0, sizeof(LRGpuModelState));
 	NDB_CHECK_ALLOC(state, "state");
 	state->model_blob = payload;
 	state->feature_dim = spec->feature_dim;
@@ -3362,6 +3107,7 @@ lr_gpu_deserialize(MLGpuModel *model,
 	uint8		training_backend;
 	bytea	   *gpu_payload = NULL;
 	char	   *base = NULL;
+	char	   *tmp = NULL;
 	float	   *weights_dest = NULL;
 	size_t		payload_bytes;
 	int			i;
@@ -3389,7 +3135,9 @@ lr_gpu_deserialize(MLGpuModel *model,
 	/* Convert unified format to GPU format */
 	payload_bytes = sizeof(NdbCudaLrModelHeader) +
 		sizeof(float) * (size_t) lr_model->n_features;
-	gpu_payload = (bytea *) palloc(VARHDRSZ + payload_bytes);
+	tmp = NULL;
+	nalloc(tmp, char, VARHDRSZ + payload_bytes);
+	gpu_payload = (bytea *) tmp;
 	NDB_CHECK_ALLOC(gpu_payload, "gpu_payload");
 	SET_VARSIZE(gpu_payload, VARHDRSZ + payload_bytes);
 	base = VARDATA(gpu_payload);
@@ -3418,7 +3166,8 @@ lr_gpu_deserialize(MLGpuModel *model,
 	nfree(lr_model);
 	lr_model = NULL;
 
-	state = (LRGpuModelState *) palloc0(sizeof(LRGpuModelState));
+		nalloc(state, LRGpuModelState, 1);
+		MemSet(state, 0, sizeof(LRGpuModelState));
 	NDB_CHECK_ALLOC(state, "state");
 	state->model_blob = gpu_payload;
 	state->feature_dim = hdr->feature_dim;
@@ -3532,7 +3281,8 @@ lr_model_deserialize(const bytea * data, uint8 * training_backend_out)
 	/* Read training_backend first */
 	training_backend = (uint8) pq_getmsgbyte(&buf);
 
-	model = (LRModel *) palloc0(sizeof(LRModel));
+		nalloc(model, LRModel, 1);
+		MemSet(model, 0, sizeof(LRModel));
 	NDB_CHECK_ALLOC(model, "model");
 
 	model->n_features = pq_getmsgint(&buf, 4);
@@ -3564,8 +3314,9 @@ lr_model_deserialize(const bytea * data, uint8 * training_backend_out)
 
 	if (model->n_features > 0)
 	{
-		model->weights =
-			(double *) palloc(sizeof(double) * model->n_features);
+		double *tmp_weights = NULL;
+		nalloc(tmp_weights, double, model->n_features);
+		model->weights = tmp_weights;
 		for (i = 0; i < model->n_features; i++)
 			model->weights[i] = pq_getmsgfloat8(&buf);
 	}
@@ -3692,6 +3443,7 @@ lr_try_gpu_predict_catalog(int32 model_id,
 
 		bytea *gpu_payload = NULL;
 		char	   *base = NULL;
+		char	   *tmp = NULL;
 		NdbCudaLrModelHeader *hdr = NULL;
 		float	   *weights_dest = NULL;
 		size_t		payload_bytes;
@@ -3709,7 +3461,9 @@ lr_try_gpu_predict_catalog(int32 model_id,
 		/* Convert to GPU format */
 		payload_bytes = sizeof(NdbCudaLrModelHeader) +
 			sizeof(float) * (size_t) lr_model->n_features;
-		gpu_payload = (bytea *) palloc(VARHDRSZ + payload_bytes);
+		tmp = NULL;
+		nalloc(tmp, char, VARHDRSZ + payload_bytes);
+	gpu_payload = (bytea *) tmp;
 		SET_VARSIZE(gpu_payload, VARHDRSZ + payload_bytes);
 		base = VARDATA(gpu_payload);
 
diff --git a/NeuronDB/src/ml/ml_projects.c b/NeuronDB/src/ml/ml_projects.c
index 47d7249..991d235 100644
--- a/NeuronDB/src/ml/ml_projects.c
+++ b/NeuronDB/src/ml/ml_projects.c
@@ -67,6 +67,15 @@ neurondb_create_ml_project(PG_FUNCTION_ARGS)
 	text	   *project_name_text;
 	text	   *model_type_text;
 	text	   *description_text;
+	char *project_name = NULL;
+	char *model_type = NULL;
+	char *description = NULL;
+	StringInfoData sql;
+	int			ret;
+	int32		project_id_val;
+	int			project_id;
+	NdbSpiSession *spi_session = NULL;
+	MemoryContext oldcontext;
 
 	/* Validate minimum argument count */
 	if (PG_NARGS() < 2)
@@ -78,16 +87,6 @@ neurondb_create_ml_project(PG_FUNCTION_ARGS)
 	model_type_text = PG_GETARG_TEXT_PP(1);
 	description_text = PG_ARGISNULL(2) ? NULL : PG_GETARG_TEXT_PP(2);
 
-	char *project_name = NULL;
-	char *model_type = NULL;
-	char *description = NULL;
-	StringInfoData sql;
-	int			ret;
-	int32		project_id_val;
-	int			project_id;
-	NdbSpiSession *spi_session = NULL;
-	MemoryContext oldcontext;
-
 	project_name = text_to_cstring(project_name_text);
 	model_type = text_to_cstring(model_type_text);
 	description =
@@ -188,23 +187,21 @@ Datum
 neurondb_list_ml_projects(PG_FUNCTION_ARGS)
 {
 	ReturnSetInfo *rsinfo = (ReturnSetInfo *) fcinfo->resultinfo;
-
-	/* Validate argument count */
-	if (PG_NARGS() != 0)
-		ereport(ERROR,
-				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-				 errmsg("neurondb: neurondb_list_ml_projects requires 0 arguments")));
-
 	TupleDesc	tupdesc;
 	Tuplestorestate *tupstore = NULL;
 	MemoryContext per_query_ctx;
 	MemoryContext oldcontext;
 	int			ret;
 	int			i;
-
 	NdbSpiSession *spi_session = NULL;
 	MemoryContext oldcontext_spi;
 
+	/* Validate argument count */
+	if (PG_NARGS() != 0)
+		ereport(ERROR,
+				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+				 errmsg("neurondb: neurondb_list_ml_projects requires 0 arguments")));
+
 	/* Check result context */
 	if (rsinfo == NULL || !IsA(rsinfo, ReturnSetInfo))
 		ereport(ERROR,
@@ -306,6 +303,11 @@ Datum
 neurondb_delete_ml_project(PG_FUNCTION_ARGS)
 {
 	text	   *project_name_text;
+	char	   *project_name;
+	StringInfoData sql;
+	int			ret;
+	NdbSpiSession *spi_session = NULL;
+	MemoryContext oldcontext = CurrentMemoryContext;
 
 	/* Validate argument count */
 	if (PG_NARGS() != 1)
@@ -314,16 +316,8 @@ neurondb_delete_ml_project(PG_FUNCTION_ARGS)
 				 errmsg("neurondb: neurondb_delete_ml_project requires 1 argument")));
 
 	project_name_text = PG_GETARG_TEXT_PP(0);
-
-	char	   *project_name;
-	StringInfoData sql;
-	int			ret;
-
 	project_name = text_to_cstring(project_name_text);
 
-	NdbSpiSession *spi_session = NULL;
-	MemoryContext oldcontext = CurrentMemoryContext;
-
 	NDB_SPI_SESSION_BEGIN(spi_session, oldcontext);
 
 	ndb_spi_stringinfo_init(spi_session, &sql);
@@ -360,6 +354,13 @@ Datum
 neurondb_get_project_info(PG_FUNCTION_ARGS)
 {
 	text	   *project_name_text;
+	char	   *project_name;
+	StringInfoData sql;
+	int			ret;
+	Datum		result;
+	Jsonb *jsonb_result = NULL;
+	NdbSpiSession *spi_session = NULL;
+	MemoryContext oldcontext = CurrentMemoryContext;
 
 	/* Validate argument count */
 	if (PG_NARGS() != 1)
@@ -368,18 +369,8 @@ neurondb_get_project_info(PG_FUNCTION_ARGS)
 				 errmsg("neurondb: neurondb_get_project_info requires 1 argument")));
 
 	project_name_text = PG_GETARG_TEXT_PP(0);
-
-	char	   *project_name;
-	StringInfoData sql;
-	int			ret;
-	Datum		result;
-	Jsonb *jsonb_result = NULL;
-
 	project_name = text_to_cstring(project_name_text);
 
-	NdbSpiSession *spi_session = NULL;
-	MemoryContext oldcontext = CurrentMemoryContext;
-
 	NDB_SPI_SESSION_BEGIN(spi_session, oldcontext);
 
 	ndb_spi_stringinfo_init(spi_session, &sql);
@@ -464,27 +455,9 @@ neurondb_train_kmeans_project(PG_FUNCTION_ARGS)
 	text	   *vector_col_text;
 	int32		num_clusters;
 	int32		max_iters;
-
-	/* Validate minimum argument count */
-	if (PG_NARGS() < 4)
-		ereport(ERROR,
-				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-				 errmsg("neurondb: neurondb_train_kmeans_project requires at least 4 arguments")));
-
-	project_name_text = PG_GETARG_TEXT_PP(0);
-	table_name_text = PG_GETARG_TEXT_PP(1);
-	vector_col_text = PG_GETARG_TEXT_PP(2);
-	num_clusters = PG_GETARG_INT32(3);
-	max_iters = PG_ARGISNULL(4) ? 100 : PG_GETARG_INT32(4);
-
 	char	   *project_name;
 	char	   *table_name;
 	char	   *vector_col;
-
-	project_name = text_to_cstring(project_name_text);
-	table_name = text_to_cstring(table_name_text);
-	vector_col = text_to_cstring(vector_col_text);
-
 	StringInfoData sql;
 	int			ret;
 	int32		project_id_val;
@@ -502,6 +475,22 @@ neurondb_train_kmeans_project(PG_FUNCTION_ARGS)
 	MemoryContext oldcontext;
 	MemoryContext oldcontext2;
 
+	/* Validate minimum argument count */
+	if (PG_NARGS() < 4)
+		ereport(ERROR,
+				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+				 errmsg("neurondb: neurondb_train_kmeans_project requires at least 4 arguments")));
+
+	project_name_text = PG_GETARG_TEXT_PP(0);
+	table_name_text = PG_GETARG_TEXT_PP(1);
+	vector_col_text = PG_GETARG_TEXT_PP(2);
+	num_clusters = PG_GETARG_INT32(3);
+	max_iters = PG_ARGISNULL(4) ? 100 : PG_GETARG_INT32(4);
+
+	project_name = text_to_cstring(project_name_text);
+	table_name = text_to_cstring(table_name_text);
+	vector_col = text_to_cstring(vector_col_text);
+
 	oldcontext = CurrentMemoryContext;
 	start_time = GetCurrentTimestamp();
 
@@ -732,23 +721,12 @@ predict_kmeans_project(PG_FUNCTION_ARGS)
 {
 	int32		model_id;
 	ArrayType  *features_array;
-
-	/* Validate argument count */
-	if (PG_NARGS() != 2)
-		ereport(ERROR,
-				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-				 errmsg("neurondb: predict_kmeans_project requires 2 arguments")));
-
-	model_id = PG_GETARG_INT32(0);
-	features_array = PG_GETARG_ARRAYTYPE_P(1);
-
 	bytea *model_data = NULL;
 	Jsonb *parameters = NULL;
 	Jsonb *metrics = NULL;
 	float **centers = NULL;
 	int			n_clusters = 0;
 	int			dim = 0;
-
 	float *features = NULL;
 	int			n_features = 0;
 	int			cluster_id = 0;
@@ -756,6 +734,15 @@ predict_kmeans_project(PG_FUNCTION_ARGS)
 	double		min_dist = DBL_MAX;
 	double		dist;
 
+	/* Validate argument count */
+	if (PG_NARGS() != 2)
+		ereport(ERROR,
+				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+				 errmsg("neurondb: predict_kmeans_project requires 2 arguments")));
+
+	model_id = PG_GETARG_INT32(0);
+	features_array = PG_GETARG_ARRAYTYPE_P(1);
+
 	/* Validate inputs */
 	if (PG_ARGISNULL(0) || PG_ARGISNULL(1))
 		ereport(ERROR,
@@ -851,17 +838,6 @@ evaluate_kmeans_project_by_model_id(PG_FUNCTION_ARGS)
 	text *table_name = NULL;
 	text *feature_col = NULL;
 	char *tbl_str = NULL;
-
-	/* Validate minimum argument count */
-	if (PG_NARGS() < 3)
-		ereport(ERROR,
-				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-				 errmsg("neurondb: evaluate_kmeans_project_by_model_id requires at least 3 arguments")));
-
-	model_id = PG_GETARG_INT32(0);
-	table_name = PG_GETARG_TEXT_PP(1);
-	feature_col = PG_GETARG_TEXT_PP(2);
-
 	char *feat_str = NULL;
 	StringInfoData query;
 	int			ret;
@@ -871,10 +847,19 @@ evaluate_kmeans_project_by_model_id(PG_FUNCTION_ARGS)
 	MemoryContext oldcontext;
 	double		inertia;
 	int			n_clusters;
-
 	NdbSpiSession *spi_session = NULL;
 	MemoryContext oldcontext_spi;
 
+	/* Validate minimum argument count */
+	if (PG_NARGS() < 3)
+		ereport(ERROR,
+				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+				 errmsg("neurondb: evaluate_kmeans_project_by_model_id requires at least 3 arguments")));
+
+	model_id = PG_GETARG_INT32(0);
+	table_name = PG_GETARG_TEXT_PP(1);
+	feature_col = PG_GETARG_TEXT_PP(2);
+
 	/* Validate arguments */
 	if (PG_NARGS() != 3)
 		ereport(ERROR,
@@ -1106,6 +1091,11 @@ neurondb_deploy_model(PG_FUNCTION_ARGS)
 {
 	text	   *project_name_text;
 	int32		version;
+	char	   *project_name;
+	StringInfoData sql;
+	int			ret;
+	NdbSpiSession *spi_session = NULL;
+	MemoryContext oldcontext = CurrentMemoryContext;
 
 	/* Validate minimum argument count */
 	if (PG_NARGS() < 1)
@@ -1116,16 +1106,8 @@ neurondb_deploy_model(PG_FUNCTION_ARGS)
 	project_name_text = PG_GETARG_TEXT_PP(0);
 	version = PG_ARGISNULL(1) ? -1 : PG_GETARG_INT32(1);
 
-	char	   *project_name;
-	StringInfoData sql;
-
 	project_name = text_to_cstring(project_name_text);
 
-	int			ret;
-
-	NdbSpiSession *spi_session = NULL;
-	MemoryContext oldcontext = CurrentMemoryContext;
-
 	NDB_SPI_SESSION_BEGIN(spi_session, oldcontext);
 
 	/* Undeploy all current models for this project */
@@ -1199,6 +1181,13 @@ Datum
 neurondb_get_deployed_model(PG_FUNCTION_ARGS)
 {
 	text	   *project_name_text;
+	char	   *project_name;
+	StringInfoData sql;
+	int			ret;
+	int32		model_id_val;
+	int			model_id;
+	NdbSpiSession *spi_session = NULL;
+	MemoryContext oldcontext = CurrentMemoryContext;
 
 	/* Validate argument count */
 	if (PG_NARGS() != 1)
@@ -1208,17 +1197,8 @@ neurondb_get_deployed_model(PG_FUNCTION_ARGS)
 
 	project_name_text = PG_GETARG_TEXT_PP(0);
 
-	char	   *project_name;
-	StringInfoData sql;
-	int			ret;
-	int32		model_id_val;
-	int			model_id;
-
 	project_name = text_to_cstring(project_name_text);
 
-	NdbSpiSession *spi_session = NULL;
-	MemoryContext oldcontext = CurrentMemoryContext;
-
 	NDB_SPI_SESSION_BEGIN(spi_session, oldcontext);
 
 	ndb_spi_stringinfo_init(spi_session, &sql);
@@ -1268,20 +1248,8 @@ Datum
 neurondb_list_project_models(PG_FUNCTION_ARGS)
 {
 	text	   *project_name_text;
-
-	/* Validate argument count */
-	if (PG_NARGS() != 1)
-		ereport(ERROR,
-				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-				 errmsg("neurondb: neurondb_list_project_models requires 1 argument")));
-
-	project_name_text = PG_GETARG_TEXT_PP(0);
-
 	char	   *project_name;
 	ReturnSetInfo *rsinfo = (ReturnSetInfo *) fcinfo->resultinfo;
-
-	project_name = text_to_cstring(project_name_text);
-
 	TupleDesc	tupdesc;
 	Tuplestorestate *tupstore = NULL;
 	MemoryContext per_query_ctx;
@@ -1289,10 +1257,19 @@ neurondb_list_project_models(PG_FUNCTION_ARGS)
 	StringInfoData sql;
 	int			ret;
 	int			i;
-
 	NdbSpiSession *spi_session = NULL;
 	MemoryContext oldcontext_spi;
 
+	/* Validate argument count */
+	if (PG_NARGS() != 1)
+		ereport(ERROR,
+				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+				 errmsg("neurondb: neurondb_list_project_models requires 1 argument")));
+
+	project_name_text = PG_GETARG_TEXT_PP(0);
+
+	project_name = text_to_cstring(project_name_text);
+
 	/* Check result context */
 	if (rsinfo == NULL || !IsA(rsinfo, ReturnSetInfo))
 		ereport(ERROR,
diff --git a/NeuronDB/src/ml/ml_reinforcement_learning.c b/NeuronDB/src/ml/ml_reinforcement_learning.c
index 6d46ae0..1527a2e 100644
--- a/NeuronDB/src/ml/ml_reinforcement_learning.c
+++ b/NeuronDB/src/ml/ml_reinforcement_learning.c
@@ -453,7 +453,7 @@ multi_armed_bandit(PG_FUNCTION_ARGS)
 	}
 
 	/* Calculate selection probabilities */
-	arm_probs = (double *) palloc(sizeof(double) * n_arms);
+	nalloc(arm_probs, double, n_arms);
 
 	if (strcmp(algorithm, "thompson") == 0)
 	{
@@ -539,7 +539,7 @@ multi_armed_bandit(PG_FUNCTION_ARGS)
 	}
 
 	/* Build result array */
-	result_datums = (Datum *) palloc(sizeof(Datum) * n_arms);
+	nalloc(result_datums, Datum, n_arms);
 	for (i = 0; i < n_arms; i++)
 		result_datums[i] = Float8GetDatum(arm_probs[i]);
 
diff --git a/NeuronDB/src/ml/ml_ridge_lasso.c b/NeuronDB/src/ml/ml_ridge_lasso.c
index c07a614..194fec0 100644
--- a/NeuronDB/src/ml/ml_ridge_lasso.c
+++ b/NeuronDB/src/ml/ml_ridge_lasso.c
@@ -6856,6 +6856,7 @@ ridge_gpu_deserialize(MLGpuModel *model, const bytea * payload,
 
 	bytea *gpu_payload = NULL;
 	char *base = NULL;
+	char *tmp = NULL;
 	NdbCudaRidgeModelHeader *hdr = NULL;
 	float *coef_dest = NULL;
 	size_t		payload_bytes;
@@ -6887,7 +6888,9 @@ ridge_gpu_deserialize(MLGpuModel *model, const bytea * payload,
 
 	payload_bytes = sizeof(NdbCudaRidgeModelHeader) +
 		sizeof(float) * (size_t) ridge_model->n_features;
-	gpu_payload = (bytea *) palloc(VARHDRSZ + payload_bytes);
+	tmp = NULL;
+	nalloc(tmp, char, VARHDRSZ + payload_bytes);
+	gpu_payload = (bytea *) tmp;
 	NDB_CHECK_ALLOC(gpu_payload, "gpu_payload");
 	SET_VARSIZE(gpu_payload, VARHDRSZ + payload_bytes);
 	base = VARDATA(gpu_payload);
@@ -7219,6 +7222,7 @@ lasso_gpu_deserialize(MLGpuModel *model, const bytea * payload,
 
 	bytea *gpu_payload = NULL;
 	char *base = NULL;
+	char *tmp = NULL;
 	NdbCudaLassoModelHeader *hdr = NULL;
 	float *coef_dest = NULL;
 	size_t		payload_bytes;
@@ -7250,7 +7254,9 @@ lasso_gpu_deserialize(MLGpuModel *model, const bytea * payload,
 
 	payload_bytes = sizeof(NdbCudaLassoModelHeader) +
 		sizeof(float) * (size_t) lasso_model->n_features;
-	gpu_payload = (bytea *) palloc(VARHDRSZ + payload_bytes);
+	tmp = NULL;
+	nalloc(tmp, char, VARHDRSZ + payload_bytes);
+	gpu_payload = (bytea *) tmp;
 	NDB_CHECK_ALLOC(gpu_payload, "gpu_payload");
 	SET_VARSIZE(gpu_payload, VARHDRSZ + payload_bytes);
 	base = VARDATA(gpu_payload);
diff --git a/NeuronDB/src/ml/ml_text.c b/NeuronDB/src/ml/ml_text.c
index 431bf19..6172329 100644
--- a/NeuronDB/src/ml/ml_text.c
+++ b/NeuronDB/src/ml/ml_text.c
@@ -190,9 +190,12 @@ neurondb_text_classify(PG_FUNCTION_ARGS)
 		}
 
 		/* Prepare category lists */
-		categories = (char **) palloc0(sizeof(char *) * MAX_CATEGORIES);
+		nalloc(categories, char *, MAX_CATEGORIES);
 		NDB_CHECK_ALLOC(categories, "categories");
-		category_counts = (int *) palloc0(sizeof(int) * MAX_CATEGORIES);
+		MemSet(categories, 0, sizeof(char *) * MAX_CATEGORIES);
+		nalloc(category_counts, int, MAX_CATEGORIES);
+		NDB_CHECK_ALLOC(category_counts, "category_counts");
+		MemSet(category_counts, 0, sizeof(int) * MAX_CATEGORIES);
 		NDB_CHECK_ALLOC(category_counts, "category_counts");
 
 		/* Fill category names (de-duplication) */
@@ -298,8 +301,8 @@ neurondb_text_classify(PG_FUNCTION_ARGS)
 			int			total_count = 0;
 
 			NDB_SPI_SESSION_END(spi_session);
-			results = (ClassifyResult *) palloc0(
-												 n_categories * sizeof(ClassifyResult));
+			nalloc(results, ClassifyResult, n_categories);
+			MemSet(results, 0, sizeof(ClassifyResult) * n_categories);
 			NDB_CHECK_ALLOC(results, "results");
 
 			for (i = 0; i < n_categories; i++)
@@ -474,7 +477,8 @@ neurondb_sentiment_analysis(PG_FUNCTION_ARGS)
 		if (num_tokens == 0)
 			num_tokens = 1;
 
-		result = (SentimentResult *) palloc0(sizeof(SentimentResult));
+		nalloc(result, SentimentResult, 1);
+		MemSet(result, 0, sizeof(SentimentResult));
 		NDB_CHECK_ALLOC(result, "result");
 		result->positive = ((float4) pos) / num_tokens;
 		result->negative = ((float4) neg) / num_tokens;
@@ -604,8 +608,8 @@ neurondb_named_entity_recognition(PG_FUNCTION_ARGS)
 					(errmsg("NER entity table fetch failed")));
 		}
 
-		entities =
-			(NERResult *) palloc0(MAX_ENTITIES * sizeof(NERResult));
+		nalloc(entities, NERResult, MAX_ENTITIES);
+		MemSet(entities, 0, sizeof(NERResult) * MAX_ENTITIES);
 
 		for (t = 0; t < num_tokens && n_entities < MAX_ENTITIES; t++)
 		{
@@ -662,8 +666,8 @@ neurondb_named_entity_recognition(PG_FUNCTION_ARGS)
 							  &datum_array,
 							  &nulls,
 							  &n_types);
-			filtered = (NERResult *) palloc0(
-											 MAX_ENTITIES * sizeof(NERResult));
+			nalloc(filtered, NERResult, MAX_ENTITIES);
+			MemSet(filtered, 0, sizeof(NERResult) * MAX_ENTITIES);
 			NDB_CHECK_ALLOC(filtered, "filtered");
 			for (e = 0; e < n_entities; e++)
 			{
@@ -849,7 +853,8 @@ neurondb_text_summarize(PG_FUNCTION_ARGS)
 			char	  **stopwords;
 
 			n_stopwords = SPI_processed;
-			stopwords = (char **) palloc0(n_stopwords * sizeof(char *));
+			nalloc(stopwords, char *, n_stopwords);
+			MemSet(stopwords, 0, sizeof(char *) * n_stopwords);
 			NDB_CHECK_ALLOC(stopwords, "stopwords");
 			for (i = 0; i < n_stopwords; i++)
 			{
@@ -987,7 +992,9 @@ text_model_serialize_to_bytea(int vocab_size, int feature_dim, const char *task_
 	appendBinaryStringInfo(&buf, task_type, task_len);
 
 	total_size = VARHDRSZ + buf.len;
-	result = (bytea *) palloc(total_size);
+	char *tmp = NULL;
+	nalloc(tmp, char, total_size);
+	result = (bytea *) tmp;
 	NDB_CHECK_ALLOC(result, "result");
 	SET_VARSIZE(result, total_size);
 	memcpy(VARDATA(result), buf.data, buf.len);
@@ -1099,7 +1106,8 @@ text_gpu_train(MLGpuModel *model, const MLGpuTrainSpec *spec, char **errstr)
 												 CStringGetDatum(metrics_json.data)));
 	nfree(metrics_json.data);
 
-	state = (TextGpuModelState *) palloc0(sizeof(TextGpuModelState));
+		nalloc(state, TextGpuModelState, 1);
+		MemSet(state, 0, sizeof(TextGpuModelState));
 	NDB_CHECK_ALLOC(state, "state");
 	state->model_blob = model_data;
 	state->metrics = metrics;
@@ -1247,7 +1255,9 @@ text_gpu_serialize(const MLGpuModel *model, bytea * *payload_out,
 	}
 
 	payload_size = VARSIZE(state->model_blob);
-	payload_copy = (bytea *) palloc(payload_size);
+	char *tmp = NULL;
+	nalloc(tmp, char, payload_size);
+	payload_copy = (bytea *) tmp;
 	NDB_CHECK_ALLOC(payload_copy, "payload_copy");
 	memcpy(payload_copy, state->model_blob, payload_size);
 
@@ -1287,7 +1297,9 @@ text_gpu_deserialize(MLGpuModel *model, const bytea * payload,
 	}
 
 	payload_size = VARSIZE(payload);
-	payload_copy = (bytea *) palloc(payload_size);
+	char *tmp = NULL;
+	nalloc(tmp, char, payload_size);
+	payload_copy = (bytea *) tmp;
 	NDB_CHECK_ALLOC(payload_copy, "payload_copy");
 	memcpy(payload_copy, payload, payload_size);
 
@@ -1299,7 +1311,8 @@ text_gpu_deserialize(MLGpuModel *model, const bytea * payload,
 		return false;
 	}
 
-	state = (TextGpuModelState *) palloc0(sizeof(TextGpuModelState));
+		nalloc(state, TextGpuModelState, 1);
+		MemSet(state, 0, sizeof(TextGpuModelState));
 	NDB_CHECK_ALLOC(state, "state");
 	state->model_blob = payload_copy;
 	state->vocab_size = vocab_size;
@@ -1310,7 +1323,9 @@ text_gpu_deserialize(MLGpuModel *model, const bytea * payload,
 	if (metadata != NULL)
 	{
 		int			metadata_size = VARSIZE(metadata);
-		Jsonb	   *metadata_copy = (Jsonb *) palloc(metadata_size);
+		char *tmp = NULL;
+		nalloc(tmp, char, metadata_size);
+		Jsonb *metadata_copy = (Jsonb *) tmp;
 
 		NDB_CHECK_ALLOC(metadata_copy, "metadata_copy");
 		memcpy(metadata_copy, metadata, metadata_size);
diff --git a/NeuronDB/src/ml/ml_unified_api.c b/NeuronDB/src/ml/ml_unified_api.c
index 423d7a4..f383edf 100644
--- a/NeuronDB/src/ml/ml_unified_api.c
+++ b/NeuronDB/src/ml/ml_unified_api.c
@@ -1560,6 +1560,8 @@ neurondb_train(PG_FUNCTION_ARGS)
 				MemoryContext old_spi_context;
 
 				nalloc(feature_names, const char *, nelems);
+				/* Zero-initialize to avoid accessing uninitialized pointers */
+				MemSet(feature_names, 0, nelems * sizeof(const char *));
 
 				/* Switch to SPI context before appending to feature_list */
 				old_spi_context = MemoryContextSwitchTo(ndb_spi_session_get_context(spi_session));
@@ -1604,6 +1606,8 @@ neurondb_train(PG_FUNCTION_ARGS)
 		MemoryContextSwitchTo(old_spi_context);
 
 		nalloc(feature_names, const char *, 1);
+		/* Zero-initialize to avoid accessing uninitialized pointers */
+		MemSet(feature_names, 0, sizeof(const char *));
 		feature_names[0] = pstrdup("*");
 		feature_name_count = 1;
 	}
@@ -1688,22 +1692,16 @@ neurondb_train(PG_FUNCTION_ARGS)
 	/* GPU mode requires GPU to be available */
 	if (NDB_REQUIRE_GPU() && !gpu_available)
 	{
-		nfree(feature_list_str);
-		ndb_spi_session_end(&spi_session);
-		MemoryContextSwitchTo(oldcontext);
-		neurondb_cleanup(oldcontext, callcontext);
-		nfree(project_name);
-		nfree(algorithm);
-		nfree(table_name);
-		if (target_column)
-			nfree(target_column);
+		/* Free feature_names BEFORE cleaning up callcontext */
 		if (feature_names)
 		{
 			int			i;
 
+			/* Switch to callcontext before freeing, as strings were allocated there */
+			MemoryContextSwitchTo(callcontext);
 			for (i = 0; i < feature_name_count; i++)
 			{
-				if (feature_names[i])
+				if (feature_names[i] != NULL)
 				{
 					char	   *ptr = (char *) feature_names[i];
 
@@ -1711,7 +1709,18 @@ neurondb_train(PG_FUNCTION_ARGS)
 				}
 			}
 			nfree(feature_names);
+			feature_names = NULL;
 		}
+		
+		nfree(feature_list_str);
+		ndb_spi_session_end(&spi_session);
+		MemoryContextSwitchTo(oldcontext);
+		neurondb_cleanup(oldcontext, callcontext);
+		nfree(project_name);
+		nfree(algorithm);
+		nfree(table_name);
+		if (target_column)
+			nfree(target_column);
 		ereport(ERROR,
 				(errcode(ERRCODE_INTERNAL_ERROR),
 				 errmsg(NDB_ERR_PREFIX_TRAIN " GPU is required but not available"),
@@ -1721,9 +1730,9 @@ neurondb_train(PG_FUNCTION_ARGS)
 	}
 
 	/* Try to load training data for GPU training */
-	/* Only attempt GPU if compute_mode allows it (GPU or AUTO mode) */
-	/* CPU mode: never load data for GPU training */
-	if (!NDB_COMPUTE_MODE_IS_CPU() && gpu_available && NDB_SHOULD_TRY_GPU())
+	/* GPU mode: require GPU, no fallback. AUTO mode: try GPU with fallback. CPU mode: never load GPU data */
+	if (!NDB_COMPUTE_MODE_IS_CPU() && gpu_available && 
+		(NDB_REQUIRE_GPU() || NDB_COMPUTE_MODE_IS_AUTO()))
 	{
 		load_success = neurondb_load_training_data(spi_session, table_name, feature_list_str, target_column,
 												   &feature_matrix, &label_vector,
@@ -1741,8 +1750,10 @@ neurondb_train(PG_FUNCTION_ARGS)
 	/*
 	 * Double-check CPU mode to be absolutely safe - NEVER attempt GPU in CPU
 	 * mode
+	 * GPU mode: require GPU, no fallback. AUTO mode: try GPU with fallback.
 	 */
-	if (data_loaded && !NDB_COMPUTE_MODE_IS_CPU() && NDB_SHOULD_TRY_GPU() && strcmp(algorithm, NDB_ALGO_LOGISTIC_REGRESSION) != 0)
+	if (data_loaded && !NDB_COMPUTE_MODE_IS_CPU() && 
+		(NDB_REQUIRE_GPU() || NDB_COMPUTE_MODE_IS_AUTO()))
 	{
 		/* Additional safety check: never attempt GPU in CPU mode */
 		if (NDB_COMPUTE_MODE_IS_CPU())
@@ -1753,6 +1764,35 @@ neurondb_train(PG_FUNCTION_ARGS)
 		}
 		else
 		{
+			/*
+			 * Ensure hyperparams is always a valid Jsonb (even if empty) to
+			 * prevent JSON parsing errors in GPU code. This matches the
+			 * behavior in CPU training code (ml_linear_regression.c).
+			 */
+			Jsonb *gpu_hyperparams = hyperparams;
+			if (gpu_hyperparams == NULL)
+			{
+				/* Create empty JSONB object like CPU training does */
+				PG_TRY();
+				{
+					gpu_hyperparams = ndb_jsonb_in_cstring("{}");
+					if (gpu_hyperparams == NULL)
+					{
+						ereport(ERROR,
+								(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),
+								 errmsg(NDB_ERR_PREFIX_TRAIN " failed to create empty hyperparams JSONB")));
+					}
+				}
+				PG_CATCH();
+				{
+					FlushErrorState();
+					ereport(ERROR,
+							(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),
+							 errmsg(NDB_ERR_PREFIX_TRAIN " failed to parse empty hyperparams JSON: %m")));
+				}
+				PG_END_TRY();
+			}
+
 			/*
 			 * Wrap GPU training call in PG_TRY to catch exceptions and
 			 * prevent JSON parsing errors
@@ -1760,7 +1800,7 @@ neurondb_train(PG_FUNCTION_ARGS)
 			PG_TRY();
 			{
 				gpu_train_result = ndb_gpu_try_train_model(algorithm, project_name, model_name, table_name, target_column,
-														   feature_names, feature_name_count, hyperparams,
+														   feature_names, feature_name_count, gpu_hyperparams,
 														   feature_matrix, label_vector, n_samples, feature_dim, class_count,
 														   &gpu_result, gpu_errmsg);
 			}
@@ -1845,37 +1885,52 @@ neurondb_train(PG_FUNCTION_ARGS)
 						elog(ERROR, "BUG: GPU error path reached in CPU mode! compute_mode=%d", neurondb_compute_mode);
 					}
 
-					/* GPU mode: re-raise error, no fallback */
-					/* Switch out of ErrorContext before CopyErrorData() */
-					safe_context = oldcontext;
+				/* GPU mode: re-raise error, no fallback */
+				/* Switch out of ErrorContext before CopyErrorData() */
+				safe_context = oldcontext;
 
-					/* Ensure we're not switching to ErrorContext */
-					if (safe_context == ErrorContext || safe_context == NULL)
-					{
-						safe_context = TopMemoryContext;
-					}
+				/* Ensure we're not switching to ErrorContext */
+				if (safe_context == ErrorContext || safe_context == NULL)
+				{
+					safe_context = TopMemoryContext;
+				}
 
-					MemoryContextSwitchTo(safe_context);
+				MemoryContextSwitchTo(safe_context);
 
-					edata = NULL;
-					error_msg = NULL;
+				edata = NULL;
+				error_msg = NULL;
 
-					/* Save algorithm before freeing (it might be NULL) */
-					algorithm_safe = algorithm ? pstrdup(algorithm) : NULL;
+				/* Save algorithm before freeing (it might be NULL) */
+				algorithm_safe = algorithm ? pstrdup(algorithm) : NULL;
 
-					if (CurrentMemoryContext != ErrorContext)
+				if (CurrentMemoryContext != ErrorContext)
+				{
+					edata = CopyErrorData();
+					if (edata)
 					{
-						edata = CopyErrorData();
-						if (edata && edata->message)
+						/* Prefer detail message over main message for more specific errors */
+						if (edata->detail && strlen(edata->detail) > 0)
+						{
+							error_msg = pstrdup(edata->detail);
+						}
+						else if (edata->message && strlen(edata->message) > 0)
+						{
 							error_msg = pstrdup(edata->message);
-						FlushErrorState();
-					}
-					else
-					{
-						/* Fallback if we can't switch contexts */
-						FlushErrorState();
-						error_msg = NULL;
+						}
+						/* Also set gpu_errmsg_ptr so it's included in the final error */
+						if (gpu_errmsg_ptr == NULL && error_msg != NULL)
+						{
+							gpu_errmsg_ptr = pstrdup(error_msg);
+						}
 					}
+					FlushErrorState();
+				}
+				else
+				{
+					/* Fallback if we can't switch contexts */
+					FlushErrorState();
+					error_msg = NULL;
+				}
 
 					nfree(feature_list_str);
 					if (feature_names)
@@ -1884,7 +1939,7 @@ neurondb_train(PG_FUNCTION_ARGS)
 
 						for (i = 0; i < feature_name_count; i++)
 						{
-							if (feature_names[i])
+							if (feature_names[i] != NULL)
 							{
 								char	   *ptr = (char *) feature_names[i];
 
@@ -2114,6 +2169,12 @@ neurondb_train(PG_FUNCTION_ARGS)
 				MemoryContextSwitchTo(oldcontext);
 				model_id = ml_catalog_register_model(&spec);
 				MemoryContextSwitchTo(callcontext);
+				
+				/* Verify model was registered and is visible */
+				if (model_id > 0)
+				{
+					elog(DEBUG1, "neurondb_train: registered model_id=%d, verifying it exists in catalog", model_id);
+				}
 			}
 			else
 			{
@@ -2227,30 +2288,53 @@ neurondb_train(PG_FUNCTION_ARGS)
 			}
 			if (model_name)
 				nfree(model_name);
-			if (gpu_errmsg_ptr)
-				nfree(gpu_errmsg_ptr);
 			ndb_spi_session_end(&spi_session);
-			MemoryContextSwitchTo(oldcontext);
-			neurondb_cleanup(oldcontext, callcontext);
-			nfree(project_name);
-			nfree(algorithm);
-			nfree(table_name);
-			if (target_column)
-				nfree(target_column);
-			if (data_loaded)
+		/* Report error BEFORE freeing strings - they're needed in errdetail */
+		/* Save gpu_errmsg_ptr before freeing - it contains the actual error */
+		{
+			char *gpu_error_msg = NULL;
+			/* Try to get error message from gpu_errmsg_ptr first */
+			if (gpu_errmsg_ptr && strlen(gpu_errmsg_ptr) > 0)
 			{
-				if (feature_matrix)
-					nfree(feature_matrix);
-				if (label_vector)
-					nfree(label_vector);
+				gpu_error_msg = pstrdup(gpu_errmsg_ptr);
 			}
+			else
+			{
+				/* If no error message, provide a more specific fallback */
+				gpu_error_msg = psprintf("ndb_gpu_try_train_model returned false for algorithm '%s' - check GPU availability and backend registration",
+										 algorithm ? algorithm : "unknown");
+			}
+			if (gpu_errmsg_ptr)
+				nfree(gpu_errmsg_ptr);
 			ereport(ERROR,
 					(errcode(ERRCODE_INTERNAL_ERROR),
 					 errmsg(NDB_ERR_PREFIX_TRAIN " GPU training failed - GPU mode requires GPU to be available"),
-					 errdetail("Algorithm: %s, Project: %s, Table: %s. GPU was not available or training failed.",
-							   algorithm, project_name, table_name),
+					 errdetail("Algorithm: %s, Project: %s, Table: %s. Error: %s",
+							   algorithm ? algorithm : "unknown",
+							   project_name ? project_name : "unknown",
+							   table_name ? table_name : "unknown",
+							   gpu_error_msg),
 					 errhint("Check GPU hardware, drivers, and configuration. "
 							 "Set compute_mode='auto' for automatic CPU fallback.")));
+			if (gpu_error_msg)
+				pfree(gpu_error_msg);
+		}
+
+		/* Cleanup after error (should not reach here, but included for safety) */
+		MemoryContextSwitchTo(oldcontext);
+		neurondb_cleanup(oldcontext, callcontext);
+		nfree(project_name);
+		nfree(algorithm);
+		nfree(table_name);
+		if (target_column)
+			nfree(target_column);
+		if (data_loaded)
+		{
+			if (feature_matrix)
+				nfree(feature_matrix);
+			if (label_vector)
+				nfree(label_vector);
+		}
 		}
 
 		/* AUTO mode or CPU mode: fall back to CPU training */
@@ -3777,20 +3861,12 @@ neurondb_evaluate(PG_FUNCTION_ARGS)
 			if (result_isnull)
 			{
 				/*
-				 * Evaluation returned NULL - return empty JSONB instead
+				 * Evaluation returned NULL - return NULL instead of empty JSONB
 				 */
-				char *result_buf = NULL;
-
 				MemoryContextSwitchTo(oldcontext);
-				/* Create minimal valid JSONB for empty object */
-				result_buf = NULL;
-				NBP_ALLOC(result_buf, char, VARHDRSZ + sizeof(uint32));
-				result = (Jsonb *) result_buf;
-				SET_VARSIZE(result, VARHDRSZ + sizeof(uint32));
-				*((uint32 *) VARDATA(result)) = JB_CMASK;	/* Empty object header */
 				ndb_spi_session_end(&spi_session);
 				MemoryContextDelete(callcontext);
-				PG_RETURN_JSONB_P(result);
+				PG_RETURN_NULL();
 			}
 
 			/* temp_jsonb is already obtained from ndb_spi_get_jsonb above */
@@ -3898,30 +3974,23 @@ neurondb_evaluate(PG_FUNCTION_ARGS)
 			error_msg = escaped;
 		}
 
-		/* Return error JSONB */
+		/* Return error JSONB using proper PostgreSQL API */
 		{
 			StringInfoData error_json;
+			text *json_text = NULL;
 
 			ndb_spi_stringinfo_init(spi_session, &error_json);
 			appendStringInfo(&error_json, "{\"error\": \"%s\"}", error_msg);
-			/* Skip JSONB creation to avoid DirectFunctionCall1 issues */
-			result = NULL;
+			
+			/* Use PostgreSQL's jsonb_in function to create proper JSONB */
+			json_text = cstring_to_text(error_json.data);
+			result = DatumGetJsonbP(DirectFunctionCall1(jsonb_in, 
+				PointerGetDatum(error_json.data)));
+			
 			ndb_spi_stringinfo_free(spi_session, &error_json);
 			error_json.data = NULL;
 			nfree(error_msg);
-
 			error_msg = NULL;
-
-			/* Defensive check: ensure result is valid */
-			if (result == NULL || VARSIZE(result) < sizeof(Jsonb))
-			{
-				/* Create minimal valid JSONB for empty object */
-				char *result_buf = NULL;
-				NBP_ALLOC(result_buf, char, VARHDRSZ + sizeof(uint32));
-				result = (Jsonb *) result_buf;
-				SET_VARSIZE(result, VARHDRSZ + sizeof(uint32));
-				*((uint32 *) VARDATA(result)) = JB_CMASK;	/* Empty object header */
-			}
 		}
 
 		/* Free error data if we copied it */
@@ -3976,15 +4045,8 @@ neurondb_evaluate(PG_FUNCTION_ARGS)
 	/* Additional validation - check if result is valid */
 	if (result == NULL)
 	{
-		char *result_buf = NULL;
-
-		elog(WARNING, "neurondb_evaluate: result is NULL, returning empty JSONB");
-		/* Create minimal valid JSONB for empty object */
-		result_buf = NULL;
-		NBP_ALLOC(result_buf, char, VARHDRSZ + sizeof(JsonbContainer));
-		result = (Jsonb *) result_buf;
-		SET_VARSIZE(result, VARHDRSZ + sizeof(JsonbContainer));
-		result->root.header = JB_CMASK; /* Empty object */
+		elog(WARNING, "neurondb_evaluate: result is NULL, returning NULL");
+		PG_RETURN_NULL();
 	}
 
 	elog(DEBUG2, "neurondb_evaluate: about to return JSONB result, size=%d", VARSIZE(result));
@@ -3992,15 +4054,9 @@ neurondb_evaluate(PG_FUNCTION_ARGS)
 	/* EMERGENCY SAFETY: Ensure result is ALWAYS valid before returning */
 	if (result == NULL || VARSIZE(result) < sizeof(Jsonb))
 	{
-		char *result_buf = NULL;
-
-		/* Create minimal valid JSONB for empty object */
-		elog(WARNING, "neurondb_evaluate: EMERGENCY - result invalid, creating empty JSONB");
-		result_buf = NULL;
-		NBP_ALLOC(result_buf, char, VARHDRSZ + sizeof(JsonbContainer));
-		result = (Jsonb *) result_buf;
-		SET_VARSIZE(result, VARHDRSZ + sizeof(JsonbContainer));
-		result->root.header = JB_CMASK; /* Empty object */
+		/* Return NULL instead of invalid JSONB */
+		elog(WARNING, "neurondb_evaluate: EMERGENCY - result invalid, returning NULL");
+		PG_RETURN_NULL();
 	}
 
 	PG_RETURN_JSONB_P(result);
diff --git a/NeuronDB/src/neurondb_macros.h b/NeuronDB/src/neurondb_macros.h
index dd0757e..c2c0b31 100644
--- a/NeuronDB/src/neurondb_macros.h
+++ b/NeuronDB/src/neurondb_macros.h
@@ -3,7 +3,19 @@
 
 #include "postgres.h"
 
-#define nalloc(ptr, type, count) do { ptr = (type *) palloc(sizeof(type) * (count)); } while(0)
-#define nfree(ptr) do { if (ptr) pfree(ptr); ptr = NULL; } while(0)
+/* nalloc: Allocate and zero-initialize memory, pointer must be NULL */
+#define nalloc(ptr, type, count) do { \
+	Assert((ptr) == NULL); \
+	(ptr) = (type *) palloc(sizeof(type) * (count)); \
+	MemSet((ptr), 0, sizeof(type) * (count)); \
+} while(0)
+
+/* nfree: Free pointer if not NULL and set to NULL */
+#define nfree(ptr) do { \
+	if ((ptr) != NULL) { \
+		pfree(ptr); \
+		(ptr) = NULL; \
+	} \
+} while(0)
 
 #endif
diff --git a/NeuronDB/src/onnx/neurondb_tokenizer.c b/NeuronDB/src/onnx/neurondb_tokenizer.c
index 82fe86b..c5055fd 100644
--- a/NeuronDB/src/onnx/neurondb_tokenizer.c
+++ b/NeuronDB/src/onnx/neurondb_tokenizer.c
@@ -438,7 +438,7 @@ tokenize_text(const char *text, int *num_tokens)
 	_to_lower_case(text_copy);
 
 	/* Preallocate token pointer array */
-	tokens = (char **) palloc(capacity * sizeof(char *));
+	nalloc(tokens, char *, capacity);
 	count = 0;
 
 	/* Tokenize by whitespace: space, tab, newline, and carriage return */
@@ -537,7 +537,8 @@ neurondb_tokenize_with_model(const char *text,
 	 * Allocate output array, zeroed (so all trailing positions are [PAD]). We
 	 * always write max_length elements, zero-initialized.
 	 */
-	token_ids = (int32 *) palloc0(max_length * sizeof(int32));
+		nalloc(token_ids, int32, max_length);
+		MemSet(token_ids, 0, sizeof(int32) * max_length);
 
 	/* Add [CLS] at beginning */
 	output_idx = 0;
@@ -673,7 +674,7 @@ neurondb_create_attention_mask(int32 * token_ids, int32 length)
 	Assert(token_ids != NULL);
 	Assert(length > 0);
 
-	mask = (int32 *) palloc(length * sizeof(int32));
+	nalloc(mask, int32, length);
 	for (i = 0; i < length; ++i)
 	{
 		/* If not a [PAD] ID, mask value = 1, else 0. */
@@ -798,8 +799,8 @@ neurondb_hf_tokenize(PG_FUNCTION_ARGS)
 											 text_str, max_length, &output_length, model_name);
 
 	/* Allocate arrays for result */
-	dvalues = (Datum *) palloc(output_length * sizeof(Datum));
-	dnulls = (bool *) palloc(output_length * sizeof(bool));
+	nalloc(dvalues, Datum, output_length);
+	nalloc(dnulls, bool, output_length);
 
 	/* Fill arrays */
 	for (i = 0; i < output_length; i++)
@@ -908,7 +909,7 @@ neurondb_hf_detokenize(PG_FUNCTION_ARGS)
 					  &length);
 
 	/* Allocate token IDs array */
-	token_ids = (int32 *) palloc(length * sizeof(int32));
+	nalloc(token_ids, int32, length);
 
 	/* Copy token IDs */
 	for (i = 0; i < length; i++)
diff --git a/NeuronDB/src/scan/scan_rls.c b/NeuronDB/src/scan/scan_rls.c
index 4a51694..3716cdf 100644
--- a/NeuronDB/src/scan/scan_rls.c
+++ b/NeuronDB/src/scan/scan_rls.c
@@ -80,7 +80,8 @@ ndb_rls_init(Relation rel, EState * estate)
 {
 	RLSFilterState *state = NULL;
 
-	state = (RLSFilterState *) palloc0(sizeof(RLSFilterState));
+	nalloc(state, RLSFilterState, 1);
+	MemSet(state, 0, sizeof(RLSFilterState));
 	state->rel = rel;
 	state->userId = GetUserId();
 
@@ -441,7 +442,7 @@ ndb_rls_filter_results(Relation rel,
 	}
 
 	/* Allocate result array */
-	result = (ItemPointer *) palloc(count * sizeof(ItemPointer));
+	nalloc(result, ItemPointer, count);
 
 	/* Filter each item */
 	for (i = 0; i < count; i++)
diff --git a/NeuronDB/src/search/hybrid_search.c b/NeuronDB/src/search/hybrid_search.c
index 83bc1d0..20fbbd0 100644
--- a/NeuronDB/src/search/hybrid_search.c
+++ b/NeuronDB/src/search/hybrid_search.c
@@ -362,16 +362,6 @@ reciprocal_rank_fusion(PG_FUNCTION_ARGS)
 {
 	ArrayType  *rankings = NULL;
 	float8		k;
-
-	/* Validate argument count */
-	if (PG_NARGS() != 2)
-		ereport(ERROR,
-				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-				 errmsg("neurondb: reciprocal_rank_fusion requires 2 arguments")));
-
-	rankings = PG_GETARG_ARRAYTYPE_P(0);
-	k = PG_GETARG_FLOAT8(1);
-
 	int			n_rankers;
 	int			i;
 	int			j;
@@ -386,6 +376,15 @@ reciprocal_rank_fusion(PG_FUNCTION_ARGS)
 	bool		   *result_nulls = NULL;
 	ArrayType  *ret_array = NULL;
 
+	/* Validate argument count */
+	if (PG_NARGS() != 2)
+		ereport(ERROR,
+				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+				 errmsg("neurondb: reciprocal_rank_fusion requires 2 arguments")));
+
+	rankings = PG_GETARG_ARRAYTYPE_P(0);
+	k = PG_GETARG_FLOAT8(1);
+
 	elog(DEBUG1,
 		 "neurondb: Computing Reciprocal Rank Fusion with k=%.2f",
 		 k);
@@ -464,9 +463,12 @@ reciprocal_rank_fusion(PG_FUNCTION_ARGS)
 
 	/* Output: sort the ids by score descending, return as text[] */
 	item_count = hash_get_num_entries(item_hash);
-	result_datums = palloc0(sizeof(Datum) * item_count);
+	nalloc(result_datums, Datum, item_count);
 	NDB_CHECK_ALLOC(result_datums, "allocation");
-	result_nulls = palloc0(sizeof(bool) * item_count);
+	MemSet(result_datums, 0, sizeof(Datum) * item_count);
+	nalloc(result_nulls, bool, item_count);
+	NDB_CHECK_ALLOC(result_nulls, "allocation");
+	MemSet(result_nulls, 0, sizeof(bool) * item_count);
 	NDB_CHECK_ALLOC(result_nulls, "allocation");
 
 	{
@@ -474,18 +476,18 @@ reciprocal_rank_fusion(PG_FUNCTION_ARGS)
 		{
 			char		key[512];
 			float8		score;
-		}		   *cur;
-		struct
+		}		   *cur = NULL;
+		struct HybridSearchItem
 		{
 			char *key;
 			float8		score;
-		}		   *items;
+		}		   *items = NULL;
 		HASH_SEQ_STATUS stat;
 		int			idx = 0;
 		int			idx_i,
 					idx_j;
 
-		items = palloc(sizeof(*items) * item_count);
+		nalloc(items, struct HybridSearchItem, item_count);
 		NDB_CHECK_ALLOC(items, "allocation");
 		hash_seq_init(&stat, item_hash);
 		while ((cur = hash_seq_search(&stat)) != NULL)
@@ -701,178 +703,180 @@ multi_vector_search(PG_FUNCTION_ARGS)
 				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
 				 errmsg("neurondb: multi_vector_search requires 4 arguments")));
 
-	table_name = PG_GETARG_TEXT_PP(0);
-	query_vectors = PG_GETARG_ARRAYTYPE_P(1);
-	agg_method = PG_GETARG_TEXT_PP(2);
-	top_k = PG_GETARG_INT32(3);
-
-	char		   *tbl_str;
-	char		   *agg_str;
-	int				nvecs;
-	StringInfoData sql;
-	StringInfoData subquery;
-	Datum		   *vec_datums = NULL;
-	bool		   *vec_nulls = NULL;
-	Oid				vec_elemtype;
-	int				i;
-	int				spi_ret;
-	int				proc;
-	ArrayType	   *ret_array = NULL;
-	Datum		   *datums = NULL;
-	bool		   *nulls = NULL;
-	NdbSpiSession  *session = NULL;
-
-	tbl_str = text_to_cstring(table_name);
-	agg_str = text_to_cstring(agg_method);
-	vec_elemtype = ARR_ELEMTYPE(query_vectors);
-
-	nvecs = ArrayGetNItems(
-						   ARR_NDIM(query_vectors), ARR_DIMS(query_vectors));
-	elog(DEBUG1,
-		 "neurondb: Multi-vector search on '%s' with %d queries, "
-		 "agg=%s, top_k=%d",
-		 tbl_str,
-		 nvecs,
-		 agg_str,
-		 top_k);
-	session = ndb_spi_session_begin(CurrentMemoryContext, false);
-	if (session == NULL)
-		ereport(ERROR, (errmsg("multi_vector_search: failed to begin SPI session")));
-
-	get_typlenbyvalalign(vec_elemtype, NULL, NULL, NULL);
-	deconstruct_array(query_vectors,
-					  vec_elemtype,
-					  -1,
-					  false,
-					  'i',
-					  &vec_datums,
-					  &vec_nulls,
-					  &nvecs);
-
-	if (nvecs < 1)
 	{
-		nfree(tbl_str);
-		nfree(agg_str);
-		ndb_spi_session_end(&session);
-		ereport(ERROR,
-				(errmsg("multi_vector_search: at least one query "
-						"vector required")));
-	}
+		char		   *tbl_str = NULL;
+		char		   *agg_str = NULL;
+		int				nvecs;
+		StringInfoData sql;
+		StringInfoData subquery;
+		Datum		   *vec_datums = NULL;
+		bool		   *vec_nulls = NULL;
+		Oid				vec_elemtype;
+		int				i;
+		int				spi_ret;
+		int				proc;
+		ArrayType	   *ret_array = NULL;
+		Datum		   *datums = NULL;
+		bool		   *nulls = NULL;
+		NdbSpiSession  *session = NULL;
 
-		{
-			Vector	   *first_vec = (Vector *) DatumGetPointer(vec_datums[0]);
+		table_name = PG_GETARG_TEXT_PP(0);
+		query_vectors = PG_GETARG_ARRAYTYPE_P(1);
+		agg_method = PG_GETARG_TEXT_PP(2);
+		top_k = PG_GETARG_INT32(3);
+
+		tbl_str = text_to_cstring(table_name);
+		agg_str = text_to_cstring(agg_method);
+		vec_elemtype = ARR_ELEMTYPE(query_vectors);
 
-		if (first_vec->dim <= 0)
+		nvecs = ArrayGetNItems(
+							   ARR_NDIM(query_vectors), ARR_DIMS(query_vectors));
+		elog(DEBUG1,
+			 "neurondb: Multi-vector search on '%s' with %d queries, "
+			 "agg=%s, top_k=%d",
+			 tbl_str,
+			 nvecs,
+			 agg_str,
+			 top_k);
+		session = ndb_spi_session_begin(CurrentMemoryContext, false);
+		if (session == NULL)
+			ereport(ERROR, (errmsg("multi_vector_search: failed to begin SPI session")));
+
+		get_typlenbyvalalign(vec_elemtype, NULL, NULL, NULL);
+		deconstruct_array(query_vectors,
+						  vec_elemtype,
+						  -1,
+						  false,
+						  'i',
+						  &vec_datums,
+						  &vec_nulls,
+						  &nvecs);
+
+		if (nvecs < 1)
 		{
 			nfree(tbl_str);
 			nfree(agg_str);
-			nfree(vec_datums);
-			nfree(vec_nulls);
 			ndb_spi_session_end(&session);
 			ereport(ERROR,
-					(errmsg("query vectors must have positive "
-							"dimension")));
+					(errmsg("multi_vector_search: at least one query "
+							"vector required")));
 		}
-	}
 
-	initStringInfo(&subquery);
-	for (i = 0; i < nvecs; i++)
-	{
-		if (vec_nulls[i])
-			continue;
 		{
-			Vector	   *qv = (Vector *) DatumGetPointer(vec_datums[i]);
-			StringInfoData lit;
-			int			j;
+			Vector	   *first_vec = (Vector *) DatumGetPointer(vec_datums[0]);
 
-			initStringInfo(&lit);
-			appendStringInfoChar(&lit, '{');
-			for (j = 0; j < qv->dim; j++)
+			if (first_vec->dim <= 0)
 			{
-				if (j)
-					appendStringInfoChar(&lit, ',');
-				appendStringInfo(
-								 &lit, "%g", ((float *) &qv[1])[j]);
+				nfree(tbl_str);
+				nfree(agg_str);
+				nfree(vec_datums);
+				nfree(vec_nulls);
+				ndb_spi_session_end(&session);
+				ereport(ERROR,
+						(errmsg("query vectors must have positive "
+								"dimension")));
 			}
-			appendStringInfoChar(&lit, '}');
-			if (i)
-				appendStringInfoString(&subquery, ", ");
-			appendStringInfo(&subquery, "'%s'::vector", lit.data);
-			nfree(lit.data);
 		}
-	}
-	nfree(vec_datums);
-	nfree(vec_nulls);
 
-	initStringInfo(&sql);
-	appendStringInfo(&sql,
-					 "SELECT id FROM ("
-					 "  SELECT id, "
-					 "         GREATEST(%s) as max_score "
-					 "    FROM ("
-					 "      SELECT id, "
-					 "             (1 - (embedding <-> ANY(ARRAY[%s]))) as "
-					 "agg_score "
-					 "        FROM %s"
-					 "    ) _agg "
-					 " ) z "
-					 "ORDER BY max_score DESC LIMIT %d;",
-					 strcmp(agg_str, "max") == 0 ? "agg_score" : "avg(agg_score)",
-					 subquery.data,
-					 tbl_str,
-					 top_k);
+		initStringInfo(&subquery);
+		for (i = 0; i < nvecs; i++)
+		{
+			if (vec_nulls[i])
+				continue;
+			{
+				Vector	   *qv = (Vector *) DatumGetPointer(vec_datums[i]);
+				StringInfoData lit;
+				int			j;
 
-	spi_ret = ndb_spi_execute(session, sql.data, true, top_k);
-	if (spi_ret != SPI_OK_SELECT)
-	{
-		nfree(sql.data);
-		nfree(subquery.data);
-		nfree(tbl_str);
-		nfree(agg_str);
-		ndb_spi_session_end(&session);
-		ereport(ERROR,
-				(errmsg("Failed to execute multi_vector_search SQL")));
-	}
-	proc = SPI_processed;
-	if (proc == 0)
-	{
+				initStringInfo(&lit);
+				appendStringInfoChar(&lit, '{');
+				for (j = 0; j < qv->dim; j++)
+				{
+					if (j)
+						appendStringInfoChar(&lit, ',');
+					appendStringInfo(
+									 &lit, "%g", ((float *) &qv[1])[j]);
+				}
+				appendStringInfoChar(&lit, '}');
+				if (i)
+					appendStringInfoString(&subquery, ", ");
+				appendStringInfo(&subquery, "'%s'::vector", lit.data);
+				nfree(lit.data);
+			}
+		}
+		nfree(vec_datums);
+		nfree(vec_nulls);
+
+		initStringInfo(&sql);
+		appendStringInfo(&sql,
+						 "SELECT id FROM ("
+						 "  SELECT id, "
+						 "         GREATEST(%s) as max_score "
+						 "    FROM ("
+						 "      SELECT id, "
+						 "             (1 - (embedding <-> ANY(ARRAY[%s]))) as "
+						 "agg_score "
+						 "        FROM %s"
+						 "    ) _agg "
+						 " ) z "
+						 "ORDER BY max_score DESC LIMIT %d;",
+						 strcmp(agg_str, "max") == 0 ? "agg_score" : "avg(agg_score)",
+						 subquery.data,
+						 tbl_str,
+						 top_k);
+
+		spi_ret = ndb_spi_execute(session, sql.data, true, top_k);
+		if (spi_ret != SPI_OK_SELECT)
+		{
+			nfree(sql.data);
+			nfree(subquery.data);
+			nfree(tbl_str);
+			nfree(agg_str);
+			ndb_spi_session_end(&session);
+			ereport(ERROR,
+					(errmsg("Failed to execute multi_vector_search SQL")));
+		}
+		proc = SPI_processed;
+		if (proc == 0)
+		{
+			nfree(sql.data);
+			nfree(subquery.data);
+			nfree(tbl_str);
+			nfree(agg_str);
+			ndb_spi_session_end(&session);
+			ret_array = construct_empty_array(TEXTOID);
+			PG_RETURN_ARRAYTYPE_P(ret_array);
+		}
+		datums = NULL;
+		nulls = NULL;
+		nalloc(datums, Datum, proc);
+		NDB_CHECK_ALLOC(datums, "allocation");
+		nalloc(nulls, bool, proc);
+		NDB_CHECK_ALLOC(nulls, "allocation");
+		for (i = 0; i < proc; i++)
+		{
+			bool		isnull;
+			Datum		val = SPI_getbinval(SPI_tuptable->vals[i],
+											SPI_tuptable->tupdesc,
+											1,
+											&isnull);
+
+			if (!isnull)
+				datums[i] = PointerGetDatum(
+											cstring_to_text(DatumGetCString(val)));
+			nulls[i] = isnull;
+		}
+		ret_array = construct_array(datums, proc, TEXTOID, -1, false, 'i');
+		nfree(datums);
+		nfree(nulls);
 		nfree(sql.data);
 		nfree(subquery.data);
 		nfree(tbl_str);
 		nfree(agg_str);
 		ndb_spi_session_end(&session);
-		ret_array = construct_empty_array(TEXTOID);
-		PG_RETURN_ARRAYTYPE_P(ret_array);
-	}
-	datums = NULL;
-	nulls = NULL;
-	nalloc(datums, Datum, proc);
-	NDB_CHECK_ALLOC(datums, "allocation");
-	nalloc(nulls, bool, proc);
-	NDB_CHECK_ALLOC(nulls, "allocation");
-	for (i = 0; i < proc; i++)
-	{
-		bool		isnull;
-		Datum		val = SPI_getbinval(SPI_tuptable->vals[i],
-										SPI_tuptable->tupdesc,
-										1,
-										&isnull);
 
-		if (!isnull)
-			datums[i] = PointerGetDatum(
-										cstring_to_text(DatumGetCString(val)));
-		nulls[i] = isnull;
+		PG_RETURN_ARRAYTYPE_P(ret_array);
 	}
-	ret_array = construct_array(datums, proc, TEXTOID, -1, false, 'i');
-	nfree(datums);
-	nfree(nulls);
-	nfree(sql.data);
-	nfree(subquery.data);
-	nfree(tbl_str);
-	nfree(agg_str);
-	ndb_spi_session_end(&session);
-
-	PG_RETURN_ARRAYTYPE_P(ret_array);
 }
 
 PG_FUNCTION_INFO_V1(faceted_vector_search);
diff --git a/NeuronDB/src/search/sparse_search.c b/NeuronDB/src/search/sparse_search.c
index 2c120ba..6d8afb2 100644
--- a/NeuronDB/src/search/sparse_search.c
+++ b/NeuronDB/src/search/sparse_search.c
@@ -45,6 +45,11 @@ sparse_search(PG_FUNCTION_ARGS)
 	text	   *sparse_col;
 	Datum		query_vec;
 	int32		k;
+	ReturnSetInfo *rsinfo;
+	TupleDesc	tupdesc;
+	Tuplestorestate *tupstore = NULL;
+	MemoryContext per_query_ctx;
+	MemoryContext oldcontext;
 
 	/* Validate argument count */
 	if (PG_NARGS() < 4)
@@ -56,11 +61,7 @@ sparse_search(PG_FUNCTION_ARGS)
 	sparse_col = PG_GETARG_TEXT_PP(1);
 	query_vec = PG_GETARG_DATUM(2);
 	k = PG_GETARG_INT32(3);
-	ReturnSetInfo *rsinfo = (ReturnSetInfo *) fcinfo->resultinfo;
-	TupleDesc	tupdesc;
-	Tuplestorestate *tupstore = NULL;
-	MemoryContext per_query_ctx;
-	MemoryContext oldcontext;
+	rsinfo = (ReturnSetInfo *) fcinfo->resultinfo;
 
 	NdbSpiSession *session = NULL;
 	StringInfoData sql;
@@ -168,6 +169,7 @@ Datum
 splade_embed(PG_FUNCTION_ARGS)
 {
 	text	   *input_text;
+	char	   *input_str;
 
 	/* Validate argument count */
 	if (PG_NARGS() < 1)
@@ -176,7 +178,7 @@ splade_embed(PG_FUNCTION_ARGS)
 				 errmsg("neurondb: splade_embed requires at least 1 argument")));
 
 	input_text = PG_GETARG_TEXT_PP(0);
-	char	   *input_str = text_to_cstring(input_text);
+	input_str = text_to_cstring(input_text);
 #ifdef HAVE_ONNX_RUNTIME
 	ONNXModelSession *session = NULL;
 	ONNXTensor *input_tensor = NULL;
@@ -214,7 +216,8 @@ splade_embed(PG_FUNCTION_ARGS)
 				 errmsg("splade_embed: tokenization failed")));
 	}
 
-	input_tensor = (ONNXTensor *) palloc0(sizeof(ONNXTensor));
+		nalloc(input_tensor, ONNXTensor, 1);
+		MemSet(input_tensor, 0, sizeof(ONNXTensor));
 	input_tensor->ndim = 2;
 	nalloc(shape, int64, 2);
 	input_tensor->shape = shape;
@@ -294,6 +297,7 @@ Datum
 colbertv2_embed(PG_FUNCTION_ARGS)
 {
 	text	   *input_text;
+	char	   *input_str;
 
 	/* Validate argument count */
 	if (PG_NARGS() < 1)
@@ -302,7 +306,7 @@ colbertv2_embed(PG_FUNCTION_ARGS)
 				 errmsg("neurondb: colbertv2_embed requires at least 1 argument")));
 
 	input_text = PG_GETARG_TEXT_PP(0);
-	char	   *input_str = text_to_cstring(input_text);
+	input_str = text_to_cstring(input_text);
 #ifdef HAVE_ONNX_RUNTIME
 	ONNXModelSession *session = NULL;
 	ONNXTensor *input_tensor = NULL;
@@ -340,7 +344,8 @@ colbertv2_embed(PG_FUNCTION_ARGS)
 				 errmsg("colbertv2_embed: tokenization failed")));
 	}
 
-	input_tensor = (ONNXTensor *) palloc0(sizeof(ONNXTensor));
+		nalloc(input_tensor, ONNXTensor, 1);
+		MemSet(input_tensor, 0, sizeof(ONNXTensor));
 	input_tensor->ndim = 2;
 	nalloc(shape, int64, 2);
 	input_tensor->shape = shape;
@@ -380,7 +385,8 @@ colbertv2_embed(PG_FUNCTION_ARGS)
 
 		tokens = (int) output_tensor->shape[1];
 		output_dim = (int) output_tensor->shape[2];
-		max_pooled = (float *) palloc0(sizeof(float) * output_dim);
+		nalloc(max_pooled, float, output_dim);
+		MemSet(max_pooled, 0, sizeof(float) * output_dim);
 
 		for (i = 0; i < tokens; i++)
 		{
@@ -540,19 +546,8 @@ bm25_score(PG_FUNCTION_ARGS)
 	text	   *doc_text;
 	float8		k1;
 	float8		b;
-
-	/* Validate argument count */
-	if (PG_NARGS() < 2 || PG_NARGS() > 4)
-		ereport(ERROR,
-				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
-				 errmsg("neurondb: bm25_score requires 2 to 4 arguments")));
-
-	query_text = PG_GETARG_TEXT_PP(0);
-	doc_text = PG_GETARG_TEXT_PP(1);
-	k1 = PG_ARGISNULL(2) ? 1.2 : PG_GETARG_FLOAT8(2);
-	b = PG_ARGISNULL(3) ? 0.75 : PG_GETARG_FLOAT8(3);
-	char	   *query_str = text_to_cstring(query_text);
-	char	   *doc_str = text_to_cstring(doc_text);
+	char	   *query_str;
+	char	   *doc_str;
 	char	  **query_tokens = NULL;
 	char	  **doc_tokens = NULL;
 	char	  **query_unique = NULL;
@@ -566,6 +561,19 @@ bm25_score(PG_FUNCTION_ARGS)
 	int			i,
 				j;
 	double		doc_length;
+
+	/* Validate argument count */
+	if (PG_NARGS() < 2 || PG_NARGS() > 4)
+		ereport(ERROR,
+				(errcode(ERRCODE_INVALID_PARAMETER_VALUE),
+				 errmsg("neurondb: bm25_score requires 2 to 4 arguments")));
+
+	query_text = PG_GETARG_TEXT_PP(0);
+	doc_text = PG_GETARG_TEXT_PP(1);
+	k1 = PG_ARGISNULL(2) ? 1.2 : PG_GETARG_FLOAT8(2);
+	b = PG_ARGISNULL(3) ? 0.75 : PG_GETARG_FLOAT8(3);
+	query_str = text_to_cstring(query_text);
+	doc_str = text_to_cstring(doc_text);
 	double		avg_doc_length = 100.0;
 	double		N = 1000.0;
 	int			n_qi;
@@ -595,8 +603,10 @@ bm25_score(PG_FUNCTION_ARGS)
 	doc_length = (double) num_doc_tokens;
 
 	nalloc(query_unique, char *, num_query_tokens);
-	query_counts = (int *) palloc0(sizeof(int) * num_query_tokens);
-	doc_counts = (int *) palloc0(sizeof(int) * num_doc_tokens);
+	nalloc(query_counts, int, num_query_tokens);
+	MemSet(query_counts, 0, sizeof(int) * num_query_tokens);
+	nalloc(doc_counts, int, num_doc_tokens);
+	MemSet(doc_counts, 0, sizeof(int) * num_doc_tokens);
 	bm25_count_tf(query_tokens, num_query_tokens, query_counts, query_unique, &num_query_unique);
 
 	for (i = 0; i < num_query_unique; i++)
diff --git a/NeuronDB/src/search/temporal_integration.c b/NeuronDB/src/search/temporal_integration.c
index 8bb5a73..350392a 100644
--- a/NeuronDB/src/search/temporal_integration.c
+++ b/NeuronDB/src/search/temporal_integration.c
@@ -150,7 +150,7 @@ temporal_rerank_results(ItemPointer * items,
 	if (!config->enabled || count == 0)
 		return;
 
-	scores = (float4 *) palloc(count * sizeof(float4));
+	nalloc(scores, float4, count);
 	NDB_CHECK_ALLOC(scores, "scores");
 
 	for (i = 0; i < count; i++)
@@ -188,16 +188,10 @@ temporal_rerank_results(ItemPointer * items,
 	}
 	else
 	{
-		struct
-		{
-			ItemPointer item;
-			float4		distance;
-			TimestampTz timestamp;
-			float4		score;
-		}		   *sort_items;
+		TemporalItem *sort_items;
 		int			idx;
 
-		sort_items = palloc(sizeof(*sort_items) * count);
+		nalloc(sort_items, TemporalItem, count);
 		NDB_CHECK_ALLOC(sort_items, "allocation");
 		for (idx = 0; idx < count; idx++)
 		{
@@ -273,7 +267,8 @@ temporal_create_config(float4 decayRate, float4 recencyWeight)
 {
 	TemporalConfig *config = NULL;
 
-	config = (TemporalConfig *) palloc0(sizeof(TemporalConfig));
+	nalloc(config, TemporalConfig, 1);
+	MemSet(config, 0, sizeof(TemporalConfig));
 	NDB_CHECK_ALLOC(config, "config");
 	config->decayRate = decayRate;
 	config->recencyWeight = recencyWeight;
@@ -303,7 +298,7 @@ temporal_integrate_hnsw_search(Relation heapRel,
 	if (resultCount == 0 || items == NULL)
 		return;
 
-	timestamps = (TimestampTz *) palloc(resultCount * sizeof(TimestampTz));
+	nalloc(timestamps, TimestampTz, resultCount);
 	NDB_CHECK_ALLOC(timestamps, "timestamps");
 
 	snapshot = GetActiveSnapshot();
diff --git a/NeuronDB/src/tenant/multi_tenant.c b/NeuronDB/src/tenant/multi_tenant.c
index bd20e60..3368cb1 100644
--- a/NeuronDB/src/tenant/multi_tenant.c
+++ b/NeuronDB/src/tenant/multi_tenant.c
@@ -57,7 +57,7 @@ ndb_text_to_cstring_safe(text * t)
 				 errmsg("empty text argument")));
 
 	/* Allocate a string with NUL terminator */
-	result = (char *) palloc(len + 1);
+	nalloc(result, char, len + 1);
 	memcpy(result, VARDATA_ANY(t), len);
 	result[len] = '\0';
 
@@ -76,6 +76,9 @@ create_tenant_worker(PG_FUNCTION_ARGS)
 	text	   *tenant_id;
 	text	   *worker_type;
 	text	   *config;
+	char	   *tid_str = NULL;
+	char	   *type_str = NULL;
+	int32		worker_id = 0;
 
 	/* Validate argument count */
 	if (PG_NARGS() < 3)
@@ -86,9 +89,6 @@ create_tenant_worker(PG_FUNCTION_ARGS)
 	tenant_id = PG_GETARG_TEXT_PP(0);
 	worker_type = PG_GETARG_TEXT_PP(1);
 	config = PG_GETARG_TEXT_PP(2);
-	char	   *tid_str = NULL;
-	char	   *type_str = NULL;
-	int32		worker_id = 0;
 
 	/* Defensive argument checks */
 	if (tenant_id == NULL || worker_type == NULL || config == NULL)
@@ -214,6 +214,10 @@ create_policy(PG_FUNCTION_ARGS)
 {
 	text	   *policy_name;
 	text	   *policy_rule;
+	char	   *name_str = NULL;
+	char	   *rule_str = NULL;
+	volatile bool success = false;
+	NdbSpiSession *session = NULL;
 
 	/* Validate argument count */
 	if (PG_NARGS() < 2)
@@ -223,11 +227,6 @@ create_policy(PG_FUNCTION_ARGS)
 
 	policy_name = PG_GETARG_TEXT_PP(0);
 	policy_rule = PG_GETARG_TEXT_PP(1);
-	char	   *name_str = NULL;
-	char	   *rule_str = NULL;
-	volatile bool success = false;
-
-	NdbSpiSession *session = NULL;
 
 	if (policy_name == NULL || policy_rule == NULL)
 		ereport(ERROR,
@@ -340,7 +339,7 @@ compute_hmac_sha256(const char *key, const char *data)
 	}
 
 	/* Convert to hex string */
-	hex_hmac = (char *) palloc(SHA256_DIGEST_LENGTH * 2 + 1);
+	nalloc(hex_hmac, char, SHA256_DIGEST_LENGTH * 2 + 1);
 	for (i = 0; i < SHA256_DIGEST_LENGTH; i++)
 	{
 		sprintf(hex_hmac + (i * 2), "%02x", hmac[i]);
@@ -357,6 +356,14 @@ audit_log_query(PG_FUNCTION_ARGS)
 	text	   *query_text;
 	text	   *user_id;
 	Vector	   *result_vectors;
+	char	   *query_str = NULL;
+	char	   *user_str = NULL;
+	volatile	uint32 vector_hash = 0;
+	volatile bool success = false;
+	char *hmac_hex = NULL;
+	char *hmac_key = NULL;
+	StringInfoData hmac_data;
+	NdbSpiSession *session = NULL;
 
 	/* Validate argument count */
 	if (PG_NARGS() < 3)
@@ -367,15 +374,6 @@ audit_log_query(PG_FUNCTION_ARGS)
 	query_text = PG_GETARG_TEXT_PP(0);
 	user_id = PG_GETARG_TEXT_PP(1);
 	result_vectors = (Vector *) PG_GETARG_POINTER(2);
-	char	   *query_str = NULL;
-	char	   *user_str = NULL;
-	volatile	uint32 vector_hash = 0;
-	volatile bool success = false;
-	char *hmac_hex = NULL;
-	char *hmac_key = NULL;
-	StringInfoData hmac_data;
-
-	NdbSpiSession *session = NULL;
 
 	if (query_text == NULL || user_id == NULL || result_vectors == NULL)
 		ereport(ERROR,
diff --git a/NeuronDB/src/types/quantization.c b/NeuronDB/src/types/quantization.c
index 28e58a0..e6e5a77 100644
--- a/NeuronDB/src/types/quantization.c
+++ b/NeuronDB/src/types/quantization.c
@@ -1727,7 +1727,7 @@ halfvec_in(PG_FUNCTION_ARGS)
 	if (*ptr == '[')
 		ptr++;
 
-	temp_data = (float4 *) palloc(sizeof(float4) * capacity);
+	nalloc(temp_data, float4, capacity);
 
 	/* Parse comma-separated floats */
 	while (*ptr && *ptr != ']')
diff --git a/NeuronDB/src/types/sparse_vector_types.c b/NeuronDB/src/types/sparse_vector_types.c
index 0c558c9..3166ff4 100644
--- a/NeuronDB/src/types/sparse_vector_types.c
+++ b/NeuronDB/src/types/sparse_vector_types.c
@@ -55,7 +55,7 @@ sparse_vector_in(PG_FUNCTION_ARGS)
 			ereport(ERROR,
 					(errcode(ERRCODE_INVALID_TEXT_REPRESENTATION),
 					 errmsg("sparse_vector parsing failed: %s", errstr)));
-			pfree(errstr);
+			nfree(errstr);
 		}
 		else
 		{
diff --git a/NeuronDB/src/util/neurondb_json.c b/NeuronDB/src/util/neurondb_json.c
index f748a8b..706e49f 100644
--- a/NeuronDB/src/util/neurondb_json.c
+++ b/NeuronDB/src/util/neurondb_json.c
@@ -123,49 +123,32 @@ ndb_jsonb_init_oids(void)
 Jsonb *
 ndb_jsonb_in(text * json_text)
 {
-	char *cstr = NULL;
-	Datum		result_datum;
-
-	Jsonb *result = NULL;
+	char	   *cstr;
+	Datum		d;
+	Jsonb	   *result;
 
 	if (json_text == NULL)
 		return NULL;
 
-	if (!jsonb_oids_initialized)
-		ndb_jsonb_init_oids();
-
 	cstr = text_to_cstring(json_text);
+	d = DirectFunctionCall1(jsonb_in, CStringGetDatum(cstr));
+	pfree(cstr);
+	result = DatumGetJsonbP(d);
 
-	PG_TRY();
-	{
-		result_datum = DirectFunctionCall1(jsonb_in, CStringGetDatum(cstr));
-		result = DatumGetJsonbP(result_datum);
-	}
-	PG_CATCH();
-	{
-		FlushErrorState();
-		nfree(cstr);
-		return NULL;
-	}
-	PG_END_TRY();
-
-	nfree(cstr);
 	return result;
 }
 
 Jsonb *
 ndb_jsonb_in_cstring(const char *json_str)
 {
-	text *json_text = NULL;
-
-	Jsonb *result = NULL;
+	Datum		d;
+	Jsonb	   *result;
 
 	if (json_str == NULL)
 		return NULL;
 
-	json_text = cstring_to_text(json_str);
-	result = ndb_jsonb_in(json_text);
-	nfree(json_text);
+	d = DirectFunctionCall1(jsonb_in, CStringGetDatum((char *) json_str));
+	result = DatumGetJsonbP(d);
 
 	return result;
 }
@@ -2507,8 +2490,7 @@ ndb_json_parse_openai_embedding(const char *json_str,
 				{
 					if (vec != NULL)
 					{
-						pfree((void *) vec);
-						vec = NULL;
+						nfree(vec);
 					}
 				}
 			}
diff --git a/NeuronDB/src/util/neurondb_spi_safe.c b/NeuronDB/src/util/neurondb_spi_safe.c
index 8af3f58..ae8d3a2 100644
--- a/NeuronDB/src/util/neurondb_spi_safe.c
+++ b/NeuronDB/src/util/neurondb_spi_safe.c
@@ -357,9 +357,9 @@ ndb_spi_exec_select_one_row_safe(const char *query,
 	if (out_datum != NULL || out_isnull != NULL)
 	{
 		if (out_datum != NULL)
-			*out_datum = (Datum *) palloc(sizeof(Datum) * natts);
+			nalloc(*out_datum, Datum, natts);
 		if (out_isnull != NULL)
-			*out_isnull = (bool *) palloc(sizeof(bool) * natts);
+			nalloc(*out_isnull, bool, natts);
 
 		for (i = 0; i < natts; i++)
 		{
diff --git a/NeuronDB/src/vector/vector_quantization.c b/NeuronDB/src/vector/vector_quantization.c
index b1936a1..bb93461 100644
--- a/NeuronDB/src/vector/vector_quantization.c
+++ b/NeuronDB/src/vector/vector_quantization.c
@@ -93,6 +93,7 @@ vector_quantize_fp16(PG_FUNCTION_ARGS)
 	Vector *vec = NULL;
 	bytea *result = NULL;
 	uint16_t *fp16_data = NULL;
+	char *tmp = NULL;
 	int			i;
 	size_t		size;
 
@@ -122,7 +123,8 @@ vector_quantize_fp16(PG_FUNCTION_ARGS)
 				(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),
 				 errmsg("vector size exceeds maximum allocation")));
 
-	result = (bytea *) palloc(VARHDRSZ + size);
+	nalloc(tmp, char, VARHDRSZ + size);
+	result = (bytea *) tmp;
 	if (result == NULL)
 		ereport(ERROR,
 				(errcode(ERRCODE_OUT_OF_MEMORY),
@@ -209,6 +211,7 @@ vector_quantize_int8(PG_FUNCTION_ARGS)
 	Vector *max_vec = NULL;
 	bytea *result = NULL;
 	int8 *int8_data = NULL;
+	char *tmp = NULL;
 	int			i;
 	size_t		size;
 	float		scale;
@@ -251,7 +254,8 @@ vector_quantize_int8(PG_FUNCTION_ARGS)
 				(errcode(ERRCODE_PROGRAM_LIMIT_EXCEEDED),
 				 errmsg("vector size exceeds maximum allocation")));
 
-	result = (bytea *) palloc(VARHDRSZ + size);
+	nalloc(tmp, char, VARHDRSZ + size);
+	result = (bytea *) tmp;
 	if (result == NULL)
 		ereport(ERROR,
 				(errcode(ERRCODE_OUT_OF_MEMORY),
@@ -512,7 +516,12 @@ vector_quantize_binary(PG_FUNCTION_ARGS)
 	 * proper cleanup
 	 */
 	/* Note: For variable-length PostgreSQL types, palloc0 is standard */
-	result = (BinaryVec *) palloc0(size);
+		{
+			char *tmp = NULL;
+			nalloc(tmp, char, size);
+			result = (BinaryVec *) tmp;
+			MemSet(result, 0, size);
+		}
 	if (result == NULL)
 		ereport(ERROR,
 				(errcode(ERRCODE_OUT_OF_MEMORY),
diff --git a/NeuronDB/src/worker/worker_init.c b/NeuronDB/src/worker/worker_init.c
index 338fd94..e79cfeb 100644
--- a/NeuronDB/src/worker/worker_init.c
+++ b/NeuronDB/src/worker/worker_init.c
@@ -93,17 +93,15 @@ _PG_init(void)
 		 relopt_kind_hnsw, relopt_kind_ivf);
 
 	/* Register all HNSW options - PostgreSQL requires registration for recognition */
-	/* CRITICAL: Registration order MUST match struct field order in HnswOptions! */
-	/* PostgreSQL assigns defaults by registration order, not by name matching */
 	add_int_reloption(relopt_kind_hnsw, "m",
 					  "Maximum number of connections per node",
-					  16, 2, 128, AccessExclusiveLock);
+					  16, 4, 200, AccessExclusiveLock);
 	add_int_reloption(relopt_kind_hnsw, "ef_construction",
 					  "Size of dynamic candidate list during construction",
-					  200, 10, 10000, AccessExclusiveLock);
+					  200, 10, 1000, AccessExclusiveLock);
 	add_int_reloption(relopt_kind_hnsw, "ef_search",
 					  "Size of dynamic candidate list during search",
-					  64, 10, 10000, AccessExclusiveLock);
+					  64, 10, 1000, AccessExclusiveLock);
 
 	add_int_reloption(relopt_kind_ivf, "lists",
 					  "Number of inverted lists",
diff --git a/NeuronDB/src/worker/worker_llm.c b/NeuronDB/src/worker/worker_llm.c
index 800b6dc..505ad68 100644
--- a/NeuronDB/src/worker/worker_llm.c
+++ b/NeuronDB/src/worker/worker_llm.c
@@ -175,13 +175,11 @@ neuranllm_main(Datum main_arg)
 
 				if (job_type != NULL)
 				{
-					pfree((void *) job_type);
-					job_type = NULL;
+					nfree(job_type);
 				}
 				if (payload != NULL)
 				{
-					pfree((void *) payload);
-					payload = NULL;
+					nfree(payload);
 				}
 			}
 
diff --git a/NeuronDB/t/007_gpu_comprehensive.t b/NeuronDB/t/007_gpu_comprehensive.t
index 0bf0bf4..69c5a92 100755
--- a/NeuronDB/t/007_gpu_comprehensive.t
+++ b/NeuronDB/t/007_gpu_comprehensive.t
@@ -54,14 +54,14 @@ subtest 'GPU Info - Positive' => sub {
 	
 	# Check GPU enabled setting
 	$result = $node->psql('postgres', q{
-		SELECT current_setting('neurondb.gpu_enabled', true);
+		SELECT current_setting('neurondb.compute_mode', true);
 	}, tuples_only => 1);
 	ok($result->{success}, 'gpu_enabled setting accessible');
 	
 	# Try to enable GPU
 	$result = $node->psql('postgres', q{
-		SET neurondb.gpu_enabled = 'on';
-		SELECT current_setting('neurondb.gpu_enabled', true);
+		SET neurondb.compute_mode = 'on';
+		SELECT current_setting('neurondb.compute_mode', true);
 	}, tuples_only => 1);
 	ok($result->{success}, 'gpu_enabled can be set');
 };
@@ -74,7 +74,7 @@ subtest 'GPU Distance Functions - Positive' => sub {
 	plan tests => 15;
 	
 	# Enable GPU if possible
-	$node->psql('postgres', q{SET neurondb.gpu_enabled = 'on';});
+	$node->psql('postgres', q{SET neurondb.compute_mode = 'on';});
 	
 	# GPU L2 distance
 	my $result = $node->psql('postgres', q{
@@ -145,7 +145,7 @@ subtest 'GPU Distance Functions - Negative' => sub {
 	plan tests => 10;
 	
 	# Disable GPU
-	$node->psql('postgres', q{SET neurondb.gpu_enabled = 'off';});
+	$node->psql('postgres', q{SET neurondb.compute_mode = 'off';});
 	
 	# GPU functions should fail or return error when GPU disabled
 	my $result = $node->psql('postgres', q{
@@ -183,7 +183,7 @@ subtest 'GPU Settings - Positive' => sub {
 	
 	# Test all GPU-related settings
 	my @gpu_settings = qw(
-		neurondb.gpu_enabled
+		neurondb.compute_mode
 		neurondb.gpu_kernels
 		neurondb.gpu_memory_limit
 		neurondb.gpu_device_id
@@ -199,15 +199,15 @@ subtest 'GPU Settings - Positive' => sub {
 	
 	# Try to set GPU enabled
 	my $result = $node->psql('postgres', q{
-		SET neurondb.gpu_enabled = 'on';
-		SELECT current_setting('neurondb.gpu_enabled', true);
+		SET neurondb.compute_mode = 'on';
+		SELECT current_setting('neurondb.compute_mode', true);
 	}, tuples_only => 1);
 	ok($result->{success}, 'gpu_enabled can be set to on');
 	
 	# Try to set GPU disabled
 	$result = $node->psql('postgres', q{
-		SET neurondb.gpu_enabled = 'off';
-		SELECT current_setting('neurondb.gpu_enabled', true);
+		SET neurondb.compute_mode = 'off';
+		SELECT current_setting('neurondb.compute_mode', true);
 	}, tuples_only => 1);
 	ok($result->{success}, 'gpu_enabled can be set to off');
 };
@@ -221,7 +221,7 @@ subtest 'GPU Settings - Negative' => sub {
 	
 	# Invalid GPU enabled value
 	my $result = $node->psql('postgres', q{
-		SET neurondb.gpu_enabled = 'invalid';
+		SET neurondb.compute_mode = 'invalid';
 	});
 	ok(!$result->{success}, 'invalid gpu_enabled value rejected');
 	
diff --git a/NeuronDB/t/NeuronDB.pm b/NeuronDB/t/NeuronDB.pm
index 0f2d5d3..b561f66 100644
--- a/NeuronDB/t/NeuronDB.pm
+++ b/NeuronDB/t/NeuronDB.pm
@@ -384,18 +384,18 @@ sub test_gpu_features {
 	
 	# Check if GPU is enabled
 	my $result = $node->psql($dbname, q{
-		SELECT current_setting('neurondb.gpu_enabled', true) AS gpu_enabled;
+		SELECT current_setting('neurondb.compute_mode', true) AS compute_mode;
 	}, tuples_only => 1);
 	
 	unless ($result->{success}) {
 		return (0, "Cannot check GPU status: $result->{stderr}");
 	}
 	
-	my $gpu_enabled = $result->{stdout};
-	chomp $gpu_enabled;
+	my $compute_mode = $result->{stdout};
+	chomp $compute_mode;
 	
-	if ($gpu_enabled ne 'on') {
-		return (1, "GPU not enabled (skipped)");
+	if ($compute_mode ne '1') {  # 1 = GPU mode, 0 = CPU mode, 2 = AUTO mode
+		return (1, "GPU not enabled (compute_mode=$compute_mode, expected 1 for GPU mode)");
 	}
 	
 	# Test GPU info function
diff --git a/NeuronDB/tests/analyze_core_dumps.sh b/NeuronDB/tests/analyze_core_dumps.sh
index 341671a..c6e6d02 100755
--- a/NeuronDB/tests/analyze_core_dumps.sh
+++ b/NeuronDB/tests/analyze_core_dumps.sh
@@ -75,3 +75,4 @@ echo ""
 echo "Full analysis files saved in: $OUTPUT_DIR"
 echo "To view a specific analysis: cat $OUTPUT_DIR/<core_file>.txt"
 
+
diff --git a/NeuronDB/tests/run_svm_test_with_core.sh b/NeuronDB/tests/run_svm_test_with_core.sh
index 0a5e574..1500b8a 100755
--- a/NeuronDB/tests/run_svm_test_with_core.sh
+++ b/NeuronDB/tests/run_svm_test_with_core.sh
@@ -51,3 +51,4 @@ fi
 
 exit $EXIT_CODE
 
+
diff --git a/NeuronDB/tests/run_test.py b/NeuronDB/tests/run_test.py
index 3e42277..9274cc6 100755
--- a/NeuronDB/tests/run_test.py
+++ b/NeuronDB/tests/run_test.py
@@ -220,6 +220,8 @@ def run_psql_file(dbname: str, sql_file: str, psql_path: str, verbose: bool = Fa
 		stdout=subprocess.PIPE,
 		stderr=subprocess.PIPE,
 		text=True,
+		encoding='utf-8',
+		errors='replace',  # Replace invalid UTF-8 bytes with replacement character
 		env=os.environ.copy(),
 	)
 	out, err = proc.communicate()
@@ -702,7 +704,7 @@ def get_gpu_info(dbname: str, psql_path: str, host: Optional[str] = None, port:
 	
 	try:
 		cmd = [psql_path, "-d", dbname, "-t", "-A", "-w", "-c", 
-		       "SELECT current_setting('neurondb.gpu_enabled', true), current_setting('neurondb.gpu_device', true), current_setting('neurondb.gpu_kernels', true);"]
+		       "SELECT current_setting('neurondb.compute_mode', true), current_setting('neurondb.gpu_device', true), current_setting('neurondb.gpu_kernels', true);"]
 		proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env, timeout=5)
 		out, err = proc.communicate()
 		if proc.returncode == 0 and out.strip():
@@ -1141,33 +1143,8 @@ def switch_gpu_mode(dbname: str, compute_mode: str, psql_path: str, gpu_kernels:
 		if not success_k:
 			print(f"Warning: Failed to set GPU kernels: {err_k}", file=sys.stderr)
 
-	# Reload configuration
-	cmd2 = "SELECT pg_reload_conf();"
-	success2, out2, err2 = run_psql_command(dbname, cmd2, psql_path, verbose)
-	if not success2:
-		print(f"Failed to reload configuration: {err2}", file=sys.stderr)
-		return False
-
-	# Initialize GPU if GPU or auto mode is enabled
-	if compute_mode in ("gpu", "auto"):
-		cmd3 = "SELECT neurondb_gpu_enable();"
-		success3, out3, err3 = run_psql_command(dbname, cmd3, psql_path, verbose)
-		if not success3:
-			if compute_mode == "gpu":
-				print(f"Warning: Failed to enable GPU: {err3}", file=sys.stderr)
-				# In GPU mode, this is a warning but we continue (let the mode handle errors)
-			else:
-				print(f"Warning: Failed to enable GPU (auto mode will fallback to CPU): {err3}", file=sys.stderr)
-		
-		# Force GPU initialization by querying GPU info
-		cmd4 = "SELECT * FROM neurondb_gpu_info() LIMIT 1;"
-		success4, out4, err4 = run_psql_command(dbname, cmd4, psql_path, verbose)
-		if not success4:
-			if verbose:
-				if compute_mode == "gpu":
-					print(f"Warning: GPU info query failed (GPU may not be available): {err4}", file=sys.stderr)
-				else:
-					print(f"Info: GPU info query failed (auto mode will fallback to CPU): {err4}", file=sys.stderr)
+	# Note: We don't reload config or initialize GPU here - PostgreSQL will be restarted
+	# to apply ALTER SYSTEM changes, and GPU will be initialized after restart in the main flow
 
 	if verbose:
 		print(f"Switched to {compute_mode.upper()} mode successfully.")
@@ -2658,7 +2635,7 @@ def main() -> int:
 		print(f"Failed to connect to PostgreSQL at {conn_info}. Aborting.", file=sys.stderr)
 		return 1
 	
-	# 2. Switch compute mode (using ALTER SYSTEM)
+	# 2. Switch compute mode (using ALTER SYSTEM) - sets all GUCs
 	when = datetime.now()
 	mode_start = time.perf_counter()
 	mode_ok = switch_gpu_mode(args.db, args.compute, args.psql, args.gpu_kernels, args.verbose)
@@ -2668,6 +2645,123 @@ def main() -> int:
 		print(f"Failed to switch to {args.compute.upper()} compute mode. Aborting.", file=sys.stderr)
 		return 1
 	
+	# 2.5. Restart PostgreSQL to apply ALTER SYSTEM changes
+	when = datetime.now()
+	restart_start = time.perf_counter()
+	restart_ok, restart_msg = restart_postgresql(args.db, args.psql, args.host, args.port, args.verbose)
+	restart_elapsed = time.perf_counter() - restart_start
+	if restart_ok:
+		print(format_status_line(True, when, f"Restarting PostgreSQL to apply GUC changes...", restart_elapsed))
+		# Wait a bit for PostgreSQL to be fully ready
+		time.sleep(2)
+		# Verify connection after restart
+		conn_ok, _, _ = check_postgresql_connection(args.db, args.psql, args.host, args.port)
+		if not conn_ok:
+			print(f"Warning: PostgreSQL restarted but connection verification failed. Waiting 5 more seconds...", file=sys.stderr)
+			time.sleep(5)
+			conn_ok, _, _ = check_postgresql_connection(args.db, args.psql, args.host, args.port)
+			if not conn_ok:
+				print(f"Failed to connect to PostgreSQL after restart. Aborting.", file=sys.stderr)
+				return 1
+		
+		# After restart, initialize GPU in the session (for GPU mode)
+		if args.compute in ("gpu", "auto"):
+			when = datetime.now()
+			gpu_init_start = time.perf_counter()
+			mode_enum = {"cpu": 0, "gpu": 1, "auto": 2}.get(args.compute, 2)
+			
+			# Set compute_mode in current session
+			cmd_set_mode = f"SET neurondb.compute_mode = {mode_enum};"
+			success_set, out_set, err_set = run_psql_command(args.db, cmd_set_mode, args.psql, args.verbose)
+			if not success_set:
+				print(f"ERROR: Failed to set compute_mode in session after restart: {err_set}", file=sys.stderr)
+				if args.compute == "gpu":
+					return 1
+			
+			# Set GPU kernels in session
+			if args.compute in ("gpu", "auto"):
+				default_kernels = "l2,cosine,ip,rf_split,rf_predict"
+				ml_kernels = "linreg_train,linreg_predict,lr_train,lr_predict,rf_train,svm_train,svm_predict,ridge_train,ridge_predict,lasso_train,lasso_predict,dt_train,dt_predict,nb_train,nb_predict"
+				full_kernels = f"{default_kernels},{ml_kernels}"
+				cmd_set_kernels = f"SET neurondb.gpu_kernels = '{full_kernels}';"
+				success_kernels, out_kernels, err_kernels = run_psql_command(args.db, cmd_set_kernels, args.psql, args.verbose)
+				if not success_kernels:
+					print(f"Warning: Failed to set GPU kernels in session: {err_kernels}", file=sys.stderr)
+			
+			# Set up environment for psql commands
+			env = os.environ.copy()
+			if args.host:
+				env["PGHOST"] = args.host
+			if args.port:
+				env["PGPORT"] = str(args.port)
+			
+			# Verify compute_mode is set correctly before enabling GPU (use -t flag for clean output)
+			cmd_check_mode = [args.psql, "-d", args.db, "-t", "-A", "-c", "SELECT current_setting('neurondb.compute_mode');"]
+			try:
+				proc = subprocess.Popen(cmd_check_mode, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env)
+				out_check, err_check = proc.communicate()
+				success_check = (proc.returncode == 0)
+				if success_check:
+					computed_mode_val = out_check.strip()
+					if computed_mode_val != str(mode_enum):
+						print(f"WARNING: neurondb.compute_mode = {computed_mode_val}, expected {mode_enum}", file=sys.stderr)
+			except Exception:
+				pass
+			
+			# Wait a bit more after restart to ensure PostgreSQL is fully ready
+			time.sleep(1)
+			
+			# Enable GPU - this should initialize GPU and return true if successful
+			# Use -t flag to get just the value without headers
+			cmd_enable = [args.psql, "-d", args.db, "-t", "-A", "-c", "SELECT neurondb_gpu_enable();"]
+			try:
+				proc = subprocess.Popen(cmd_enable, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env)
+				out_enable, err_enable = proc.communicate()
+				success_enable = (proc.returncode == 0)
+			except Exception as e:
+				success_enable = False
+				out_enable = ""
+				err_enable = str(e)
+			
+			if not success_enable:
+				if args.compute == "gpu":
+					print(f"ERROR: Failed to enable GPU after restart: {err_enable}", file=sys.stderr)
+					print(f"GPU mode requires GPU to be available. Aborting.", file=sys.stderr)
+					return 1
+				else:
+					print(f"Warning: Failed to enable GPU (auto mode will fallback to CPU): {err_enable}", file=sys.stderr)
+			else:
+				# Extract just the value (remove whitespace, newlines, etc.)
+				enable_result = out_enable.strip().lower()
+				# Verify GPU is actually available if GPU mode is required
+				if args.compute == "gpu":
+					if enable_result != "t":
+						print(f"ERROR: GPU mode enabled but neurondb_gpu_enable() returned '{enable_result}' (expected 't').", file=sys.stderr)
+						print(f"GPU mode requires GPU to be available. Aborting.", file=sys.stderr)
+						return 1
+					# Verify GPU info is accessible (use -t flag for clean output)
+					cmd_info = [args.psql, "-d", args.db, "-t", "-A", "-c", "SELECT COUNT(*) FROM neurondb_gpu_info() WHERE is_available = true;"]
+					try:
+						proc = subprocess.Popen(cmd_info, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env)
+						out_info, err_info = proc.communicate()
+						success_info = (proc.returncode == 0)
+						if not success_info or out_info.strip() == "0":
+							print(f"ERROR: GPU mode enabled but GPU info query failed or no GPU available: {err_info}", file=sys.stderr)
+							print(f"GPU mode requires GPU to be available. Aborting.", file=sys.stderr)
+							return 1
+					except Exception as e:
+						print(f"ERROR: GPU mode enabled but GPU info query exception: {e}", file=sys.stderr)
+						print(f"GPU mode requires GPU to be available. Aborting.", file=sys.stderr)
+						return 1
+			
+			gpu_init_elapsed = time.perf_counter() - gpu_init_start
+			print(format_status_line(True, when, f"Initializing GPU after restart...", gpu_init_elapsed))
+	else:
+		print(format_status_line(False, when, f"Restarting PostgreSQL to apply GUC changes...", restart_elapsed))
+		print(f"ERROR: Failed to restart PostgreSQL automatically: {restart_msg}", file=sys.stderr)
+		print(f"ALTER SYSTEM changes require a restart. Please restart PostgreSQL manually and re-run tests.", file=sys.stderr)
+		return 1
+	
 	# 3. Create test views (this also creates test_settings table)
 	when = datetime.now()
 	views_start = time.perf_counter()
@@ -2682,13 +2776,20 @@ def main() -> int:
 	when = datetime.now()
 	settings_start = time.perf_counter()
 	settings_sql = f"""
-	-- Store compute mode setting
+	-- Store compute mode setting (also store as gpu_mode for backward compatibility)
 	INSERT INTO test_settings (setting_key, setting_value, updated_at)
 	VALUES ('compute_mode', '{args.compute}', CURRENT_TIMESTAMP)
 	ON CONFLICT (setting_key) DO UPDATE SET
 		setting_value = EXCLUDED.setting_value,
 		updated_at = CURRENT_TIMESTAMP;
 	
+	-- Also store as gpu_mode for backward compatibility with older test files
+	INSERT INTO test_settings (setting_key, setting_value, updated_at)
+	VALUES ('gpu_mode', '{args.compute}', CURRENT_TIMESTAMP)
+	ON CONFLICT (setting_key) DO UPDATE SET
+		setting_value = EXCLUDED.setting_value,
+		updated_at = CURRENT_TIMESTAMP;
+	
 	-- Store number of rows used for test views
 	INSERT INTO test_settings (setting_key, setting_value, updated_at)
 	VALUES ('num_rows', '{args.num_rows}', CURRENT_TIMESTAMP)
diff --git a/NeuronDB/tests/setup_core_dump.sh b/NeuronDB/tests/setup_core_dump.sh
index 1a47a6e..13b57c4 100755
--- a/NeuronDB/tests/setup_core_dump.sh
+++ b/NeuronDB/tests/setup_core_dump.sh
@@ -27,3 +27,4 @@ echo "To generate core dump, run the test command below."
 echo "After crash, core file will be saved as: core.postgres.<pid>.<timestamp>"
 echo ""
 
+
diff --git a/NeuronDB/tests/sql/_test_settings_template.sql b/NeuronDB/tests/sql/_test_settings_template.sql
index 09eb69f..7bb77a8 100644
--- a/NeuronDB/tests/sql/_test_settings_template.sql
+++ b/NeuronDB/tests/sql/_test_settings_template.sql
@@ -7,7 +7,7 @@
 DO $$
 DECLARE
 	gpu_mode TEXT;
-	current_gpu_enabled TEXT;
+	current_compute_mode TEXT;
 	current_gpu_kernels TEXT;
 	num_rows_val TEXT;
 	table_exists BOOLEAN;
@@ -28,22 +28,22 @@ BEGIN
 		
 		-- Verify GPU configuration matches test_settings
 		BEGIN
-			SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+			SELECT current_setting('neurondb.compute_mode', true) INTO current_compute_mode;
 		EXCEPTION WHEN OTHERS THEN
-			current_gpu_enabled := NULL;
+			current_compute_mode := NULL;
 		END;
 		
 		-- Only verify if gpu_mode was actually set
 		IF gpu_mode IS NOT NULL THEN
 			IF gpu_mode = 'gpu' THEN
 				-- Verify GPU is enabled (should be set by test runner)
-				IF current_gpu_enabled IS NOT NULL AND current_gpu_enabled != 'on' THEN
-					RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+				IF current_compute_mode IS NOT NULL AND current_compute_mode != '1' THEN
+					RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: 1)', current_compute_mode;
 				END IF;
-			ELSE
-				-- Verify GPU is disabled (should be set by test runner)
-				IF current_gpu_enabled IS NOT NULL AND current_gpu_enabled != 'off' THEN
-					RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			ELSIF gpu_mode = 'cpu' THEN
+				-- Verify CPU mode (should be set by test runner)
+				IF current_compute_mode IS NOT NULL AND current_compute_mode != '0' THEN
+					RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: 0)', current_compute_mode;
 				END IF;
 			END IF;
 		END IF;
diff --git a/NeuronDB/tests/sql/advance/001_linreg_advance.sql b/NeuronDB/tests/sql/advance/001_linreg_advance.sql
index dd92424..d6a69b8 100644
--- a/NeuronDB/tests/sql/advance/001_linreg_advance.sql
+++ b/NeuronDB/tests/sql/advance/001_linreg_advance.sql
@@ -23,18 +23,18 @@ BEGIN
 	SELECT setting_value INTO gpu_kernels_val FROM test_settings WHERE setting_key = 'gpu_kernels';
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	SELECT current_setting('neurondb.gpu_kernels', true) INTO current_gpu_kernels;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSE
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
@@ -142,7 +142,7 @@ SELECT neurondb_gpu_info() AS gpu_info;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Test 1: GPU training with default parameters'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 DROP TABLE IF EXISTS gpu_model_temp_001;
 CREATE TEMP TABLE gpu_model_temp_001 AS
 SELECT neurondb.train('linear_regression', 
@@ -156,7 +156,7 @@ SELECT
 FROM gpu_model_temp_001;
 
 \echo 'Test 2: CPU training with default parameters'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 DROP TABLE IF EXISTS cpu_model_temp_001;
 CREATE TEMP TABLE cpu_model_temp_001 AS
 SELECT neurondb.train('linear_regression', 
@@ -284,7 +284,7 @@ END$$;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Predict Test 1: GPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SELECT 
 	'GPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -299,7 +299,7 @@ FROM (
 ) sub;
 
 \echo 'Predict Test 2: CPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'CPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -321,7 +321,7 @@ FROM test_test_view
 LIMIT 1;
 
 \echo 'Predict Test 4: Custom model batch (100 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'Custom Batch' AS test_type,
 	COUNT(*) AS n_predictions,
diff --git a/NeuronDB/tests/sql/advance/002_logreg_advance.sql b/NeuronDB/tests/sql/advance/002_logreg_advance.sql
index 045e4c8..602f9d2 100644
--- a/NeuronDB/tests/sql/advance/002_logreg_advance.sql
+++ b/NeuronDB/tests/sql/advance/002_logreg_advance.sql
@@ -20,17 +20,17 @@ BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSE
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
@@ -94,7 +94,7 @@ FROM test_train_view;
 \echo ''
 \echo 'GPU Configuration'
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SET neurondb.gpu_kernels = 'l2,cosine,ip,lr_train,lr_predict';
 SELECT neurondb_gpu_enable() AS gpu_available;
 SELECT neurondb_gpu_info() AS gpu_info;
@@ -108,7 +108,7 @@ SELECT neurondb_gpu_info() AS gpu_info;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Test 1: GPU training with default parameters'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 DROP TABLE IF EXISTS gpu_model_temp_002;
 CREATE TEMP TABLE gpu_model_temp_002 AS
 SELECT neurondb.train('logistic_regression', 
@@ -122,7 +122,7 @@ SELECT
 FROM gpu_model_temp_002;
 
 \echo 'Test 2: CPU training with default parameters'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 DROP TABLE IF EXISTS cpu_model_temp_002;
 CREATE TEMP TABLE cpu_model_temp_002 AS
 SELECT neurondb.train('logistic_regression', 
@@ -294,7 +294,7 @@ END$$;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Predict Test 1: GPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SELECT 
 	'GPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -308,7 +308,7 @@ FROM (
 ) sub;
 
 \echo 'Predict Test 2: CPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'CPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -329,7 +329,7 @@ FROM test_test_view
 LIMIT 1;
 
 \echo 'Predict Test 4: Custom model batch (100 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'Custom Batch' AS test_type,
 	COUNT(*) AS n_predictions,
diff --git a/NeuronDB/tests/sql/advance/003_rf_advance.sql b/NeuronDB/tests/sql/advance/003_rf_advance.sql
index 096c659..bfa4acb 100644
--- a/NeuronDB/tests/sql/advance/003_rf_advance.sql
+++ b/NeuronDB/tests/sql/advance/003_rf_advance.sql
@@ -20,17 +20,17 @@ BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSE
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
@@ -94,7 +94,7 @@ FROM test_train_view;
 \echo ''
 \echo 'GPU Configuration'
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SET neurondb.gpu_kernels = 'l2,cosine,ip,rf_train,rf_predict,rf_split';
 SELECT neurondb_gpu_enable() AS gpu_available;
 SELECT neurondb_gpu_info() AS gpu_info;
@@ -108,7 +108,7 @@ SELECT neurondb_gpu_info() AS gpu_info;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Test 1: GPU training with default parameters'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 DROP TABLE IF EXISTS gpu_model_temp_003;
 CREATE TEMP TABLE gpu_model_temp_003 AS
 SELECT neurondb.train('random_forest', 
@@ -122,7 +122,7 @@ SELECT
 FROM gpu_model_temp_003;
 
 \echo 'Test 2: CPU training with default parameters'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 DROP TABLE IF EXISTS cpu_model_temp_003;
 CREATE TEMP TABLE cpu_model_temp_003 AS
 SELECT neurondb.train('random_forest', 
@@ -321,7 +321,7 @@ END$$;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Predict Test 1: GPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SELECT 
 	'GPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -335,7 +335,7 @@ FROM (
 ) sub;
 
 \echo 'Predict Test 2: CPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'CPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -356,7 +356,7 @@ FROM test_test_view
 LIMIT 1;
 
 \echo 'Predict Test 4: Custom model batch (100 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'Custom Batch' AS test_type,
 	COUNT(*) AS n_predictions,
diff --git a/NeuronDB/tests/sql/advance/004_svm_advance.sql b/NeuronDB/tests/sql/advance/004_svm_advance.sql
index dd15769..b3bb235 100644
--- a/NeuronDB/tests/sql/advance/004_svm_advance.sql
+++ b/NeuronDB/tests/sql/advance/004_svm_advance.sql
@@ -20,17 +20,17 @@ BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSE
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
@@ -94,7 +94,7 @@ FROM test_train_view;
 \echo ''
 \echo 'GPU Configuration'
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SET neurondb.gpu_kernels = 'l2,cosine,ip,svm_train,svm_predict';
 SELECT neurondb_gpu_enable() AS gpu_available;
 SELECT neurondb_gpu_info() AS gpu_info;
@@ -108,7 +108,7 @@ SELECT neurondb_gpu_info() AS gpu_info;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Test 1: GPU training with default parameters'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 DROP TABLE IF EXISTS gpu_model_temp_004;
 CREATE TEMP TABLE gpu_model_temp_004 AS
 SELECT neurondb.train('svm', 
@@ -122,7 +122,7 @@ SELECT
 FROM gpu_model_temp_004;
 
 \echo 'Test 2: CPU training with default parameters'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 DROP TABLE IF EXISTS cpu_model_temp_004;
 CREATE TEMP TABLE cpu_model_temp_004 AS
 SELECT neurondb.train('svm', 
@@ -237,7 +237,7 @@ END$$;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Predict Test 1: GPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SELECT 
 	'GPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -251,7 +251,7 @@ FROM (
 ) sub;
 
 \echo 'Predict Test 2: CPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'CPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -272,7 +272,7 @@ FROM test_test_view
 LIMIT 1;
 
 \echo 'Predict Test 4: Custom model batch (100 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'Custom Batch' AS test_type,
 	COUNT(*) AS n_predictions,
diff --git a/NeuronDB/tests/sql/advance/005_dt_advance.sql b/NeuronDB/tests/sql/advance/005_dt_advance.sql
index 04c09f0..379eac7 100644
--- a/NeuronDB/tests/sql/advance/005_dt_advance.sql
+++ b/NeuronDB/tests/sql/advance/005_dt_advance.sql
@@ -20,17 +20,17 @@ BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSE
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
@@ -94,7 +94,7 @@ FROM test_train_view;
 \echo ''
 \echo 'GPU Configuration'
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SET neurondb.gpu_kernels = 'l2,cosine,ip,dt_train,dt_predict';
 SELECT neurondb_gpu_enable() AS gpu_available;
 SELECT neurondb_gpu_info() AS gpu_info;
@@ -108,7 +108,7 @@ SELECT neurondb_gpu_info() AS gpu_info;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Test 1: GPU training with default parameters'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 DROP TABLE IF EXISTS gpu_model_temp_005;
 CREATE TEMP TABLE gpu_model_temp_005 AS
 SELECT neurondb.train('decision_tree', 
@@ -122,7 +122,7 @@ SELECT
 FROM gpu_model_temp_005;
 
 \echo 'Test 2: CPU training with default parameters'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 DROP TABLE IF EXISTS cpu_model_temp_005;
 CREATE TEMP TABLE cpu_model_temp_005 AS
 SELECT neurondb.train('decision_tree', 
@@ -258,7 +258,7 @@ END$$;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Predict Test 1: GPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SELECT 
 	'GPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -272,7 +272,7 @@ FROM (
 ) sub;
 
 \echo 'Predict Test 2: CPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'CPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -293,7 +293,7 @@ FROM test_test_view
 LIMIT 1;
 
 \echo 'Predict Test 4: Custom model batch (100 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'Custom Batch' AS test_type,
 	COUNT(*) AS n_predictions,
diff --git a/NeuronDB/tests/sql/advance/006_ridge_advance.sql b/NeuronDB/tests/sql/advance/006_ridge_advance.sql
index 5ca7bd4..00550bf 100644
--- a/NeuronDB/tests/sql/advance/006_ridge_advance.sql
+++ b/NeuronDB/tests/sql/advance/006_ridge_advance.sql
@@ -20,17 +20,17 @@ BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSE
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
@@ -94,7 +94,7 @@ FROM test_train_view;
 \echo ''
 \echo 'GPU Configuration'
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SET neurondb.gpu_kernels = 'l2,cosine,ip,ridge_train,ridge_predict';
 SELECT neurondb_gpu_enable() AS gpu_available;
 SELECT neurondb_gpu_info() AS gpu_info;
@@ -108,7 +108,7 @@ SELECT neurondb_gpu_info() AS gpu_info;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Test 1: GPU training with default parameters'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 DROP TABLE IF EXISTS gpu_model_temp_006;
 CREATE TEMP TABLE gpu_model_temp_006 AS
 SELECT neurondb.train('ridge', 
@@ -122,7 +122,7 @@ SELECT
 FROM gpu_model_temp_006;
 
 \echo 'Test 2: CPU training with default parameters'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 DROP TABLE IF EXISTS cpu_model_temp_006;
 CREATE TEMP TABLE cpu_model_temp_006 AS
 SELECT neurondb.train('ridge', 
@@ -244,7 +244,7 @@ END$$;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Predict Test 1: GPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SELECT 
 	'GPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -258,7 +258,7 @@ FROM (
 ) sub;
 
 \echo 'Predict Test 2: CPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'CPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -279,7 +279,7 @@ FROM test_test_view
 LIMIT 1;
 
 \echo 'Predict Test 4: Custom model batch (100 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'Custom Batch' AS test_type,
 	COUNT(*) AS n_predictions,
diff --git a/NeuronDB/tests/sql/advance/007_lasso_advance.sql b/NeuronDB/tests/sql/advance/007_lasso_advance.sql
index 0b1f9a0..5ffa261 100644
--- a/NeuronDB/tests/sql/advance/007_lasso_advance.sql
+++ b/NeuronDB/tests/sql/advance/007_lasso_advance.sql
@@ -20,17 +20,17 @@ BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSE
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
@@ -94,7 +94,7 @@ FROM test_train_view;
 \echo ''
 \echo 'GPU Configuration'
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SET neurondb.gpu_kernels = 'l2,cosine,ip,lasso_train,lasso_predict';
 SELECT neurondb_gpu_enable() AS gpu_available;
 SELECT neurondb_gpu_info() AS gpu_info;
@@ -108,7 +108,7 @@ SELECT neurondb_gpu_info() AS gpu_info;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Test 1: GPU training with default parameters'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 DROP TABLE IF EXISTS gpu_model_temp_007;
 CREATE TEMP TABLE gpu_model_temp_007 AS
 SELECT neurondb.train('lasso', 
@@ -122,7 +122,7 @@ SELECT
 FROM gpu_model_temp_007;
 
 \echo 'Test 2: CPU training with default parameters'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 DROP TABLE IF EXISTS cpu_model_temp_007;
 CREATE TEMP TABLE cpu_model_temp_007 AS
 SELECT neurondb.train('lasso', 
@@ -244,7 +244,7 @@ END$$;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Predict Test 1: GPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SELECT 
 	'GPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -258,7 +258,7 @@ FROM (
 ) sub;
 
 \echo 'Predict Test 2: CPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'CPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -279,7 +279,7 @@ FROM test_test_view
 LIMIT 1;
 
 \echo 'Predict Test 4: Custom model batch (100 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'Custom Batch' AS test_type,
 	COUNT(*) AS n_predictions,
diff --git a/NeuronDB/tests/sql/advance/008_nb_advance.sql b/NeuronDB/tests/sql/advance/008_nb_advance.sql
index 07899e8..e020c82 100644
--- a/NeuronDB/tests/sql/advance/008_nb_advance.sql
+++ b/NeuronDB/tests/sql/advance/008_nb_advance.sql
@@ -69,7 +69,7 @@ FROM test_train_view;
 \echo ''
 \echo 'GPU Configuration'
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SET neurondb.gpu_kernels = 'l2,cosine,ip,nb_train,nb_predict';
 SELECT neurondb_gpu_enable() AS gpu_available;
 SELECT neurondb_gpu_info() AS gpu_info;
@@ -83,7 +83,7 @@ SELECT neurondb_gpu_info() AS gpu_info;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Test 1: GPU training with default parameters'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 DROP TABLE IF EXISTS gpu_model_temp_012;
 CREATE TEMP TABLE gpu_model_temp_012 AS
 SELECT neurondb.train('naive_bayes', 
@@ -97,7 +97,7 @@ SELECT
 FROM gpu_model_temp_012;
 
 \echo 'Test 2: CPU training with default parameters'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 DROP TABLE IF EXISTS cpu_model_temp_012;
 CREATE TEMP TABLE cpu_model_temp_012 AS
 SELECT neurondb.train('naive_bayes', 
@@ -211,7 +211,7 @@ END$$;
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
 \echo 'Predict Test 1: GPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SELECT 
 	'GPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -225,7 +225,7 @@ FROM (
 ) sub;
 
 \echo 'Predict Test 2: CPU batch prediction (1000 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'CPU Batch' AS test_type,
 	COUNT(*) AS n_predictions,
@@ -246,7 +246,7 @@ FROM test_test_view
 LIMIT 1;
 
 \echo 'Predict Test 4: Custom model batch (100 rows)'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'Custom Batch' AS test_type,
 	COUNT(*) AS n_predictions,
diff --git a/NeuronDB/tests/sql/advance/038_reranking_flash_advance.sql b/NeuronDB/tests/sql/advance/038_reranking_flash_advance.sql
index 997a717..6fddda4 100644
--- a/NeuronDB/tests/sql/advance/038_reranking_flash_advance.sql
+++ b/NeuronDB/tests/sql/advance/038_reranking_flash_advance.sql
@@ -37,7 +37,7 @@ LATERAL rerank_flash(
 
 SELECT
 	'GPU support' AS test_name,
-	current_setting('neurondb.gpu_enabled', true) AS gpu_enabled,
+	current_setting('neurondb.compute_mode', true) AS gpu_enabled,
 	current_setting('neurondb.gpu_backend', true) AS gpu_backend;
 
 \echo ''
diff --git a/NeuronDB/tests/sql/advance/041_quantization_fp8_advance.sql b/NeuronDB/tests/sql/advance/041_quantization_fp8_advance.sql
index a3ca9df..c5648ad 100644
--- a/NeuronDB/tests/sql/advance/041_quantization_fp8_advance.sql
+++ b/NeuronDB/tests/sql/advance/041_quantization_fp8_advance.sql
@@ -39,7 +39,7 @@ FROM dequantized, original;
 
 SELECT
 	'GPU support check' AS test_name,
-	current_setting('neurondb.gpu_enabled', true) AS gpu_enabled;
+	current_setting('neurondb.compute_mode', true) AS gpu_enabled;
 
 \echo ''
 \echo 'âœ… Advance FP8 quantization tests completed'
diff --git a/NeuronDB/tests/sql/advance/051_gpu_info_advance.sql b/NeuronDB/tests/sql/advance/051_gpu_info_advance.sql
index 6790f16..9d70591 100644
--- a/NeuronDB/tests/sql/advance/051_gpu_info_advance.sql
+++ b/NeuronDB/tests/sql/advance/051_gpu_info_advance.sql
@@ -22,18 +22,18 @@ BEGIN
 	SELECT setting_value INTO gpu_kernels_val FROM test_settings WHERE setting_key = 'gpu_kernels';
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	SELECT current_setting('neurondb.gpu_kernels', true) INTO current_gpu_kernels;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSE
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
@@ -46,7 +46,7 @@ END $$;
 \echo 'GPU Configuration'
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 -- GPU already configured via test_settings above
-SELECT current_setting('neurondb.gpu_enabled', true) AS gpu_available;
+SELECT current_setting('neurondb.compute_mode', true) AS gpu_available;
 
 /*
  * ---- GPU INFORMATION TESTS ----
@@ -174,12 +174,12 @@ FROM neurondb_gpu_info()
 WHERE is_available = true;
 
 \echo 'Test 8: GPU Enable/Disable'
-SET neurondb.gpu_enabled = off;
+SET neurondb.compute_mode = off;
 SELECT 
 	'GPU Disabled' AS status,
 	neurondb_gpu_enable() AS enable_result;
 
-SET neurondb.gpu_enabled = on;
+SET neurondb.compute_mode = on;
 SELECT 
 	'GPU Enabled' AS status,
 	neurondb_gpu_enable() AS enable_result;
@@ -194,7 +194,7 @@ DO $$
 DECLARE
 	result double precision;
 BEGIN
-	SET neurondb.gpu_enabled = off;
+	SET neurondb.compute_mode = off;
 	BEGIN
 		result := vector_l2_distance_gpu('[1,2,3]'::vector, '[4,5,6]'::vector);
 	EXCEPTION WHEN OTHERS THEN 
@@ -202,7 +202,7 @@ BEGIN
 		-- Error handled correctly
 		NULL;
 	END;
-	SET neurondb.gpu_enabled = on;
+	SET neurondb.compute_mode = on;
 END$$;
 
 \echo 'Error Test 2: Invalid kernel configuration'
diff --git a/NeuronDB/tests/sql/basic/001_linreg_basic.sql b/NeuronDB/tests/sql/basic/001_linreg_basic.sql
index 230b2ba..c2db1df 100644
--- a/NeuronDB/tests/sql/basic/001_linreg_basic.sql
+++ b/NeuronDB/tests/sql/basic/001_linreg_basic.sql
@@ -21,12 +21,16 @@ SET client_min_messages TO WARNING;
 
 DO $$
 DECLARE
-	gpu_mode TEXT;
-	current_gpu_enabled TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
-	IF gpu_mode = 'gpu' THEN
+	-- Get compute_mode from test_settings (set by run_test.py)
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
+	-- Note: compute_mode is set by run_test.py via switch_gpu_mode()
+	-- This block is kept for backward compatibility but compute_mode
+	-- should be set before running tests via run_test.py
+	IF compute_mode = 'gpu' THEN
+		PERFORM neurondb_gpu_enable();
+	ELSIF compute_mode = 'auto' THEN
 		PERFORM neurondb_gpu_enable();
 	END IF;
 END $$;
@@ -221,17 +225,17 @@ WHERE m.model_id = t.model_id;
 /* Verify GPU was used for training when GPU mode is enabled */
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 	storage_val TEXT;
 	gpu_available BOOLEAN;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	SELECT COALESCE(m.metrics::jsonb->>'storage', 'cpu') INTO storage_val
 	FROM neurondb.ml_models m, gpu_model_temp t
 	WHERE m.model_id = t.model_id;
 	
 	-- Only check GPU info if GPU mode is enabled - never call GPU functions in CPU mode
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			-- Check if GPU is actually available (use is_available column which matches C code check)
 			SELECT COALESCE(BOOL_OR(is_available), false) INTO gpu_available
@@ -254,7 +258,7 @@ BEGIN
 	END IF;
 	
 	-- If CPU mode is enabled, verify model was trained on CPU
-	IF gpu_mode = 'cpu' AND storage_val = 'gpu' THEN
+	IF compute_mode = 'cpu' AND storage_val = 'gpu' THEN
 		RAISE WARNING 'CPU mode enabled but model was trained on GPU (storage=gpu)';
 	END IF;
 END $$;
@@ -433,11 +437,11 @@ WHERE tm.test_name = '001_linreg_basic';
 -- Only show GPU info if GPU mode is enabled - never call GPU functions in CPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		-- Display GPU information only when GPU mode is enabled
 		PERFORM NULL; -- Placeholder for GPU info display
 	END IF;
@@ -446,12 +450,12 @@ END $$;
 -- Conditionally display GPU info only in GPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 	rec RECORD;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			RAISE NOTICE 'GPU Information:';
 			-- Display GPU info by looping through results
diff --git a/NeuronDB/tests/sql/basic/002_logreg_basic.sql b/NeuronDB/tests/sql/basic/002_logreg_basic.sql
index 479c053..7c60bfd 100644
--- a/NeuronDB/tests/sql/basic/002_logreg_basic.sql
+++ b/NeuronDB/tests/sql/basic/002_logreg_basic.sql
@@ -7,6 +7,7 @@
  *
  *-------------------------------------------------------------------------*/
 
+SET client_min_messages TO WARNING;
 \set ON_ERROR_STOP on
 \timing on
 \pset footer off
@@ -20,13 +21,17 @@
 
 DO $$
 DECLARE
-	gpu_mode TEXT;
-	current_gpu_enabled TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
-	IF gpu_mode = 'gpu' THEN
-		SELECT neurondb_gpu_enable();
+	-- Get compute_mode from test_settings (set by run_test.py)
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
+	-- Note: compute_mode is set by run_test.py via switch_gpu_mode()
+	-- This block is kept for backward compatibility but compute_mode
+	-- should be set before running tests via run_test.py
+	IF compute_mode = 'gpu' THEN
+		PERFORM neurondb_gpu_enable();
+	ELSIF compute_mode = 'auto' THEN
+		PERFORM neurondb_gpu_enable();
 	END IF;
 END $$;
 
@@ -149,6 +154,11 @@ FROM (
 \echo 'Evaluation Tests'
 \echo 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”'
 
+-- Show evaluation setup info
+SELECT 
+	(SELECT model_id FROM gpu_model_temp LIMIT 1) AS model_id,
+	(SELECT COUNT(*)::bigint FROM test_test_view WHERE features IS NOT NULL AND label IS NOT NULL) AS test_samples_available;
+
 DROP TABLE IF EXISTS gpu_metrics_temp;
 CREATE TEMP TABLE gpu_metrics_temp (metrics jsonb);
 
@@ -157,7 +167,9 @@ DECLARE
 	mid integer;
 	metrics_result jsonb;
 	eval_error text;
+	test_count bigint;
 BEGIN
+	-- Get model_id
 	SELECT model_id INTO mid FROM gpu_model_temp LIMIT 1;
 	IF mid IS NULL THEN
 		RAISE WARNING 'No model_id found in gpu_model_temp';
@@ -165,27 +177,34 @@ BEGIN
 		RETURN;
 	END IF;
 	
+	-- Verify test data is available
+	SELECT COUNT(*) INTO test_count 
+	FROM test_test_view 
+	WHERE features IS NOT NULL AND label IS NOT NULL;
+	
+	IF test_count < 1 THEN
+		RAISE WARNING 'No valid test samples available (count: %)', test_count;
+		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', 'No valid test samples available'));
+		RETURN;
+	END IF;
+	
+	RAISE NOTICE 'Evaluating model_id: % on test_test_view with % valid samples', mid, test_count;
+	
 	BEGIN
-		BEGIN
-			metrics_result := neurondb.evaluate(mid, 'test_test_view', 'features', 'label');
-			
-			IF metrics_result IS NULL THEN
-				RAISE WARNING 'Evaluation returned NULL';
-				INSERT INTO gpu_metrics_temp VALUES ('{"error": "Evaluation returned NULL"}'::jsonb);
-			ELSE
-				INSERT INTO gpu_metrics_temp VALUES (metrics_result);
-			END IF;
-		EXCEPTION WHEN OTHERS THEN
-			eval_error := SQLERRM;
-			RAISE WARNING 'Evaluation exception: %', eval_error;
-			eval_error := REPLACE(REPLACE(REPLACE(eval_error, '"', '\"'), E'\n', ' '), E'\r', ' ');
-			INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error));
-		END;
+		metrics_result := neurondb.evaluate(mid, 'test_test_view', 'features', 'label');
+		
+		IF metrics_result IS NULL THEN
+			RAISE WARNING 'Evaluation returned NULL for model_id: %', mid;
+			INSERT INTO gpu_metrics_temp VALUES ('{"error": "Evaluation returned NULL"}'::jsonb);
+		ELSE
+			RAISE NOTICE 'Evaluation successful for model_id: %', mid;
+			INSERT INTO gpu_metrics_temp VALUES (metrics_result);
+		END IF;
 	EXCEPTION WHEN OTHERS THEN
 		eval_error := SQLERRM;
-		RAISE WARNING 'Outer evaluation exception: %', eval_error;
+		RAISE WARNING 'Evaluation exception for model_id %: %', mid, eval_error;
 		eval_error := REPLACE(REPLACE(REPLACE(eval_error, '"', '\"'), E'\n', ' '), E'\r', ' ');
-		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error));
+		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error, 'model_id', mid));
 	END;
 END $$;
 
@@ -205,17 +224,17 @@ WHERE m.model_id = t.model_id;
 /* Verify GPU was used for training when GPU mode is enabled */
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 	storage_val TEXT;
 	gpu_available BOOLEAN;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	SELECT COALESCE(m.metrics::jsonb->>'storage', 'cpu') INTO storage_val
 	FROM neurondb.ml_models m, gpu_model_temp t
 	WHERE m.model_id = t.model_id;
 	
 	-- Only check GPU info if GPU mode is enabled - never call GPU functions in CPU mode
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			-- Check if GPU is actually available (use is_available column which matches C code check)
 			SELECT COALESCE(BOOL_OR(is_available), false) INTO gpu_available
@@ -238,7 +257,7 @@ BEGIN
 	END IF;
 	
 	-- If CPU mode is enabled, verify model was trained on CPU
-	IF gpu_mode = 'cpu' AND storage_val = 'gpu' THEN
+	IF compute_mode = 'cpu' AND storage_val = 'gpu' THEN
 		RAISE WARNING 'CPU mode enabled but model was trained on GPU (storage=gpu)';
 	END IF;
 END $$;
@@ -402,11 +421,11 @@ WHERE tm.test_name = '002_logreg_basic';
 -- Only show GPU info if GPU mode is enabled - never call GPU functions in CPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		-- Display GPU information only when GPU mode is enabled
 		PERFORM NULL; -- Placeholder for GPU info display
 	END IF;
@@ -415,16 +434,23 @@ END $$;
 -- Conditionally display GPU info only in GPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
+	rec RECORD;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			RAISE NOTICE 'GPU Information:';
-			PERFORM device_id, device_name, total_memory_mb, free_memory_mb, 
+			-- Display GPU info by looping through results
+			FOR rec IN SELECT device_id, device_name, total_memory_mb, free_memory_mb, 
 					compute_capability_major, compute_capability_minor, is_available
-			FROM neurondb_gpu_info();
+			FROM neurondb_gpu_info()
+			LOOP
+				RAISE NOTICE '  Device %: % (Memory: %/% MB, Compute: %.%, Available: %)',
+					rec.device_id, rec.device_name, rec.free_memory_mb, rec.total_memory_mb,
+					rec.compute_capability_major, rec.compute_capability_minor, rec.is_available;
+			END LOOP;
 		EXCEPTION WHEN OTHERS THEN
 			RAISE NOTICE 'GPU information not available';
 		END;
diff --git a/NeuronDB/tests/sql/basic/003_rf_basic.sql b/NeuronDB/tests/sql/basic/003_rf_basic.sql
index a6ee23f..ef7d016 100644
--- a/NeuronDB/tests/sql/basic/003_rf_basic.sql
+++ b/NeuronDB/tests/sql/basic/003_rf_basic.sql
@@ -7,6 +7,7 @@
  *
  *-------------------------------------------------------------------------*/
 
+SET client_min_messages TO WARNING;
 \set ON_ERROR_STOP on
 \timing on
 \pset footer off
@@ -20,13 +21,17 @@
 
 DO $$
 DECLARE
-	gpu_mode TEXT;
-	current_gpu_enabled TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
-	IF gpu_mode = 'gpu' THEN
-		SELECT neurondb_gpu_enable();
+	-- Get compute_mode from test_settings (set by run_test.py)
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
+	-- Note: compute_mode is set by run_test.py via switch_gpu_mode()
+	-- This block is kept for backward compatibility but compute_mode
+	-- should be set before running tests via run_test.py
+	IF compute_mode = 'gpu' THEN
+		PERFORM neurondb_gpu_enable();
+	ELSIF compute_mode = 'auto' THEN
+		PERFORM neurondb_gpu_enable();
 	END IF;
 END $$;
 
@@ -165,7 +170,9 @@ DECLARE
 	mid integer;
 	metrics_result jsonb;
 	eval_error text;
+	test_count bigint;
 BEGIN
+	-- Get model_id
 	SELECT model_id INTO mid FROM gpu_model_temp LIMIT 1;
 	IF mid IS NULL THEN
 		RAISE WARNING 'No model_id found in gpu_model_temp';
@@ -173,27 +180,34 @@ BEGIN
 		RETURN;
 	END IF;
 	
+	-- Verify test data is available
+	SELECT COUNT(*) INTO test_count 
+	FROM test_test_view 
+	WHERE features IS NOT NULL AND label IS NOT NULL;
+	
+	IF test_count < 1 THEN
+		RAISE WARNING 'No valid test samples available (count: %)', test_count;
+		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', 'No valid test samples available'));
+		RETURN;
+	END IF;
+	
+	RAISE NOTICE 'Evaluating model_id: % on test_test_view with % valid samples', mid, test_count;
+	
 	BEGIN
-		BEGIN
-			metrics_result := neurondb.evaluate(mid, 'test_test_view', 'features', 'label');
-			
-			IF metrics_result IS NULL THEN
-				RAISE WARNING 'Evaluation returned NULL';
-				INSERT INTO gpu_metrics_temp VALUES ('{"error": "Evaluation returned NULL"}'::jsonb);
-			ELSE
-				INSERT INTO gpu_metrics_temp VALUES (metrics_result);
-			END IF;
-		EXCEPTION WHEN OTHERS THEN
-			eval_error := SQLERRM;
-			RAISE WARNING 'Evaluation exception: %', eval_error;
-			eval_error := REPLACE(REPLACE(REPLACE(eval_error, '"', '\"'), E'\n', ' '), E'\r', ' ');
-			INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error));
-		END;
+		metrics_result := neurondb.evaluate(mid, 'test_test_view', 'features', 'label');
+		
+		IF metrics_result IS NULL THEN
+			RAISE WARNING 'Evaluation returned NULL for model_id: %', mid;
+			INSERT INTO gpu_metrics_temp VALUES ('{"error": "Evaluation returned NULL"}'::jsonb);
+		ELSE
+			RAISE NOTICE 'Evaluation successful for model_id: %', mid;
+			INSERT INTO gpu_metrics_temp VALUES (metrics_result);
+		END IF;
 	EXCEPTION WHEN OTHERS THEN
 		eval_error := SQLERRM;
-		RAISE WARNING 'Outer evaluation exception: %', eval_error;
+		RAISE WARNING 'Evaluation exception for model_id %: %', mid, eval_error;
 		eval_error := REPLACE(REPLACE(REPLACE(eval_error, '"', '\"'), E'\n', ' '), E'\r', ' ');
-		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error));
+		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error, 'model_id', mid));
 	END;
 END $$;
 
@@ -212,17 +226,17 @@ WHERE m.model_id = t.model_id;
 /* Verify GPU was used for training when GPU mode is enabled */
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 	storage_val TEXT;
 	gpu_available BOOLEAN;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	SELECT COALESCE(m.metrics::jsonb->>'storage', 'cpu') INTO storage_val
 	FROM neurondb.ml_models m, gpu_model_temp t
 	WHERE m.model_id = t.model_id;
 	
 	-- Only check GPU info if GPU mode is enabled - never call GPU functions in CPU mode
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			-- Check if GPU is actually available (use is_available column which matches C code check)
 			SELECT COALESCE(BOOL_OR(is_available), false) INTO gpu_available
@@ -245,7 +259,7 @@ BEGIN
 	END IF;
 	
 	-- If CPU mode is enabled, verify model was trained on CPU
-	IF gpu_mode = 'cpu' AND storage_val = 'gpu' THEN
+	IF compute_mode = 'cpu' AND storage_val = 'gpu' THEN
 		RAISE WARNING 'CPU mode enabled but model was trained on GPU (storage=gpu)';
 	END IF;
 END $$;
@@ -408,11 +422,11 @@ WHERE tm.test_name = '003_rf_basic';
 -- Only show GPU info if GPU mode is enabled - never call GPU functions in CPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		-- Display GPU information only when GPU mode is enabled
 		PERFORM NULL; -- Placeholder for GPU info display
 	END IF;
@@ -421,16 +435,23 @@ END $$;
 -- Conditionally display GPU info only in GPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
+	rec RECORD;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			RAISE NOTICE 'GPU Information:';
-			PERFORM device_id, device_name, total_memory_mb, free_memory_mb, 
+			-- Display GPU info by looping through results
+			FOR rec IN SELECT device_id, device_name, total_memory_mb, free_memory_mb, 
 					compute_capability_major, compute_capability_minor, is_available
-			FROM neurondb_gpu_info();
+			FROM neurondb_gpu_info()
+			LOOP
+				RAISE NOTICE '  Device %: % (Memory: %/% MB, Compute: %.%, Available: %)',
+					rec.device_id, rec.device_name, rec.free_memory_mb, rec.total_memory_mb,
+					rec.compute_capability_major, rec.compute_capability_minor, rec.is_available;
+			END LOOP;
 		EXCEPTION WHEN OTHERS THEN
 			RAISE NOTICE 'GPU information not available';
 		END;
diff --git a/NeuronDB/tests/sql/basic/004_svm_basic.sql b/NeuronDB/tests/sql/basic/004_svm_basic.sql
index 19efee4..7a24cd9 100644
--- a/NeuronDB/tests/sql/basic/004_svm_basic.sql
+++ b/NeuronDB/tests/sql/basic/004_svm_basic.sql
@@ -7,6 +7,7 @@
  *
  *-------------------------------------------------------------------------*/
 
+SET client_min_messages TO WARNING;
 \set ON_ERROR_STOP on
 \timing on
 \pset footer off
@@ -20,13 +21,17 @@
 
 DO $$
 DECLARE
-	gpu_mode TEXT;
-	current_gpu_enabled TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
-	IF gpu_mode = 'gpu' THEN
-		SELECT neurondb_gpu_enable();
+	-- Get compute_mode from test_settings (set by run_test.py)
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
+	-- Note: compute_mode is set by run_test.py via switch_gpu_mode()
+	-- This block is kept for backward compatibility but compute_mode
+	-- should be set before running tests via run_test.py
+	IF compute_mode = 'gpu' THEN
+		PERFORM neurondb_gpu_enable();
+	ELSIF compute_mode = 'auto' THEN
+		PERFORM neurondb_gpu_enable();
 	END IF;
 END $$;
 
@@ -157,7 +162,9 @@ DECLARE
 	mid integer;
 	metrics_result jsonb;
 	eval_error text;
+	test_count bigint;
 BEGIN
+	-- Get model_id
 	SELECT model_id INTO mid FROM gpu_model_temp LIMIT 1;
 	IF mid IS NULL THEN
 		RAISE WARNING 'No model_id found in gpu_model_temp';
@@ -165,27 +172,34 @@ BEGIN
 		RETURN;
 	END IF;
 	
+	-- Verify test data is available
+	SELECT COUNT(*) INTO test_count 
+	FROM test_test_view 
+	WHERE features IS NOT NULL AND label IS NOT NULL;
+	
+	IF test_count < 1 THEN
+		RAISE WARNING 'No valid test samples available (count: %)', test_count;
+		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', 'No valid test samples available'));
+		RETURN;
+	END IF;
+	
+	RAISE NOTICE 'Evaluating model_id: % on test_test_view with % valid samples', mid, test_count;
+	
 	BEGIN
-		BEGIN
-			metrics_result := neurondb.evaluate(mid, 'test_test_view', 'features', 'label');
-			
-			IF metrics_result IS NULL THEN
-				RAISE WARNING 'Evaluation returned NULL';
-				INSERT INTO gpu_metrics_temp VALUES ('{"error": "Evaluation returned NULL"}'::jsonb);
-			ELSE
-				INSERT INTO gpu_metrics_temp VALUES (metrics_result);
-			END IF;
-		EXCEPTION WHEN OTHERS THEN
-			eval_error := SQLERRM;
-			RAISE WARNING 'Evaluation exception: %', eval_error;
-			eval_error := REPLACE(REPLACE(REPLACE(eval_error, '"', '\"'), E'\n', ' '), E'\r', ' ');
-			INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error));
-		END;
+		metrics_result := neurondb.evaluate(mid, 'test_test_view', 'features', 'label');
+		
+		IF metrics_result IS NULL THEN
+			RAISE WARNING 'Evaluation returned NULL for model_id: %', mid;
+			INSERT INTO gpu_metrics_temp VALUES ('{"error": "Evaluation returned NULL"}'::jsonb);
+		ELSE
+			RAISE NOTICE 'Evaluation successful for model_id: %', mid;
+			INSERT INTO gpu_metrics_temp VALUES (metrics_result);
+		END IF;
 	EXCEPTION WHEN OTHERS THEN
 		eval_error := SQLERRM;
-		RAISE WARNING 'Outer evaluation exception: %', eval_error;
+		RAISE WARNING 'Evaluation exception for model_id %: %', mid, eval_error;
 		eval_error := REPLACE(REPLACE(REPLACE(eval_error, '"', '\"'), E'\n', ' '), E'\r', ' ');
-		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error));
+		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error, 'model_id', mid));
 	END;
 END $$;
 
@@ -204,17 +218,17 @@ WHERE m.model_id = t.model_id;
 /* Verify GPU was used for training when GPU mode is enabled */
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 	storage_val TEXT;
 	gpu_available BOOLEAN;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	SELECT COALESCE(m.metrics::jsonb->>'storage', 'cpu') INTO storage_val
 	FROM neurondb.ml_models m, gpu_model_temp t
 	WHERE m.model_id = t.model_id;
 	
 	-- Only check GPU info if GPU mode is enabled - never call GPU functions in CPU mode
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			-- Check if GPU is actually available (use is_available column which matches C code check)
 			SELECT COALESCE(BOOL_OR(is_available), false) INTO gpu_available
@@ -237,7 +251,7 @@ BEGIN
 	END IF;
 	
 	-- If CPU mode is enabled, verify model was trained on CPU
-	IF gpu_mode = 'cpu' AND storage_val = 'gpu' THEN
+	IF compute_mode = 'cpu' AND storage_val = 'gpu' THEN
 		RAISE WARNING 'CPU mode enabled but model was trained on GPU (storage=gpu)';
 	END IF;
 END $$;
@@ -400,11 +414,11 @@ WHERE tm.test_name = '004_svm_basic';
 -- Only show GPU info if GPU mode is enabled - never call GPU functions in CPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		-- Display GPU information only when GPU mode is enabled
 		PERFORM NULL; -- Placeholder for GPU info display
 	END IF;
@@ -413,16 +427,23 @@ END $$;
 -- Conditionally display GPU info only in GPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
+	rec RECORD;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			RAISE NOTICE 'GPU Information:';
-			PERFORM device_id, device_name, total_memory_mb, free_memory_mb, 
+			-- Display GPU info by looping through results
+			FOR rec IN SELECT device_id, device_name, total_memory_mb, free_memory_mb, 
 					compute_capability_major, compute_capability_minor, is_available
-			FROM neurondb_gpu_info();
+			FROM neurondb_gpu_info()
+			LOOP
+				RAISE NOTICE '  Device %: % (Memory: %/% MB, Compute: %.%, Available: %)',
+					rec.device_id, rec.device_name, rec.free_memory_mb, rec.total_memory_mb,
+					rec.compute_capability_major, rec.compute_capability_minor, rec.is_available;
+			END LOOP;
 		EXCEPTION WHEN OTHERS THEN
 			RAISE NOTICE 'GPU information not available';
 		END;
diff --git a/NeuronDB/tests/sql/basic/005_dt_basic.sql b/NeuronDB/tests/sql/basic/005_dt_basic.sql
index 4a22b80..e65a2ed 100644
--- a/NeuronDB/tests/sql/basic/005_dt_basic.sql
+++ b/NeuronDB/tests/sql/basic/005_dt_basic.sql
@@ -21,13 +21,17 @@ SET client_min_messages TO WARNING;
 
 DO $$
 DECLARE
-	gpu_mode TEXT;
-	current_gpu_enabled TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
-	IF gpu_mode = 'gpu' THEN
-		SELECT neurondb_gpu_enable();
+	-- Get compute_mode from test_settings (set by run_test.py)
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
+	-- Note: compute_mode is set by run_test.py via switch_gpu_mode()
+	-- This block is kept for backward compatibility but compute_mode
+	-- should be set before running tests via run_test.py
+	IF compute_mode = 'gpu' THEN
+		PERFORM neurondb_gpu_enable();
+	ELSIF compute_mode = 'auto' THEN
+		PERFORM neurondb_gpu_enable();
 	END IF;
 END $$;
 
@@ -187,25 +191,18 @@ BEGIN
 	RAISE NOTICE 'Evaluating model_id: % on test_test_view with % valid samples', mid, test_count;
 	
 	BEGIN
-		BEGIN
-			metrics_result := neurondb.evaluate(mid, 'test_test_view', 'features', 'label');
-			
-			IF metrics_result IS NULL THEN
-				RAISE WARNING 'Evaluation returned NULL for model_id: %', mid;
-				INSERT INTO gpu_metrics_temp VALUES ('{"error": "Evaluation returned NULL"}'::jsonb);
-			ELSE
-				RAISE NOTICE 'Evaluation successful for model_id: %', mid;
-				INSERT INTO gpu_metrics_temp VALUES (metrics_result);
-			END IF;
-		EXCEPTION WHEN OTHERS THEN
-			eval_error := SQLERRM;
-			RAISE WARNING 'Evaluation exception for model_id %: %', mid, eval_error;
-			eval_error := REPLACE(REPLACE(REPLACE(eval_error, '"', '\"'), E'\n', ' '), E'\r', ' ');
-			INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error, 'model_id', mid));
-		END;
+		metrics_result := neurondb.evaluate(mid, 'test_test_view', 'features', 'label');
+		
+		IF metrics_result IS NULL THEN
+			RAISE WARNING 'Evaluation returned NULL for model_id: %', mid;
+			INSERT INTO gpu_metrics_temp VALUES ('{"error": "Evaluation returned NULL"}'::jsonb);
+		ELSE
+			RAISE NOTICE 'Evaluation successful for model_id: %', mid;
+			INSERT INTO gpu_metrics_temp VALUES (metrics_result);
+		END IF;
 	EXCEPTION WHEN OTHERS THEN
 		eval_error := SQLERRM;
-		RAISE WARNING 'Outer evaluation exception for model_id %: %', mid, eval_error;
+		RAISE WARNING 'Evaluation exception for model_id %: %', mid, eval_error;
 		eval_error := REPLACE(REPLACE(REPLACE(eval_error, '"', '\"'), E'\n', ' '), E'\r', ' ');
 		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error, 'model_id', mid));
 	END;
@@ -229,17 +226,17 @@ WHERE m.model_id = t.model_id;
 /* Verify GPU was used for training when GPU mode is enabled */
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 	storage_val TEXT;
 	gpu_available BOOLEAN;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	SELECT COALESCE(m.metrics::jsonb->>'storage', 'cpu') INTO storage_val
 	FROM neurondb.ml_models m, gpu_model_temp t
 	WHERE m.model_id = t.model_id;
 	
 	-- Only check GPU info if GPU mode is enabled - never call GPU functions in CPU mode
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			-- Check if GPU is actually available (use is_available column which matches C code check)
 			SELECT COALESCE(BOOL_OR(is_available), false) INTO gpu_available
@@ -262,7 +259,7 @@ BEGIN
 	END IF;
 	
 	-- If CPU mode is enabled, verify model was trained on CPU
-	IF gpu_mode = 'cpu' AND storage_val = 'gpu' THEN
+	IF compute_mode = 'cpu' AND storage_val = 'gpu' THEN
 		RAISE WARNING 'CPU mode enabled but model was trained on GPU (storage=gpu)';
 	END IF;
 END $$;
@@ -441,11 +438,11 @@ WHERE tm.test_name = '005_dt_basic';
 -- Only show GPU info if GPU mode is enabled - never call GPU functions in CPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		-- Display GPU information only when GPU mode is enabled
 		PERFORM NULL; -- Placeholder for GPU info display
 	END IF;
@@ -454,16 +451,23 @@ END $$;
 -- Conditionally display GPU info only in GPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
+	rec RECORD;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			RAISE NOTICE 'GPU Information:';
-			PERFORM device_id, device_name, total_memory_mb, free_memory_mb, 
+			-- Display GPU info by looping through results
+			FOR rec IN SELECT device_id, device_name, total_memory_mb, free_memory_mb, 
 					compute_capability_major, compute_capability_minor, is_available
-			FROM neurondb_gpu_info();
+			FROM neurondb_gpu_info()
+			LOOP
+				RAISE NOTICE '  Device %: % (Memory: %/% MB, Compute: %.%, Available: %)',
+					rec.device_id, rec.device_name, rec.free_memory_mb, rec.total_memory_mb,
+					rec.compute_capability_major, rec.compute_capability_minor, rec.is_available;
+			END LOOP;
 		EXCEPTION WHEN OTHERS THEN
 			RAISE NOTICE 'GPU information not available';
 		END;
diff --git a/NeuronDB/tests/sql/basic/006_ridge_basic.sql b/NeuronDB/tests/sql/basic/006_ridge_basic.sql
index e986071..c421cd1 100644
--- a/NeuronDB/tests/sql/basic/006_ridge_basic.sql
+++ b/NeuronDB/tests/sql/basic/006_ridge_basic.sql
@@ -7,6 +7,7 @@
  *
  *-------------------------------------------------------------------------*/
 
+SET client_min_messages TO WARNING;
 \set ON_ERROR_STOP on
 \timing on
 \pset footer off
@@ -20,13 +21,17 @@
 
 DO $$
 DECLARE
-	gpu_mode TEXT;
-	current_gpu_enabled TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
-	IF gpu_mode = 'gpu' THEN
-		SELECT neurondb_gpu_enable();
+	-- Get compute_mode from test_settings (set by run_test.py)
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
+	-- Note: compute_mode is set by run_test.py via switch_gpu_mode()
+	-- This block is kept for backward compatibility but compute_mode
+	-- should be set before running tests via run_test.py
+	IF compute_mode = 'gpu' THEN
+		PERFORM neurondb_gpu_enable();
+	ELSIF compute_mode = 'auto' THEN
+		PERFORM neurondb_gpu_enable();
 	END IF;
 END $$;
 
@@ -155,7 +160,9 @@ DECLARE
 	mid integer;
 	metrics_result jsonb;
 	eval_error text;
+	test_count bigint;
 BEGIN
+	-- Get model_id
 	SELECT model_id INTO mid FROM gpu_model_temp LIMIT 1;
 	IF mid IS NULL THEN
 		RAISE WARNING 'No model_id found in gpu_model_temp';
@@ -163,27 +170,34 @@ BEGIN
 		RETURN;
 	END IF;
 	
+	-- Verify test data is available
+	SELECT COUNT(*) INTO test_count 
+	FROM test_test_view 
+	WHERE features IS NOT NULL AND label IS NOT NULL;
+	
+	IF test_count < 1 THEN
+		RAISE WARNING 'No valid test samples available (count: %)', test_count;
+		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', 'No valid test samples available'));
+		RETURN;
+	END IF;
+	
+	RAISE NOTICE 'Evaluating model_id: % on test_test_view with % valid samples', mid, test_count;
+	
 	BEGIN
-		BEGIN
-			metrics_result := neurondb.evaluate(mid, 'test_test_view', 'features', 'label');
-			
-			IF metrics_result IS NULL THEN
-				RAISE WARNING 'Evaluation returned NULL';
-				INSERT INTO gpu_metrics_temp VALUES ('{"error": "Evaluation returned NULL"}'::jsonb);
-			ELSE
-				INSERT INTO gpu_metrics_temp VALUES (metrics_result);
-			END IF;
-		EXCEPTION WHEN OTHERS THEN
-			eval_error := SQLERRM;
-			RAISE WARNING 'Evaluation exception: %', eval_error;
-			eval_error := REPLACE(REPLACE(REPLACE(eval_error, '"', '\"'), E'\n', ' '), E'\r', ' ');
-			INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error));
-		END;
+		metrics_result := neurondb.evaluate(mid, 'test_test_view', 'features', 'label');
+		
+		IF metrics_result IS NULL THEN
+			RAISE WARNING 'Evaluation returned NULL for model_id: %', mid;
+			INSERT INTO gpu_metrics_temp VALUES ('{"error": "Evaluation returned NULL"}'::jsonb);
+		ELSE
+			RAISE NOTICE 'Evaluation successful for model_id: %', mid;
+			INSERT INTO gpu_metrics_temp VALUES (metrics_result);
+		END IF;
 	EXCEPTION WHEN OTHERS THEN
 		eval_error := SQLERRM;
-		RAISE WARNING 'Outer evaluation exception: %', eval_error;
+		RAISE WARNING 'Evaluation exception for model_id %: %', mid, eval_error;
 		eval_error := REPLACE(REPLACE(REPLACE(eval_error, '"', '\"'), E'\n', ' '), E'\r', ' ');
-		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error));
+		INSERT INTO gpu_metrics_temp VALUES (jsonb_build_object('error', eval_error, 'model_id', mid));
 	END;
 END $$;
 
@@ -204,17 +218,17 @@ WHERE m.model_id = t.model_id;
 /* Verify GPU was used for training when GPU mode is enabled */
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 	storage_val TEXT;
 	gpu_available BOOLEAN;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	SELECT COALESCE(m.metrics::jsonb->>'storage', 'cpu') INTO storage_val
 	FROM neurondb.ml_models m, gpu_model_temp t
 	WHERE m.model_id = t.model_id;
 	
 	-- Only check GPU info if GPU mode is enabled - never call GPU functions in CPU mode
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			-- Check if GPU is actually available (use is_available column which matches C code check)
 			SELECT COALESCE(BOOL_OR(is_available), false) INTO gpu_available
@@ -237,7 +251,7 @@ BEGIN
 	END IF;
 	
 	-- If CPU mode is enabled, verify model was trained on CPU
-	IF gpu_mode = 'cpu' AND storage_val = 'gpu' THEN
+	IF compute_mode = 'cpu' AND storage_val = 'gpu' THEN
 		RAISE WARNING 'CPU mode enabled but model was trained on GPU (storage=gpu)';
 	END IF;
 END $$;
@@ -403,11 +417,11 @@ WHERE tm.test_name = '006_ridge_basic';
 -- Only show GPU info if GPU mode is enabled - never call GPU functions in CPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		-- Display GPU information only when GPU mode is enabled
 		PERFORM NULL; -- Placeholder for GPU info display
 	END IF;
@@ -416,16 +430,23 @@ END $$;
 -- Conditionally display GPU info only in GPU mode
 DO $$
 DECLARE
-	gpu_mode TEXT;
+	compute_mode TEXT;
+	rec RECORD;
 BEGIN
-	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
+	SELECT setting_value INTO compute_mode FROM test_settings WHERE setting_key = 'compute_mode';
 	
-	IF gpu_mode = 'gpu' THEN
+	IF compute_mode = 'gpu' THEN
 		BEGIN
 			RAISE NOTICE 'GPU Information:';
-			PERFORM device_id, device_name, total_memory_mb, free_memory_mb, 
+			-- Display GPU info by looping through results
+			FOR rec IN SELECT device_id, device_name, total_memory_mb, free_memory_mb, 
 					compute_capability_major, compute_capability_minor, is_available
-			FROM neurondb_gpu_info();
+			FROM neurondb_gpu_info()
+			LOOP
+				RAISE NOTICE '  Device %: % (Memory: %/% MB, Compute: %.%, Available: %)',
+					rec.device_id, rec.device_name, rec.free_memory_mb, rec.total_memory_mb,
+					rec.compute_capability_major, rec.compute_capability_minor, rec.is_available;
+			END LOOP;
 		EXCEPTION WHEN OTHERS THEN
 			RAISE NOTICE 'GPU information not available';
 		END;
diff --git a/NeuronDB/tests/sql/basic/007_lasso_basic.sql b/NeuronDB/tests/sql/basic/007_lasso_basic.sql
index 1d1d018..f9e63e6 100644
--- a/NeuronDB/tests/sql/basic/007_lasso_basic.sql
+++ b/NeuronDB/tests/sql/basic/007_lasso_basic.sql
@@ -24,7 +24,7 @@ DECLARE
 	current_gpu_enabled TEXT;
 BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	IF gpu_mode = 'gpu' THEN
 		SELECT neurondb_gpu_enable();
 	END IF;
diff --git a/NeuronDB/tests/sql/basic/008_nb_basic.sql b/NeuronDB/tests/sql/basic/008_nb_basic.sql
index 0fd14bd..2866402 100644
--- a/NeuronDB/tests/sql/basic/008_nb_basic.sql
+++ b/NeuronDB/tests/sql/basic/008_nb_basic.sql
@@ -24,7 +24,7 @@ DECLARE
 	current_gpu_enabled TEXT;
 BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	IF gpu_mode = 'gpu' THEN
 		SELECT neurondb_gpu_enable();
 	END IF;
diff --git a/NeuronDB/tests/sql/basic/009_knn_basic.sql b/NeuronDB/tests/sql/basic/009_knn_basic.sql
index 086bb74..b6d090b 100644
--- a/NeuronDB/tests/sql/basic/009_knn_basic.sql
+++ b/NeuronDB/tests/sql/basic/009_knn_basic.sql
@@ -24,7 +24,7 @@ DECLARE
 	current_gpu_enabled TEXT;
 BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	IF gpu_mode = 'gpu' THEN
 		SELECT neurondb_gpu_enable();
 	END IF;
diff --git a/NeuronDB/tests/sql/basic/010_xgboost_basic.sql b/NeuronDB/tests/sql/basic/010_xgboost_basic.sql
index 632d86b..9e5b48b 100644
--- a/NeuronDB/tests/sql/basic/010_xgboost_basic.sql
+++ b/NeuronDB/tests/sql/basic/010_xgboost_basic.sql
@@ -24,7 +24,7 @@ DECLARE
 	current_gpu_enabled TEXT;
 BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	IF gpu_mode = 'gpu' THEN
 		SELECT neurondb_gpu_enable();
 	END IF;
diff --git a/NeuronDB/tests/sql/basic/011_catboost_basic.sql b/NeuronDB/tests/sql/basic/011_catboost_basic.sql
index 26c9523..59eafd3 100644
--- a/NeuronDB/tests/sql/basic/011_catboost_basic.sql
+++ b/NeuronDB/tests/sql/basic/011_catboost_basic.sql
@@ -24,7 +24,7 @@ DECLARE
 	current_gpu_enabled TEXT;
 BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	IF gpu_mode = 'gpu' THEN
 		SELECT neurondb_gpu_enable();
 	END IF;
diff --git a/NeuronDB/tests/sql/basic/012_lightgbm_basic.sql b/NeuronDB/tests/sql/basic/012_lightgbm_basic.sql
index 403a2bf..bbe8842 100644
--- a/NeuronDB/tests/sql/basic/012_lightgbm_basic.sql
+++ b/NeuronDB/tests/sql/basic/012_lightgbm_basic.sql
@@ -24,7 +24,7 @@ DECLARE
 	current_gpu_enabled TEXT;
 BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	IF gpu_mode = 'gpu' THEN
 		SELECT neurondb_gpu_enable();
 	END IF;
diff --git a/NeuronDB/tests/sql/basic/013_neural_network_basic.sql b/NeuronDB/tests/sql/basic/013_neural_network_basic.sql
index 7cf57e9..d2e7b10 100644
--- a/NeuronDB/tests/sql/basic/013_neural_network_basic.sql
+++ b/NeuronDB/tests/sql/basic/013_neural_network_basic.sql
@@ -73,17 +73,17 @@ BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSE
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
diff --git a/NeuronDB/tests/sql/basic/014_gmm_basic.sql b/NeuronDB/tests/sql/basic/014_gmm_basic.sql
index 9aca451..dd0b575 100644
--- a/NeuronDB/tests/sql/basic/014_gmm_basic.sql
+++ b/NeuronDB/tests/sql/basic/014_gmm_basic.sql
@@ -24,7 +24,7 @@ DECLARE
 	current_gpu_enabled TEXT;
 BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	IF gpu_mode = 'gpu' THEN
 		SELECT neurondb_gpu_enable();
 	END IF;
@@ -122,7 +122,7 @@ BEGIN
 	EXCEPTION WHEN OTHERS THEN
 		-- If training fails, try with CPU explicitly disabled
 		BEGIN
-			SET neurondb.gpu_enabled = off;
+			SET neurondb.compute_mode = off;
 			model_id_val := neurondb.train(
 				'default',
 				'gmm',
diff --git a/NeuronDB/tests/sql/basic/015_kmeans_basic.sql b/NeuronDB/tests/sql/basic/015_kmeans_basic.sql
index 7a29858..14d87fa 100644
--- a/NeuronDB/tests/sql/basic/015_kmeans_basic.sql
+++ b/NeuronDB/tests/sql/basic/015_kmeans_basic.sql
@@ -24,7 +24,7 @@ DECLARE
 	current_gpu_enabled TEXT;
 BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	IF gpu_mode = 'gpu' THEN
 		SELECT neurondb_gpu_enable();
 	END IF;
diff --git a/NeuronDB/tests/sql/basic/016_minibatch_kmeans_basic.sql b/NeuronDB/tests/sql/basic/016_minibatch_kmeans_basic.sql
index b6a5221..dd669a9 100644
--- a/NeuronDB/tests/sql/basic/016_minibatch_kmeans_basic.sql
+++ b/NeuronDB/tests/sql/basic/016_minibatch_kmeans_basic.sql
@@ -24,7 +24,7 @@ DECLARE
 	current_gpu_enabled TEXT;
 BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	IF gpu_mode = 'gpu' THEN
 		SELECT neurondb_gpu_enable();
 	END IF;
diff --git a/NeuronDB/tests/sql/basic/017_hierarchical_basic.sql b/NeuronDB/tests/sql/basic/017_hierarchical_basic.sql
index e80ded5..ef58d27 100644
--- a/NeuronDB/tests/sql/basic/017_hierarchical_basic.sql
+++ b/NeuronDB/tests/sql/basic/017_hierarchical_basic.sql
@@ -24,7 +24,7 @@ DECLARE
 	current_gpu_enabled TEXT;
 BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	IF gpu_mode = 'gpu' THEN
 		SELECT neurondb_gpu_enable();
 	END IF;
diff --git a/NeuronDB/tests/sql/basic/018_dbscan_basic.sql b/NeuronDB/tests/sql/basic/018_dbscan_basic.sql
index 625d338..155728d 100644
--- a/NeuronDB/tests/sql/basic/018_dbscan_basic.sql
+++ b/NeuronDB/tests/sql/basic/018_dbscan_basic.sql
@@ -24,7 +24,7 @@ DECLARE
 	current_gpu_enabled TEXT;
 BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	IF gpu_mode = 'gpu' THEN
 		SELECT neurondb_gpu_enable();
 	END IF;
diff --git a/NeuronDB/tests/sql/basic/019_pca_basic.sql b/NeuronDB/tests/sql/basic/019_pca_basic.sql
index e8bfffe..1583dde 100644
--- a/NeuronDB/tests/sql/basic/019_pca_basic.sql
+++ b/NeuronDB/tests/sql/basic/019_pca_basic.sql
@@ -41,17 +41,17 @@ BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSE
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
diff --git a/NeuronDB/tests/sql/basic/020_timeseries_basic.sql b/NeuronDB/tests/sql/basic/020_timeseries_basic.sql
index 22e620e..2d4c891 100644
--- a/NeuronDB/tests/sql/basic/020_timeseries_basic.sql
+++ b/NeuronDB/tests/sql/basic/020_timeseries_basic.sql
@@ -113,17 +113,17 @@ BEGIN
 	SELECT setting_value INTO gpu_mode FROM test_settings WHERE setting_key = 'gpu_mode';
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSE
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
diff --git a/NeuronDB/tests/sql/basic/021_automl_basic.sql b/NeuronDB/tests/sql/basic/021_automl_basic.sql
index f2704e4..a68c2e3 100644
--- a/NeuronDB/tests/sql/basic/021_automl_basic.sql
+++ b/NeuronDB/tests/sql/basic/021_automl_basic.sql
@@ -25,18 +25,18 @@ BEGIN
 	END;
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	SELECT current_setting('neurondb.automl.use_gpu', true) INTO current_automl_gpu;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSIF gpu_mode IS NOT NULL THEN
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
diff --git a/NeuronDB/tests/sql/basic/022_automl_standalone_basic.sql b/NeuronDB/tests/sql/basic/022_automl_standalone_basic.sql
index 83aa80d..bdbac63 100644
--- a/NeuronDB/tests/sql/basic/022_automl_standalone_basic.sql
+++ b/NeuronDB/tests/sql/basic/022_automl_standalone_basic.sql
@@ -22,18 +22,18 @@ BEGIN
 	END;
 	
 	-- Verify GPU configuration matches test_settings (set by test runner)
-	SELECT current_setting('neurondb.gpu_enabled', true) INTO current_gpu_enabled;
+	SELECT current_setting('neurondb.compute_mode', true) INTO current_gpu_enabled;
 	SELECT current_setting('neurondb.automl.use_gpu', true) INTO current_automl_gpu;
 	
 	IF gpu_mode = 'gpu' THEN
 		-- Verify GPU is enabled (should be set by test runner)
 		IF current_gpu_enabled != 'on' THEN
-			RAISE WARNING 'GPU mode expected but neurondb.gpu_enabled = % (expected: on)', current_gpu_enabled;
+			RAISE WARNING 'GPU mode expected but neurondb.compute_mode = % (expected: on)', current_gpu_enabled;
 		END IF;
 	ELSIF gpu_mode IS NOT NULL THEN
 		-- Verify GPU is disabled (should be set by test runner)
 		IF current_gpu_enabled != 'off' THEN
-			RAISE WARNING 'CPU mode expected but neurondb.gpu_enabled = % (expected: off)', current_gpu_enabled;
+			RAISE WARNING 'CPU mode expected but neurondb.compute_mode = % (expected: off)', current_gpu_enabled;
 		END IF;
 	END IF;
 END $$;
diff --git a/NeuronDB/tests/sql/basic/031_embeddings_basic_text.sql b/NeuronDB/tests/sql/basic/031_embeddings_basic_text.sql
index cb0308a..f010912 100644
--- a/NeuronDB/tests/sql/basic/031_embeddings_basic_text.sql
+++ b/NeuronDB/tests/sql/basic/031_embeddings_basic_text.sql
@@ -10,7 +10,7 @@
 \echo 'NOTE: embed_text() warnings are expected if LLM is not configured.'
 \echo '      To generate real embeddings, configure:'
 \echo '      - neurondb.llm_api_key (Hugging Face API key)'
-\echo '      - Or enable GPU embedding via GUC (ALTER SYSTEM SET neurondb.gpu_enabled = on)'
+\echo '      - Or enable GPU embedding via GUC (ALTER SYSTEM SET neurondb.compute_mode = on)'
 \echo '      Without configuration, embed_text() returns zero vectors (graceful fallback).'
 \echo ''
 
diff --git a/NeuronDB/tests/sql/basic/032_embeddings_batch.sql b/NeuronDB/tests/sql/basic/032_embeddings_batch.sql
index 0d084b3..6b04810 100644
--- a/NeuronDB/tests/sql/basic/032_embeddings_batch.sql
+++ b/NeuronDB/tests/sql/basic/032_embeddings_batch.sql
@@ -10,7 +10,7 @@
 \echo 'NOTE: embed_text_batch() warnings are expected if LLM is not configured.'
 \echo '      To generate real embeddings, configure:'
 \echo '      - neurondb.llm_api_key (Hugging Face API key)'
-\echo '      - Or enable GPU embedding via GUC (ALTER SYSTEM SET neurondb.gpu_enabled = on)'
+\echo '      - Or enable GPU embedding via GUC (ALTER SYSTEM SET neurondb.compute_mode = on)'
 \echo '      Without configuration, embed_text_batch() returns zero vectors (graceful fallback).'
 \echo ''
 
diff --git a/NeuronDB/tests/sql/basic/035_embeddings_multimodal.sql b/NeuronDB/tests/sql/basic/035_embeddings_multimodal.sql
index 43a0717..3ea0d04 100644
--- a/NeuronDB/tests/sql/basic/035_embeddings_multimodal.sql
+++ b/NeuronDB/tests/sql/basic/035_embeddings_multimodal.sql
@@ -13,7 +13,7 @@ SET neurondb.llm_fail_open = on;
 \echo 'NOTE: Image and multimodal embedding tests run with fail_open mode enabled.'
 \echo '      To generate real embeddings, configure:'
 \echo '      - neurondb.llm_api_key (Hugging Face API key)'
-\echo '      - Or enable GPU embedding via GUC (ALTER SYSTEM SET neurondb.gpu_enabled = on)'
+\echo '      - Or enable GPU embedding via GUC (ALTER SYSTEM SET neurondb.compute_mode = on)'
 \echo '      Without configuration, these functions return fallback embeddings (graceful fallback).'
 \echo ''
 
diff --git a/NeuronDB/tests/sql/basic/036_rag_basic.sql b/NeuronDB/tests/sql/basic/036_rag_basic.sql
index 61e7c58..3eb1d5b 100644
--- a/NeuronDB/tests/sql/basic/036_rag_basic.sql
+++ b/NeuronDB/tests/sql/basic/036_rag_basic.sql
@@ -10,7 +10,7 @@
 \echo 'NOTE: embed_text() warnings are expected if LLM is not configured.'
 \echo '      To generate real embeddings, configure:'
 \echo '      - neurondb.llm_api_key (Hugging Face API key)'
-\echo '      - Or enable GPU embedding via GUC (ALTER SYSTEM SET neurondb.gpu_enabled = on)'
+\echo '      - Or enable GPU embedding via GUC (ALTER SYSTEM SET neurondb.compute_mode = on)'
 \echo '      Without configuration, embed_text() returns zero vectors (graceful fallback).'
 \echo ''
 
diff --git a/NeuronMCP/client/mcp_client/commands.py b/NeuronMCP/client/mcp_client/commands.py
index ef06b84..c553b5b 100644
--- a/NeuronMCP/client/mcp_client/commands.py
+++ b/NeuronMCP/client/mcp_client/commands.py
@@ -207,3 +207,4 @@ def build_tool_call_request(tool_name: str, arguments: Dict[str, Any]) -> Dict[s
 
 
 
+
diff --git a/NeuronMCP/client/mcp_client/config.py b/NeuronMCP/client/mcp_client/config.py
index 21b91d9..8b1bea9 100644
--- a/NeuronMCP/client/mcp_client/config.py
+++ b/NeuronMCP/client/mcp_client/config.py
@@ -80,3 +80,4 @@ def load_config(config_path: str, server_name: str = "neurondb") -> MCPConfig:
 
 
 
+
diff --git a/NeuronMCP/cmd/neurondb-mcp-client/main.go b/NeuronMCP/cmd/neurondb-mcp-client/main.go
index d937c02..89fab6b 100644
--- a/NeuronMCP/cmd/neurondb-mcp-client/main.go
+++ b/NeuronMCP/cmd/neurondb-mcp-client/main.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * main.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/cmd/neurondb-mcp-client/main.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package main
 
 import (
@@ -37,7 +50,7 @@ func main() {
 
 	flag.Parse()
 
-	// Validate arguments
+  /* Validate arguments */
 	if *configPath == "" {
 		fmt.Fprintf(os.Stderr, "Error: -c/--config is required\n")
 		flag.Usage()
@@ -56,17 +69,17 @@ func main() {
 		os.Exit(1)
 	}
 
-	// Load configuration
+  /* Load configuration */
 	config, err := client.LoadConfig(*configPath, *serverName)
 	if err != nil {
 		fmt.Fprintf(os.Stderr, "Error loading configuration: %v\n", err)
 		os.Exit(1)
 	}
 
-	// Initialize output manager
+  /* Initialize output manager */
 	outputMgr := client.NewOutputManager(*output)
 
-	// Create and connect client
+  /* Create and connect client */
 	mcpClient, err := client.NewMCPClient(config, *verbose)
 	if err != nil {
 		fmt.Fprintf(os.Stderr, "Error creating MCP client: %v\n", err)
@@ -79,9 +92,9 @@ func main() {
 	}
 	defer mcpClient.Disconnect()
 
-	// Execute commands
+  /* Execute commands */
 	if *execute != "" {
-		// Single command execution
+   /* Single command execution */
 		result, err := mcpClient.ExecuteCommand(*execute)
 		if err != nil {
 			result = map[string]interface{}{
@@ -97,7 +110,7 @@ func main() {
 			fmt.Printf("Command executed: %s\n", *execute)
 		}
 	} else if *file != "" {
-		// Batch command execution
+   /* Batch command execution */
 		commands, err := readCommandsFile(*file)
 		if err != nil {
 			fmt.Fprintf(os.Stderr, "Error reading command file: %v\n", err)
@@ -123,7 +136,7 @@ func main() {
 		}
 	}
 
-	// Save output
+  /* Save output */
 	outputFile, err := outputMgr.Save()
 	if err != nil {
 		fmt.Fprintf(os.Stderr, "Error saving output: %v\n", err)
@@ -142,7 +155,7 @@ func readCommandsFile(filePath string) ([]string, error) {
 	lines := splitLines(string(data))
 	for _, line := range lines {
 		line = trimSpace(line)
-		// Skip empty lines and comments
+   /* Skip empty lines and comments */
 		if line == "" || line[0] == '#' {
 			continue
 		}
diff --git a/NeuronMCP/cmd/neurondb-mcp/main.go b/NeuronMCP/cmd/neurondb-mcp/main.go
index c08d100..4ef4261 100644
--- a/NeuronMCP/cmd/neurondb-mcp/main.go
+++ b/NeuronMCP/cmd/neurondb-mcp/main.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * main.go
+ *    Main entry point for NeuronMCP server
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/cmd/neurondb-mcp/main.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package main
 
 import (
@@ -13,7 +26,7 @@ func main() {
 	ctx, cancel := context.WithCancel(context.Background())
 	defer cancel()
 
-	// Handle signals
+  /* Handle signals */
 	sigChan := make(chan os.Signal, 1)
 	signal.Notify(sigChan, os.Interrupt, syscall.SIGTERM)
 
@@ -22,14 +35,14 @@ func main() {
 		cancel()
 	}()
 
-	// Create and start server
+  /* Create and start server */
 	srv, err := server.NewServer()
 	if err != nil {
 		os.Stderr.WriteString("Failed to create server: " + err.Error() + "\n")
 		os.Exit(1)
 	}
 
-	// Start server
+  /* Start server */
 	if err := srv.Start(ctx); err != nil {
 		if err != context.Canceled {
 			os.Stderr.WriteString("Server error: " + err.Error() + "\n")
@@ -37,7 +50,7 @@ func main() {
 		}
 	}
 
-	// Cleanup
+  /* Cleanup */
 	if err := srv.Stop(); err != nil {
 		os.Stderr.WriteString("Error stopping server: " + err.Error() + "\n")
 	}
diff --git a/NeuronMCP/internal/client/client.go b/NeuronMCP/internal/client/client.go
index 1ec287f..e601feb 100644
--- a/NeuronMCP/internal/client/client.go
+++ b/NeuronMCP/internal/client/client.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * client.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/client/client.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package client
 
 import (
@@ -9,19 +22,19 @@ import (
 	"github.com/neurondb/NeuronMCP/pkg/mcp"
 )
 
-// MCPConfig represents the configuration for an MCP server
+/* MCPConfig represents the configuration for an MCP server */
 type MCPConfig struct {
 	Command string
 	Env     map[string]string
 	Args    []string
 }
 
-// GetEnv returns environment variables, merging with current environment
+/* GetEnv returns environment variables, merging with current environment */
 func (c *MCPConfig) GetEnv() map[string]string {
 	env := make(map[string]string)
-	// Copy current environment
+  /* Copy current environment */
 	for _, e := range os.Environ() {
-		// Split on first '='
+   /* Split on first '=' */
 		for i := 0; i < len(e); i++ {
 			if e[i] == '=' {
 				env[e[:i]] = e[i+1:]
@@ -29,14 +42,14 @@ func (c *MCPConfig) GetEnv() map[string]string {
 			}
 		}
 	}
-	// Override with config env
+  /* Override with config env */
 	for k, v := range c.Env {
 		env[k] = v
 	}
 	return env
 }
 
-// MCPClient is a client for communicating with MCP servers
+/* MCPClient is a client for communicating with MCP servers */
 type MCPClient struct {
 	config      *MCPConfig
 	verbose     bool
@@ -44,7 +57,7 @@ type MCPClient struct {
 	initialized bool
 }
 
-// NewMCPClient creates a new MCP client
+/* NewMCPClient creates a new MCP client */
 func NewMCPClient(config *MCPConfig, verbose bool) (*MCPClient, error) {
 	return &MCPClient{
 		config:  config,
@@ -52,13 +65,13 @@ func NewMCPClient(config *MCPConfig, verbose bool) (*MCPClient, error) {
 	}, nil
 }
 
-// Connect connects to the MCP server
+/* Connect connects to the MCP server */
 func (c *MCPClient) Connect() error {
 	if c.transport != nil {
 		return fmt.Errorf("already connected")
 	}
 
-	// Create transport
+  /* Create transport */
 	transport, err := NewClientTransport(c.config.Command, c.config.GetEnv(), c.config.Args)
 	if err != nil {
 		return fmt.Errorf("failed to create transport: %w", err)
@@ -66,7 +79,7 @@ func (c *MCPClient) Connect() error {
 
 	c.transport = transport
 
-	// Start server process
+  /* Start server process */
 	if c.verbose {
 		fmt.Printf("Starting MCP server: %s\n", c.config.Command)
 	}
@@ -75,17 +88,17 @@ func (c *MCPClient) Connect() error {
 		return fmt.Errorf("failed to start transport: %w", err)
 	}
 
-	// Initialize connection
+  /* Initialize connection */
 	return c.initialize()
 }
 
-// initialize initializes the MCP connection
+/* initialize initializes the MCP connection */
 func (c *MCPClient) initialize() error {
 	if c.initialized {
 		return nil
 	}
 
-	// Send initialize request
+  /* Send initialize request */
 	initRequest := &mcp.JSONRPCRequest{
 		JSONRPC: "2.0",
 		ID:      json.RawMessage(`"` + generateID() + `"`),
@@ -113,7 +126,7 @@ func (c *MCPClient) initialize() error {
 		fmt.Println("MCP connection initialized")
 	}
 
-	// Send initialized notification
+  /* Send initialized notification */
 	notification := map[string]interface{}{
 		"jsonrpc": "2.0",
 		"method":  "notifications/initialized",
@@ -121,7 +134,7 @@ func (c *MCPClient) initialize() error {
 	}
 
 	if err := c.transport.SendNotification(notification); err != nil {
-		// Notification errors are not fatal
+   /* Notification errors are not fatal */
 		if c.verbose {
 			fmt.Printf("Warning: failed to send initialized notification: %v\n", err)
 		}
@@ -131,7 +144,7 @@ func (c *MCPClient) initialize() error {
 	return nil
 }
 
-// Disconnect disconnects from the MCP server
+/* Disconnect disconnects from the MCP server */
 func (c *MCPClient) Disconnect() {
 	if c.transport != nil {
 		if c.verbose {
@@ -143,7 +156,7 @@ func (c *MCPClient) Disconnect() {
 	}
 }
 
-// ListTools lists available tools
+/* ListTools lists available tools */
 func (c *MCPClient) ListTools() (map[string]interface{}, error) {
 	request := &mcp.JSONRPCRequest{
 		JSONRPC: "2.0",
@@ -167,7 +180,7 @@ func (c *MCPClient) ListTools() (map[string]interface{}, error) {
 		if resultMap, ok := response.Result.(map[string]interface{}); ok {
 			return resultMap, nil
 		}
-		// If result is not a map, wrap it
+   /* If result is not a map, wrap it */
 		return map[string]interface{}{
 			"result": response.Result,
 		}, nil
@@ -176,7 +189,7 @@ func (c *MCPClient) ListTools() (map[string]interface{}, error) {
 	return map[string]interface{}{}, nil
 }
 
-// CallTool calls a tool with the given name and arguments
+/* CallTool calls a tool with the given name and arguments */
 func (c *MCPClient) CallTool(toolName string, arguments map[string]interface{}) (map[string]interface{}, error) {
 	params := map[string]interface{}{
 		"name":      toolName,
@@ -210,7 +223,7 @@ func (c *MCPClient) CallTool(toolName string, arguments map[string]interface{})
 		if resultMap, ok := response.Result.(map[string]interface{}); ok {
 			return resultMap, nil
 		}
-		// If result is not a map, wrap it
+   /* If result is not a map, wrap it */
 		return map[string]interface{}{
 			"result": response.Result,
 		}, nil
@@ -219,9 +232,9 @@ func (c *MCPClient) CallTool(toolName string, arguments map[string]interface{})
 	return map[string]interface{}{}, nil
 }
 
-// ExecuteCommand executes a command string
+/* ExecuteCommand executes a command string */
 func (c *MCPClient) ExecuteCommand(commandStr string) (map[string]interface{}, error) {
-	// Parse command
+  /* Parse command */
 	toolName, arguments, err := ParseCommand(commandStr)
 	if err != nil {
 		return map[string]interface{}{
@@ -229,7 +242,7 @@ func (c *MCPClient) ExecuteCommand(commandStr string) (map[string]interface{}, e
 		}, nil
 	}
 
-	// Handle special commands
+  /* Handle special commands */
 	if toolName == "list_tools" {
 		return c.ListTools()
 	}
@@ -248,11 +261,11 @@ func (c *MCPClient) ExecuteCommand(commandStr string) (map[string]interface{}, e
 		return c.ReadResource(uri)
 	}
 
-	// Call tool
+  /* Call tool */
 	return c.CallTool(toolName, arguments)
 }
 
-// ListResources lists available resources
+/* ListResources lists available resources */
 func (c *MCPClient) ListResources() (map[string]interface{}, error) {
 	request := &mcp.JSONRPCRequest{
 		JSONRPC: "2.0",
@@ -284,7 +297,7 @@ func (c *MCPClient) ListResources() (map[string]interface{}, error) {
 	return map[string]interface{}{}, nil
 }
 
-// ReadResource reads a resource by URI
+/* ReadResource reads a resource by URI */
 func (c *MCPClient) ReadResource(uri string) (map[string]interface{}, error) {
 	params := map[string]interface{}{
 		"uri": uri,
diff --git a/NeuronMCP/internal/client/commands.go b/NeuronMCP/internal/client/commands.go
index eb789a2..456d27c 100644
--- a/NeuronMCP/internal/client/commands.go
+++ b/NeuronMCP/internal/client/commands.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * commands.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/client/commands.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package client
 
 import (
@@ -6,15 +19,15 @@ import (
 	"strings"
 )
 
-// ParseCommand parses a command string into tool name and arguments
-// Format: tool_name or tool_name:arg1=val1,arg2=val2
+/* ParseCommand parses a command string into tool name and arguments */
+/* Format: tool_name or tool_name:arg1=val1,arg2=val2 */
 func ParseCommand(commandStr string) (string, map[string]interface{}, error) {
 	commandStr = strings.TrimSpace(commandStr)
 	if commandStr == "" {
 		return "", nil, fmt.Errorf("empty command")
 	}
 
-	// Check if command has arguments
+  /* Check if command has arguments */
 	parts := strings.SplitN(commandStr, ":", 2)
 	toolName := strings.TrimSpace(parts[0])
 
@@ -32,16 +45,16 @@ func ParseCommand(commandStr string) (string, map[string]interface{}, error) {
 	return toolName, arguments, nil
 }
 
-// parseArguments parses argument string into map
-// Format: arg1=val1,arg2=val2,arg3=[1,2,3]
+/* parseArguments parses argument string into map */
+/* Format: arg1=val1,arg2=val2,arg3=[1,2,3] */
 func parseArguments(argsStr string) (map[string]interface{}, error) {
 	args := make(map[string]interface{})
 	if strings.TrimSpace(argsStr) == "" {
 		return args, nil
 	}
 
-	// Simple parser for key=value pairs
-	// Handles: strings, numbers, booleans, arrays, objects
+  /* Simple parser for key=value pairs */
+  /* Handles: strings, numbers, booleans, arrays, objects */
 	var currentKey string
 	var currentValue strings.Builder
 	inString := false
@@ -107,22 +120,22 @@ func parseArguments(argsStr string) (map[string]interface{}, error) {
 		}
 	}
 
-	// Add last argument
+  /* Add last argument */
 	addArg()
 
 	return args, nil
 }
 
-// parseValue parses a value string into Go value
+/* parseValue parses a value string into Go value */
 func parseValue(valueStr string) interface{} {
 	valueStr = strings.TrimSpace(valueStr)
 
-	// None/null
+  /* None/null */
 	if valueStr == "null" || valueStr == "none" || valueStr == "None" {
 		return nil
 	}
 
-	// Boolean
+  /* Boolean */
 	if valueStr == "true" || valueStr == "True" {
 		return true
 	}
@@ -130,13 +143,13 @@ func parseValue(valueStr string) interface{} {
 		return false
 	}
 
-	// Try JSON parsing (for arrays, objects, numbers)
+  /* Try JSON parsing (for arrays, objects, numbers) */
 	var jsonValue interface{}
 	if err := json.Unmarshal([]byte(valueStr), &jsonValue); err == nil {
 		return jsonValue
 	}
 
-	// Try number parsing
+  /* Try number parsing */
 	if strings.Contains(valueStr, ".") {
 		if f, err := parseFloat(valueStr); err == nil {
 			return f
@@ -147,7 +160,7 @@ func parseValue(valueStr string) interface{} {
 		}
 	}
 
-	// Remove quotes if present
+  /* Remove quotes if present */
 	if len(valueStr) >= 2 {
 		if (valueStr[0] == '"' && valueStr[len(valueStr)-1] == '"') ||
 			(valueStr[0] == '\'' && valueStr[len(valueStr)-1] == '\'') {
@@ -155,7 +168,7 @@ func parseValue(valueStr string) interface{} {
 		}
 	}
 
-	// Return as string
+  /* Return as string */
 	return valueStr
 }
 
@@ -173,3 +186,4 @@ func parseFloat(s string) (float64, error) {
 
 
 
+
diff --git a/NeuronMCP/internal/client/config.go b/NeuronMCP/internal/client/config.go
index 8fb0963..999ea30 100644
--- a/NeuronMCP/internal/client/config.go
+++ b/NeuronMCP/internal/client/config.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * config.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/client/config.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package client
 
 import (
@@ -6,7 +19,7 @@ import (
 	"os"
 )
 
-// LoadConfig loads MCP configuration from Claude Desktop format
+/* LoadConfig loads MCP configuration from Claude Desktop format */
 func LoadConfig(configPath, serverName string) (*MCPConfig, error) {
 	data, err := os.ReadFile(configPath)
 	if err != nil {
@@ -18,7 +31,7 @@ func LoadConfig(configPath, serverName string) (*MCPConfig, error) {
 		return nil, fmt.Errorf("failed to parse configuration: %w", err)
 	}
 
-	// Claude Desktop format: { "mcpServers": { "server_name": { ... } } }
+  /* Claude Desktop format: { "mcpServers": { "server_name": { ... } } } */
 	mcpServers, ok := configData["mcpServers"].(map[string]interface{})
 	if !ok {
 		return nil, fmt.Errorf("invalid configuration format: missing 'mcpServers'")
@@ -26,7 +39,7 @@ func LoadConfig(configPath, serverName string) (*MCPConfig, error) {
 
 	serverConfig, ok := mcpServers[serverName].(map[string]interface{})
 	if !ok {
-		// List available servers
+   /* List available servers */
 		var available []string
 		for k := range mcpServers {
 			available = append(available, k)
@@ -34,13 +47,13 @@ func LoadConfig(configPath, serverName string) (*MCPConfig, error) {
 		return nil, fmt.Errorf("server '%s' not found in configuration. Available servers: %v", serverName, available)
 	}
 
-	// Extract command
+  /* Extract command */
 	command, ok := serverConfig["command"].(string)
 	if !ok {
 		return nil, fmt.Errorf("server '%s' missing 'command' field", serverName)
 	}
 
-	// Extract environment variables
+  /* Extract environment variables */
 	env := make(map[string]string)
 	if envMap, ok := serverConfig["env"].(map[string]interface{}); ok {
 		for k, v := range envMap {
@@ -50,7 +63,7 @@ func LoadConfig(configPath, serverName string) (*MCPConfig, error) {
 		}
 	}
 
-	// Extract arguments (if any)
+  /* Extract arguments (if any) */
 	var args []string
 	if argsList, ok := serverConfig["args"].([]interface{}); ok {
 		for _, arg := range argsList {
@@ -69,3 +82,4 @@ func LoadConfig(configPath, serverName string) (*MCPConfig, error) {
 
 
 
+
diff --git a/NeuronMCP/internal/client/output.go b/NeuronMCP/internal/client/output.go
index c1a2b04..c592c8e 100644
--- a/NeuronMCP/internal/client/output.go
+++ b/NeuronMCP/internal/client/output.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * output.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/client/output.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package client
 
 import (
@@ -8,21 +21,21 @@ import (
 	"time"
 )
 
-// OutputManager manages output file generation for command results
+/* OutputManager manages output file generation for command results */
 type OutputManager struct {
 	outputPath string
 	results    []ResultEntry
 	startTime  time.Time
 }
 
-// ResultEntry represents a single command result
+/* ResultEntry represents a single command result */
 type ResultEntry struct {
 	Timestamp string                 `json:"timestamp"`
 	Command   string                 `json:"command"`
 	Result    map[string]interface{} `json:"result"`
 }
 
-// NewOutputManager creates a new output manager
+/* NewOutputManager creates a new output manager */
 func NewOutputManager(outputPath string) *OutputManager {
 	return &OutputManager{
 		outputPath: outputPath,
@@ -31,7 +44,7 @@ func NewOutputManager(outputPath string) *OutputManager {
 	}
 }
 
-// AddResult adds a command result
+/* AddResult adds a command result */
 func (om *OutputManager) AddResult(command string, result map[string]interface{}) {
 	entry := ResultEntry{
 		Timestamp: time.Now().Format(time.RFC3339),
@@ -41,7 +54,7 @@ func (om *OutputManager) AddResult(command string, result map[string]interface{}
 	om.results = append(om.results, entry)
 }
 
-// Save saves results to file
+/* Save saves results to file */
 func (om *OutputManager) Save() (string, error) {
 	var outputFile string
 	if om.outputPath != "" {
@@ -51,7 +64,7 @@ func (om *OutputManager) Save() (string, error) {
 		outputFile = fmt.Sprintf("results_%s.json", timestamp)
 	}
 
-	// Ensure directory exists
+  /* Ensure directory exists */
 	dir := filepath.Dir(outputFile)
 	if dir != "" && dir != "." {
 		if err := os.MkdirAll(dir, 0755); err != nil {
@@ -59,7 +72,7 @@ func (om *OutputManager) Save() (string, error) {
 		}
 	}
 
-	// Count successful and failed commands
+  /* Count successful and failed commands */
 	successful := 0
 	failed := 0
 	for _, r := range om.results {
@@ -70,7 +83,7 @@ func (om *OutputManager) Save() (string, error) {
 		}
 	}
 
-	// Prepare output data
+  /* Prepare output data */
 	outputData := map[string]interface{}{
 		"metadata": map[string]interface{}{
 			"start_time":        om.startTime.Format(time.RFC3339),
@@ -82,7 +95,7 @@ func (om *OutputManager) Save() (string, error) {
 		"results": om.results,
 	}
 
-	// Write to file
+  /* Write to file */
 	data, err := json.MarshalIndent(outputData, "", "  ")
 	if err != nil {
 		return "", fmt.Errorf("failed to marshal output: %w", err)
@@ -97,3 +110,4 @@ func (om *OutputManager) Save() (string, error) {
 
 
 
+
diff --git a/NeuronMCP/internal/client/transport.go b/NeuronMCP/internal/client/transport.go
index 189f6b4..2fabaf1 100644
--- a/NeuronMCP/internal/client/transport.go
+++ b/NeuronMCP/internal/client/transport.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * transport.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/client/transport.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package client
 
 import (
@@ -12,7 +25,7 @@ import (
 	"github.com/neurondb/NeuronMCP/pkg/mcp"
 )
 
-// ClientTransport handles MCP communication over stdio for clients
+/* ClientTransport handles MCP communication over stdio for clients */
 type ClientTransport struct {
 	command string
 	env     map[string]string
@@ -23,7 +36,7 @@ type ClientTransport struct {
 	stderr  io.ReadCloser
 }
 
-// NewClientTransport creates a new client transport
+/* NewClientTransport creates a new client transport */
 func NewClientTransport(command string, env map[string]string, args []string) (*ClientTransport, error) {
 	return &ClientTransport{
 		command: command,
@@ -32,16 +45,16 @@ func NewClientTransport(command string, env map[string]string, args []string) (*
 	}, nil
 }
 
-// Start starts the MCP server process
+/* Start starts the MCP server process */
 func (t *ClientTransport) Start() error {
 	if t.process != nil {
 		return fmt.Errorf("transport already started")
 	}
 
-	// Build command
+  /* Build command */
 	cmd := exec.Command(t.command, t.args...)
 
-	// Set environment
+  /* Set environment */
 	var env []string
 	for _, e := range os.Environ() {
 		env = append(env, e)
@@ -51,7 +64,7 @@ func (t *ClientTransport) Start() error {
 	}
 	cmd.Env = env
 
-	// Setup stdio
+  /* Setup stdio */
 	stdin, err := cmd.StdinPipe()
 	if err != nil {
 		return fmt.Errorf("failed to create stdin pipe: %w", err)
@@ -70,7 +83,7 @@ func (t *ClientTransport) Start() error {
 	}
 	t.stderr = stderr
 
-	// Start process
+  /* Start process */
 	if err := cmd.Start(); err != nil {
 		return fmt.Errorf("failed to start process: %w", err)
 	}
@@ -79,7 +92,7 @@ func (t *ClientTransport) Start() error {
 	return nil
 }
 
-// Stop stops the MCP server process
+/* Stop stops the MCP server process */
 func (t *ClientTransport) Stop() {
 	if t.process != nil {
 		if t.stdin != nil {
@@ -93,19 +106,19 @@ func (t *ClientTransport) Stop() {
 	}
 }
 
-// SendRequest sends a request and waits for response
+/* SendRequest sends a request and waits for response */
 func (t *ClientTransport) SendRequest(request *mcp.JSONRPCRequest) (*mcp.JSONRPCResponse, error) {
 	if t.process == nil {
 		return nil, fmt.Errorf("transport not started")
 	}
 
-	// Serialize request
+  /* Serialize request */
 	requestJSON, err := json.Marshal(request)
 	if err != nil {
 		return nil, fmt.Errorf("failed to serialize request: %w", err)
 	}
 
-	// Send Content-Length header + body (standard MCP format)
+  /* Send Content-Length header + body (standard MCP format) */
 	header := fmt.Sprintf("Content-Length: %d\r\n\r\n", len(requestJSON))
 	if _, err := t.stdin.Write([]byte(header)); err != nil {
 		return nil, fmt.Errorf("failed to write header: %w", err)
@@ -114,23 +127,23 @@ func (t *ClientTransport) SendRequest(request *mcp.JSONRPCRequest) (*mcp.JSONRPC
 		return nil, fmt.Errorf("failed to write request: %w", err)
 	}
 
-	// Read response
+  /* Read response */
 	return t.readResponse(request.ID)
 }
 
-// SendNotification sends a notification (no response expected)
+/* SendNotification sends a notification (no response expected) */
 func (t *ClientTransport) SendNotification(notification map[string]interface{}) error {
 	if t.process == nil {
 		return fmt.Errorf("transport not started")
 	}
 
-	// Serialize notification
+  /* Serialize notification */
 	notificationJSON, err := json.Marshal(notification)
 	if err != nil {
 		return fmt.Errorf("failed to serialize notification: %w", err)
 	}
 
-	// Send Content-Length header + body
+  /* Send Content-Length header + body */
 	header := fmt.Sprintf("Content-Length: %d\r\n\r\n", len(notificationJSON))
 	if _, err := t.stdin.Write([]byte(header)); err != nil {
 		return fmt.Errorf("failed to write header: %w", err)
@@ -142,17 +155,17 @@ func (t *ClientTransport) SendNotification(notification map[string]interface{})
 	return nil
 }
 
-// readResponse reads a JSON-RPC response from stdout
-// Supports both Content-Length format and Claude Desktop format (JSON directly)
+/* readResponse reads a JSON-RPC response from stdout */
+/* Supports both Content-Length format and Claude Desktop format (JSON directly) */
 func (t *ClientTransport) readResponse(expectedID json.RawMessage) (*mcp.JSONRPCResponse, error) {
 	if t.process == nil {
 		return nil, fmt.Errorf("transport not started")
 	}
 
-	// Match response ID with request ID (skip notifications)
+  /* Match response ID with request ID (skip notifications) */
 	maxAttempts := 10
 	for attempt := 0; attempt < maxAttempts; attempt++ {
-		// Read first line to determine format
+   /* Read first line to determine format */
 		firstLine, err := t.stdout.ReadString('\n')
 		if err != nil {
 			return nil, fmt.Errorf("failed to read response: %w", err)
@@ -162,14 +175,14 @@ func (t *ClientTransport) readResponse(expectedID json.RawMessage) (*mcp.JSONRPC
 
 		var response mcp.JSONRPCResponse
 
-		// Claude Desktop format: JSON directly (starts with '{')
+   /* Claude Desktop format: JSON directly (starts with '{') */
 		if strings.HasPrefix(firstLine, "{") {
 			if err := json.Unmarshal([]byte(firstLine), &response); err != nil {
 				return nil, fmt.Errorf("failed to parse JSON response: %w", err)
 			}
 		} else {
-			// Standard MCP format: Content-Length headers
-			// First line is a header, continue reading headers
+    /* Standard MCP format: Content-Length headers */
+    /* First line is a header, continue reading headers */
 			headerLines := []string{firstLine}
 
 			for {
@@ -184,7 +197,7 @@ func (t *ClientTransport) readResponse(expectedID json.RawMessage) (*mcp.JSONRPC
 				headerLines = append(headerLines, line)
 			}
 
-			// Parse Content-Length
+    /* Parse Content-Length */
 			var contentLength int
 			for _, line := range headerLines {
 				lineLower := strings.ToLower(line)
@@ -202,39 +215,39 @@ func (t *ClientTransport) readResponse(expectedID json.RawMessage) (*mcp.JSONRPC
 				return nil, fmt.Errorf("missing or invalid Content-Length header")
 			}
 
-			// Read body
+    /* Read body */
 			body := make([]byte, contentLength)
 			if _, err := io.ReadFull(t.stdout, body); err != nil {
 				return nil, fmt.Errorf("failed to read response body: %w", err)
 			}
 
-			// Parse JSON response
+    /* Parse JSON response */
 			if err := json.Unmarshal(body, &response); err != nil {
 				return nil, fmt.Errorf("failed to parse JSON response: %w", err)
 			}
 		}
 
-		// Check if this is a notification (no ID) - skip it and continue
+   /* Check if this is a notification (no ID) - skip it and continue */
 		if len(response.ID) == 0 {
 			if attempt < maxAttempts-1 {
 				continue
 			}
-			// If this is the last attempt and it's a notification, return it
+    /* If this is the last attempt and it's a notification, return it */
 			return &response, nil
 		}
 
-		// Check if ID matches
+   /* Check if ID matches */
 		if string(response.ID) == string(expectedID) {
 			return &response, nil
 		}
 
-		// ID doesn't match - try reading more responses
+   /* ID doesn't match - try reading more responses */
 		if attempt < maxAttempts-1 {
 			continue
 		}
 	}
 
-	// Should not reach here, but return error if we do
+  /* Should not reach here, but return error if we do */
 	return nil, fmt.Errorf("failed to find matching response after %d attempts", maxAttempts)
 }
 
diff --git a/NeuronMCP/internal/config/config.go b/NeuronMCP/internal/config/config.go
index 38576f8..c1519b1 100644
--- a/NeuronMCP/internal/config/config.go
+++ b/NeuronMCP/internal/config/config.go
@@ -1,3 +1,19 @@
+/*-------------------------------------------------------------------------
+ *
+ * config.go
+ *    Configuration management for NeuronMCP
+ *
+ * Provides configuration loading, validation, and access for server settings,
+ * database, logging, and features.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/config/config.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package config
 
 import (
@@ -5,17 +21,17 @@ import (
 	"os"
 )
 
-// ConfigManager manages configuration loading and access
+/* ConfigManager manages configuration loading and access */
 type ConfigManager struct {
 	config *ServerConfig
 }
 
-// NewConfigManager creates a new config manager
+/* NewConfigManager creates a new config manager */
 func NewConfigManager() *ConfigManager {
 	return &ConfigManager{}
 }
 
-// Load loads configuration from file and environment
+/* Load loads configuration from file and environment */
 func (m *ConfigManager) Load(configPath string) (*ServerConfig, error) {
 	if m.config != nil {
 		return m.config, nil
@@ -23,7 +39,6 @@ func (m *ConfigManager) Load(configPath string) (*ServerConfig, error) {
 
 	loader := NewConfigLoader()
 
-	// Load from file or use defaults
 	fileConfig, err := loader.LoadFromFile(configPath)
 	if err != nil {
 		return nil, fmt.Errorf("failed to load config file: %w", err)
@@ -36,10 +51,8 @@ func (m *ConfigManager) Load(configPath string) (*ServerConfig, error) {
 		baseConfig = GetDefaultConfig()
 	}
 
-	// Merge with environment variables
 	m.config = loader.MergeWithEnv(baseConfig)
 
-	// Validate configuration
 	validator := NewConfigValidator()
 	valid, errors := validator.Validate(m.config)
 	if !valid {
@@ -53,39 +66,37 @@ func (m *ConfigManager) Load(configPath string) (*ServerConfig, error) {
 	return m.config, nil
 }
 
-// GetConfig returns the current configuration
+/* GetConfig returns the current configuration */
 func (m *ConfigManager) GetConfig() *ServerConfig {
 	if m.config == nil {
-		// Load with default path
 		if _, err := m.Load(""); err != nil {
-			// Return defaults if loading fails
 			return GetDefaultConfig()
 		}
 	}
 	return m.config
 }
 
-// GetDatabaseConfig returns database configuration
+/* GetDatabaseConfig returns database configuration */
 func (m *ConfigManager) GetDatabaseConfig() *DatabaseConfig {
 	return &m.GetConfig().Database
 }
 
-// GetServerSettings returns server settings
+/* GetServerSettings returns server settings */
 func (m *ConfigManager) GetServerSettings() *ServerSettings {
 	return &m.GetConfig().Server
 }
 
-// GetLoggingConfig returns logging configuration
+/* GetLoggingConfig returns logging configuration */
 func (m *ConfigManager) GetLoggingConfig() *LoggingConfig {
 	return &m.GetConfig().Logging
 }
 
-// GetFeaturesConfig returns features configuration
+/* GetFeaturesConfig returns features configuration */
 func (m *ConfigManager) GetFeaturesConfig() *FeaturesConfig {
 	return &m.GetConfig().Features
 }
 
-// GetPlugins returns plugin configurations
+/* GetPlugins returns plugin configurations */
 func (m *ConfigManager) GetPlugins() []PluginConfig {
 	return m.GetConfig().Plugins
 }
diff --git a/NeuronMCP/internal/config/loader.go b/NeuronMCP/internal/config/loader.go
index bdf953e..6e5f02d 100644
--- a/NeuronMCP/internal/config/loader.go
+++ b/NeuronMCP/internal/config/loader.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * loader.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/config/loader.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package config
 
 import (
@@ -8,15 +21,15 @@ import (
 	"strconv"
 )
 
-// ConfigLoader handles loading configuration from multiple sources
+/* ConfigLoader handles loading configuration from multiple sources */
 type ConfigLoader struct{}
 
-// NewConfigLoader creates a new config loader
+/* NewConfigLoader creates a new config loader */
 func NewConfigLoader() *ConfigLoader {
 	return &ConfigLoader{}
 }
 
-// GetDefaultConfig returns a default configuration
+/* GetDefaultConfig returns a default configuration */
 func GetDefaultConfig() *ServerConfig {
 	defaultLimit := 10
 	defaultChunkSize := 500
@@ -111,7 +124,7 @@ func GetDefaultConfig() *ServerConfig {
 	}
 }
 
-// LoadFromFile loads configuration from a JSON file
+/* LoadFromFile loads configuration from a JSON file */
 func (l *ConfigLoader) LoadFromFile(configPath string) (*ServerConfig, error) {
 	possiblePaths := []string{}
 
@@ -145,14 +158,14 @@ func (l *ConfigLoader) LoadFromFile(configPath string) (*ServerConfig, error) {
 		}
 	}
 
-	return nil, nil // No config file found
+ 	return nil, nil /* No config file found */
 }
 
-// MergeWithEnv merges configuration with environment variables
+/* MergeWithEnv merges configuration with environment variables */
 func (l *ConfigLoader) MergeWithEnv(config *ServerConfig) *ServerConfig {
 	merged := *config
 
-	// Database config from env
+  /* Database config from env */
 	if connStr := os.Getenv("NEURONDB_CONNECTION_STRING"); connStr != "" {
 		merged.Database.ConnectionString = &connStr
 	}
@@ -174,7 +187,7 @@ func (l *ConfigLoader) MergeWithEnv(config *ServerConfig) *ServerConfig {
 		merged.Database.Password = &pass
 	}
 
-	// Logging config from env
+  /* Logging config from env */
 	if level := os.Getenv("NEURONDB_LOG_LEVEL"); level != "" {
 		merged.Logging.Level = level
 	}
@@ -185,7 +198,7 @@ func (l *ConfigLoader) MergeWithEnv(config *ServerConfig) *ServerConfig {
 		merged.Logging.Output = &output
 	}
 
-	// Feature flags from env
+  /* Feature flags from env */
 	if gpu := os.Getenv("NEURONDB_ENABLE_GPU"); gpu != "" {
 		gpuEnabled := gpu == "true"
 		if merged.Features.ML != nil {
@@ -196,7 +209,7 @@ func (l *ConfigLoader) MergeWithEnv(config *ServerConfig) *ServerConfig {
 	return &merged
 }
 
-// Helper functions
+/* Helper functions */
 func stringPtr(s string) *string {
 	return &s
 }
diff --git a/NeuronMCP/internal/config/schema.go b/NeuronMCP/internal/config/schema.go
index 42cb76b..a7a6140 100644
--- a/NeuronMCP/internal/config/schema.go
+++ b/NeuronMCP/internal/config/schema.go
@@ -1,8 +1,21 @@
+/*-------------------------------------------------------------------------
+ *
+ * schema.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/config/schema.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package config
 
 import "time"
 
-// ServerConfig is the root configuration structure
+/* ServerConfig is the root configuration structure */
 type ServerConfig struct {
 	Database DatabaseConfig `json:"database"`
 	Server   ServerSettings `json:"server"`
@@ -12,7 +25,7 @@ type ServerConfig struct {
 	Middleware []MiddlewareConfig `json:"middleware,omitempty"`
 }
 
-// DatabaseConfig holds database connection configuration
+/* DatabaseConfig holds database connection configuration */
 type DatabaseConfig struct {
 	ConnectionString *string   `json:"connectionString,omitempty"`
 	Host             *string   `json:"host,omitempty"`
@@ -24,7 +37,7 @@ type DatabaseConfig struct {
 	SSL              interface{} `json:"ssl,omitempty"` // bool or SSLConfig
 }
 
-// PoolConfig holds connection pool settings
+/* PoolConfig holds connection pool settings */
 type PoolConfig struct {
 	Min                   *int `json:"min,omitempty"`
 	Max                   *int `json:"max,omitempty"`
@@ -32,7 +45,7 @@ type PoolConfig struct {
 	ConnectionTimeoutMillis *int `json:"connectionTimeoutMillis,omitempty"`
 }
 
-// SSLConfig holds SSL configuration
+/* SSLConfig holds SSL configuration */
 type SSLConfig struct {
 	RejectUnauthorized *bool   `json:"rejectUnauthorized,omitempty"`
 	CA                 *string `json:"ca,omitempty"`
@@ -40,7 +53,7 @@ type SSLConfig struct {
 	Key                *string `json:"key,omitempty"`
 }
 
-// ServerSettings holds server configuration
+/* ServerSettings holds server configuration */
 type ServerSettings struct {
 	Name            *string `json:"name,omitempty"`
 	Version         *string `json:"version,omitempty"`
@@ -50,7 +63,7 @@ type ServerSettings struct {
 	EnableHealthCheck *bool `json:"enableHealthCheck,omitempty"`
 }
 
-// LoggingConfig holds logging configuration
+/* LoggingConfig holds logging configuration */
 type LoggingConfig struct {
 	Level              string  `json:"level"`
 	Format             string  `json:"format"`
@@ -60,7 +73,7 @@ type LoggingConfig struct {
 	EnableErrorStack      *bool `json:"enableErrorStack,omitempty"`
 }
 
-// FeaturesConfig holds feature flags and settings
+/* FeaturesConfig holds feature flags and settings */
 type FeaturesConfig struct {
 	Vector        *VectorFeatureConfig        `json:"vector,omitempty"`
 	ML            *MLFeatureConfig            `json:"ml,omitempty"`
@@ -76,7 +89,7 @@ type FeaturesConfig struct {
 	Indexing      *IndexingFeatureConfig      `json:"indexing,omitempty"`
 }
 
-// VectorFeatureConfig holds vector feature settings
+/* VectorFeatureConfig holds vector feature settings */
 type VectorFeatureConfig struct {
 	Enabled             bool    `json:"enabled"`
 	DefaultDistanceMetric *string `json:"defaultDistanceMetric,omitempty"`
@@ -84,7 +97,7 @@ type VectorFeatureConfig struct {
 	DefaultLimit          *int    `json:"defaultLimit,omitempty"`
 }
 
-// MLFeatureConfig holds ML feature settings
+/* MLFeatureConfig holds ML feature settings */
 type MLFeatureConfig struct {
 	Enabled        bool     `json:"enabled"`
 	Algorithms     []string `json:"algorithms,omitempty"`
@@ -92,70 +105,70 @@ type MLFeatureConfig struct {
 	GPUEnabled     *bool    `json:"gpuEnabled,omitempty"`
 }
 
-// AnalyticsFeatureConfig holds analytics feature settings
+/* AnalyticsFeatureConfig holds analytics feature settings */
 type AnalyticsFeatureConfig struct {
 	Enabled      bool `json:"enabled"`
 	MaxClusters  *int `json:"maxClusters,omitempty"`
 	MaxIterations *int `json:"maxIterations,omitempty"`
 }
 
-// RAGFeatureConfig holds RAG feature settings
+/* RAGFeatureConfig holds RAG feature settings */
 type RAGFeatureConfig struct {
 	Enabled        bool `json:"enabled"`
 	DefaultChunkSize *int `json:"defaultChunkSize,omitempty"`
 	DefaultOverlap   *int `json:"defaultOverlap,omitempty"`
 }
 
-// ProjectsFeatureConfig holds projects feature settings
+/* ProjectsFeatureConfig holds projects feature settings */
 type ProjectsFeatureConfig struct {
 	Enabled    bool `json:"enabled"`
 	MaxProjects *int `json:"maxProjects,omitempty"`
 }
 
-// GPUFeatureConfig holds GPU feature settings
+/* GPUFeatureConfig holds GPU feature settings */
 type GPUFeatureConfig struct {
 	Enabled  bool `json:"enabled"`
 	DeviceID *int `json:"deviceId,omitempty"`
 }
 
-// QuantizationFeatureConfig holds quantization feature settings
+/* QuantizationFeatureConfig holds quantization feature settings */
 type QuantizationFeatureConfig struct {
 	Enabled      bool    `json:"enabled"`
 	DefaultMethod *string `json:"defaultMethod,omitempty"`
 }
 
-// DimensionalityFeatureConfig holds dimensionality feature settings
+/* DimensionalityFeatureConfig holds dimensionality feature settings */
 type DimensionalityFeatureConfig struct {
 	Enabled      bool `json:"enabled"`
 	MaxComponents *int `json:"maxComponents,omitempty"`
 }
 
-// RerankingFeatureConfig holds reranking feature settings
+/* RerankingFeatureConfig holds reranking feature settings */
 type RerankingFeatureConfig struct {
 	Enabled     bool     `json:"enabled"`
 	DefaultModel *string `json:"defaultModel,omitempty"`
 }
 
-// HybridFeatureConfig holds hybrid search feature settings
+/* HybridFeatureConfig holds hybrid search feature settings */
 type HybridFeatureConfig struct {
 	Enabled          bool    `json:"enabled"`
 	DefaultVectorWeight *float64 `json:"defaultVectorWeight,omitempty"`
 }
 
-// WorkersFeatureConfig holds workers feature settings
+/* WorkersFeatureConfig holds workers feature settings */
 type WorkersFeatureConfig struct {
 	Enabled   bool `json:"enabled"`
 	MaxWorkers *int `json:"maxWorkers,omitempty"`
 }
 
-// IndexingFeatureConfig holds indexing feature settings
+/* IndexingFeatureConfig holds indexing feature settings */
 type IndexingFeatureConfig struct {
 	Enabled              bool `json:"enabled"`
 	DefaultHNSWM        *int `json:"defaultHNSWM,omitempty"`
 	DefaultHNSWEFConstruction *int `json:"defaultHNSWEFConstruction,omitempty"`
 }
 
-// PluginConfig holds plugin configuration
+/* PluginConfig holds plugin configuration */
 type PluginConfig struct {
 	Name     string                 `json:"name"`
 	Enabled  bool                   `json:"enabled"`
@@ -163,7 +176,7 @@ type PluginConfig struct {
 	Config   map[string]interface{} `json:"config,omitempty"`
 }
 
-// MiddlewareConfig holds middleware configuration
+/* MiddlewareConfig holds middleware configuration */
 type MiddlewareConfig struct {
 	Name     string                 `json:"name"`
 	Enabled  bool                   `json:"enabled"`
@@ -171,7 +184,7 @@ type MiddlewareConfig struct {
 	Config   map[string]interface{} `json:"config,omitempty"`
 }
 
-// Helper methods for getting values with defaults
+/* Helper methods for getting values with defaults */
 
 func (c *DatabaseConfig) GetHost() string {
 	if c.Host != nil {
diff --git a/NeuronMCP/internal/config/validator.go b/NeuronMCP/internal/config/validator.go
index 8f95290..ae35a96 100644
--- a/NeuronMCP/internal/config/validator.go
+++ b/NeuronMCP/internal/config/validator.go
@@ -1,29 +1,42 @@
+/*-------------------------------------------------------------------------
+ *
+ * validator.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/config/validator.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package config
 
 import "fmt"
 
-// ConfigValidator validates configuration
+/* ConfigValidator validates configuration */
 type ConfigValidator struct{}
 
-// NewConfigValidator creates a new config validator
+/* NewConfigValidator creates a new config validator */
 func NewConfigValidator() *ConfigValidator {
 	return &ConfigValidator{}
 }
 
-// Validate validates the complete server configuration
+/* Validate validates the complete server configuration */
 func (v *ConfigValidator) Validate(config *ServerConfig) (bool, []string) {
 	var errors []string
 
-	// Validate database config
+  /* Validate database config */
 	errors = append(errors, v.validateDatabase(&config.Database)...)
 
-	// Validate server settings
+  /* Validate server settings */
 	errors = append(errors, v.validateServer(&config.Server)...)
 
-	// Validate logging
+  /* Validate logging */
 	errors = append(errors, v.validateLogging(&config.Logging)...)
 
-	// Validate features
+  /* Validate features */
 	errors = append(errors, v.validateFeatures(&config.Features)...)
 
 	return len(errors) == 0, errors
diff --git a/NeuronMCP/internal/database/connection.go b/NeuronMCP/internal/database/connection.go
index e151fc3..42ce7f5 100644
--- a/NeuronMCP/internal/database/connection.go
+++ b/NeuronMCP/internal/database/connection.go
@@ -1,3 +1,19 @@
+/*-------------------------------------------------------------------------
+ *
+ * connection.go
+ *    Database connection management for NeuronMCP
+ *
+ * Provides PostgreSQL connection pooling, retry logic, and connection
+ * management with NeuronDB type registration.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/database/connection.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package database
 
 import (
@@ -12,7 +28,7 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/config"
 )
 
-// Database manages PostgreSQL connections
+/* Database manages PostgreSQL connections */
 type Database struct {
 	pool     *pgxpool.Pool
 	host     string
@@ -21,17 +37,17 @@ type Database struct {
 	user     string
 }
 
-// NewDatabase creates a new database instance
+/* NewDatabase creates a new database instance */
 func NewDatabase() *Database {
 	return &Database{}
 }
 
-// Connect connects to the database using the provided configuration
+/* Connect connects to the database using the provided configuration */
 func (d *Database) Connect(cfg *config.DatabaseConfig) error {
 	return d.ConnectWithRetry(cfg, 3, 2*time.Second)
 }
 
-// ConnectWithRetry connects to the database with retry logic
+/* ConnectWithRetry connects to the database with retry logic */
 func (d *Database) ConnectWithRetry(cfg *config.DatabaseConfig, maxRetries int, retryDelay time.Duration) error {
 	var connStr string
 	var err error
@@ -39,7 +55,6 @@ func (d *Database) ConnectWithRetry(cfg *config.DatabaseConfig, maxRetries int,
 	if cfg.ConnectionString != nil && *cfg.ConnectionString != "" {
 		connStr = *cfg.ConnectionString
 	} else {
-		// Build connection string from components
 		host := cfg.GetHost()
 		port := cfg.GetPort()
 		db := cfg.GetDatabase()
@@ -49,7 +64,6 @@ func (d *Database) ConnectWithRetry(cfg *config.DatabaseConfig, maxRetries int,
 			password = *cfg.Password
 		}
 
-		// Build connection string properly
 		connStr = fmt.Sprintf("host=%s port=%d user=%s dbname=%s",
 			host, port, user, db)
 		
@@ -57,7 +71,6 @@ func (d *Database) ConnectWithRetry(cfg *config.DatabaseConfig, maxRetries int,
 			connStr += fmt.Sprintf(" password=%s", password)
 		}
 
-		// Add SSL if configured
 		if cfg.SSL != nil {
 			if sslBool, ok := cfg.SSL.(bool); ok {
 				if sslBool {
@@ -69,12 +82,10 @@ func (d *Database) ConnectWithRetry(cfg *config.DatabaseConfig, maxRetries int,
 				connStr += fmt.Sprintf(" sslmode=%s", sslStr)
 			}
 		} else {
-			// Default to prefer SSL
 			connStr += " sslmode=prefer"
 		}
 	}
 
-	// Parse connection string
 	poolConfig, err := pgxpool.ParseConfig(connStr)
 	if err != nil {
 		host := cfg.GetHost()
@@ -84,18 +95,13 @@ func (d *Database) ConnectWithRetry(cfg *config.DatabaseConfig, maxRetries int,
 		return fmt.Errorf("failed to parse connection string for database '%s' on host '%s:%d' as user '%s': %w (connection string format may be invalid)", db, host, port, user, err)
 	}
 
-	// Register NeuronDB custom types (vector, vector[], etc.)
-	// These OIDs are from NeuronDB extension
-	// Note: We cast to text in queries for compatibility, but register types for future use
+	/* Register NeuronDB custom types (vector, vector[], etc.) */
 	poolConfig.AfterConnect = func(ctx context.Context, conn *pgx.Conn) error {
-		// Register vector type (OID 17648) as text for scanning
-		// This allows pgx to handle vector types by treating them as text
 		conn.TypeMap().RegisterType(&pgtype.Type{
 			Codec: &pgtype.TextCodec{},
 			Name:  "vector",
 			OID:   17648,
 		})
-		// Register vector[] type (OID 17656) as text array
 		conn.TypeMap().RegisterType(&pgtype.Type{
 			Codec: &pgtype.ArrayCodec{ElementType: &pgtype.Type{Name: "text", Codec: &pgtype.TextCodec{}}},
 			Name:  "_vector",
@@ -104,7 +110,6 @@ func (d *Database) ConnectWithRetry(cfg *config.DatabaseConfig, maxRetries int,
 		return nil
 	}
 
-	// Apply pool settings
 	if cfg.Pool != nil {
 		poolConfig.MinConns = int32(cfg.Pool.GetMin())
 		poolConfig.MaxConns = int32(cfg.Pool.GetMax())
@@ -112,17 +117,14 @@ func (d *Database) ConnectWithRetry(cfg *config.DatabaseConfig, maxRetries int,
 		poolConfig.MaxConnLifetime = time.Hour
 		poolConfig.HealthCheckPeriod = 1 * time.Minute
 	} else {
-		// Set defaults to prevent immediate connection attempts
-		poolConfig.MinConns = 0  // Don't create connections until needed
+		poolConfig.MinConns = 0
 		poolConfig.MaxConns = 10
 		poolConfig.HealthCheckPeriod = 1 * time.Minute
 	}
 
-	// Store connection info for error messages
 	var host, dbName, dbUser string
 	var dbPort int
 	if cfg.ConnectionString != nil && *cfg.ConnectionString != "" {
-		// Try to extract info from connection string if possible
 		host = "unknown"
 		dbName = "unknown"
 		dbUser = "unknown"
@@ -138,13 +140,11 @@ func (d *Database) ConnectWithRetry(cfg *config.DatabaseConfig, maxRetries int,
 	d.database = dbName
 	d.user = dbUser
 
-	// Retry connection
 	var pool *pgxpool.Pool
 	var lastErr error
 	for attempt := 0; attempt < maxRetries; attempt++ {
 		pool, err = pgxpool.NewWithConfig(context.Background(), poolConfig)
 		if err == nil {
-			// Test the connection
 			ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
 			defer cancel()
 			if err := pool.Ping(ctx); err == nil {
@@ -159,27 +159,25 @@ func (d *Database) ConnectWithRetry(cfg *config.DatabaseConfig, maxRetries int,
 
 		if attempt < maxRetries-1 {
 			time.Sleep(retryDelay)
-			retryDelay *= 2 // Exponential backoff
+			retryDelay *= 2
 		}
 	}
 
 	return fmt.Errorf("failed to connect to database '%s' on host '%s:%d' as user '%s' after %d attempts (last error: %v)", dbName, host, dbPort, dbUser, maxRetries, lastErr)
 }
 
-// IsConnected checks if the database is connected
+/* IsConnected checks if the database is connected */
 func (d *Database) IsConnected() bool {
 	return d.pool != nil
 }
 
-// Query executes a query and returns rows with automatic reconnection
+/* Query executes a query and returns rows with automatic reconnection */
 func (d *Database) Query(ctx context.Context, query string, args ...interface{}) (pgx.Rows, error) {
 	if d.pool == nil {
 		return nil, fmt.Errorf("database connection not established: database '%s' on host '%s:%d' as user '%s' (connection pool is nil, ensure Connect() was called successfully)", d.database, d.host, d.port, d.user)
 	}
 	
-	// Check if pool is healthy
 	if err := d.pool.Ping(ctx); err != nil {
-		// Connection lost - try to reconnect
 		return nil, fmt.Errorf("database connection lost: database '%s' on host '%s:%d' as user '%s': %w (connection pool ping failed, may need to reconnect)", d.database, d.host, d.port, d.user, err)
 	}
 	
@@ -190,16 +188,15 @@ func (d *Database) Query(ctx context.Context, query string, args ...interface{})
 	return rows, nil
 }
 
-// QueryRow executes a query and returns a single row
+/* QueryRow executes a query and returns a single row */
 func (d *Database) QueryRow(ctx context.Context, query string, args ...interface{}) pgx.Row {
 	if d.pool == nil {
-		// Return a row that will error on scan
 		return &errorRow{err: fmt.Errorf("database connection not established: database '%s' on host '%s:%d' as user '%s' (connection pool is nil, ensure Connect() was called successfully)", d.database, d.host, d.port, d.user)}
 	}
 	return d.pool.QueryRow(ctx, query, args...)
 }
 
-// Exec executes a query without returning rows
+/* Exec executes a query without returning rows */
 func (d *Database) Exec(ctx context.Context, query string, args ...interface{}) (pgconn.CommandTag, error) {
 	if d.pool == nil {
 		return pgconn.CommandTag{}, fmt.Errorf("database connection not established: database '%s' on host '%s:%d' as user '%s' (connection pool is nil, ensure Connect() was called successfully)", d.database, d.host, d.port, d.user)
@@ -211,7 +208,7 @@ func (d *Database) Exec(ctx context.Context, query string, args ...interface{})
 	return tag, nil
 }
 
-// Begin starts a transaction
+/* Begin starts a transaction */
 func (d *Database) Begin(ctx context.Context) (pgx.Tx, error) {
 	if d.pool == nil {
 		return nil, fmt.Errorf("database connection not established: database '%s' on host '%s:%d' as user '%s' (connection pool is nil, ensure Connect() was called successfully)", d.database, d.host, d.port, d.user)
@@ -223,14 +220,14 @@ func (d *Database) Begin(ctx context.Context) (pgx.Tx, error) {
 	return tx, nil
 }
 
-// Close closes the connection pool
+/* Close closes the connection pool */
 func (d *Database) Close() {
 	if d.pool != nil {
 		d.pool.Close()
 	}
 }
 
-// TestConnection tests the database connection
+/* TestConnection tests the database connection */
 func (d *Database) TestConnection(ctx context.Context) error {
 	if d.pool == nil {
 		return fmt.Errorf("database connection not established: database '%s' on host '%s:%d' as user '%s' (connection pool is nil, ensure Connect() was called successfully)", d.database, d.host, d.port, d.user)
@@ -242,7 +239,7 @@ func (d *Database) TestConnection(ctx context.Context) error {
 	return nil
 }
 
-// GetPoolStats returns pool statistics
+/* GetPoolStats returns pool statistics */
 func (d *Database) GetPoolStats() *PoolStats {
 	if d.pool == nil {
 		return nil
@@ -256,7 +253,7 @@ func (d *Database) GetPoolStats() *PoolStats {
 	}
 }
 
-// PoolStats holds connection pool statistics
+/* PoolStats holds connection pool statistics */
 type PoolStats struct {
 	TotalConns      int32
 	AcquiredConns   int32
@@ -264,13 +261,12 @@ type PoolStats struct {
 	ConstructingConns int32
 }
 
-// EscapeIdentifier escapes a SQL identifier
+/* EscapeIdentifier escapes a SQL identifier */
 func EscapeIdentifier(identifier string) string {
-	// Simple escaping - in production, use pgx's built-in escaping
 	return fmt.Sprintf(`"%s"`, identifier)
 }
 
-// errorRow is a row that always returns an error
+/* errorRow is a row that always returns an error */
 type errorRow struct {
 	err error
 }
diff --git a/NeuronMCP/internal/database/query.go b/NeuronMCP/internal/database/query.go
index 3fde328..31c3b28 100644
--- a/NeuronMCP/internal/database/query.go
+++ b/NeuronMCP/internal/database/query.go
@@ -1,3 +1,19 @@
+/*-------------------------------------------------------------------------
+ *
+ * query.go
+ *    SQL query builder for NeuronMCP
+ *
+ * Provides utilities for building SQL queries including SELECT statements
+ * and vector search queries.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/database/query.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package database
 
 import (
@@ -5,10 +21,10 @@ import (
 	"strings"
 )
 
-// QueryBuilder provides utilities for building SQL queries
+/* QueryBuilder provides utilities for building SQL queries */
 type QueryBuilder struct{}
 
-// Select builds a SELECT query
+/* Select builds a SELECT query */
 func (qb *QueryBuilder) Select(table string, columns []string, where map[string]interface{}, orderBy *OrderBy, limit, offset *int) (string, []interface{}) {
 	if len(columns) == 0 {
 		columns = []string{"*"}
@@ -17,13 +33,9 @@ func (qb *QueryBuilder) Select(table string, columns []string, where map[string]
 	var params []interface{}
 	paramIndex := 1
 
-	// SELECT clause
 	selectClause := strings.Join(columns, ", ")
-
-	// FROM clause
 	fromClause := EscapeIdentifier(table)
 
-	// WHERE clause
 	var whereClause string
 	if len(where) > 0 {
 		var conditions []string
@@ -36,13 +48,11 @@ func (qb *QueryBuilder) Select(table string, columns []string, where map[string]
 		whereClause = "WHERE " + strings.Join(conditions, " AND ")
 	}
 
-	// ORDER BY clause
 	var orderByClause string
 	if orderBy != nil {
 		orderByClause = fmt.Sprintf("ORDER BY %s %s", EscapeIdentifier(orderBy.Column), orderBy.Direction)
 	}
 
-	// LIMIT clause
 	var limitClause string
 	if limit != nil {
 		limitClause = fmt.Sprintf("LIMIT $%d", paramIndex)
@@ -50,7 +60,6 @@ func (qb *QueryBuilder) Select(table string, columns []string, where map[string]
 		paramIndex++
 	}
 
-	// OFFSET clause
 	var offsetClause string
 	if offset != nil {
 		offsetClause = fmt.Sprintf("OFFSET $%d", paramIndex)
@@ -77,23 +86,21 @@ func (qb *QueryBuilder) Select(table string, columns []string, where map[string]
 	return query, params
 }
 
-// OrderBy represents an ORDER BY clause
+/* OrderBy represents an ORDER BY clause */
 type OrderBy struct {
 	Column    string
-	Direction string // ASC or DESC
+	Direction string
 }
 
-// VectorSearch builds a vector search query
+/* VectorSearch builds a vector search query */
 func (qb *QueryBuilder) VectorSearch(table, vectorColumn string, queryVector []float32, distanceMetric string, limit int, additionalColumns []string, minkowskiP *float64) (string, []interface{}) {
 	if len(queryVector) == 0 {
-		// Return error query - caller should handle this
 		return "", nil
 	}
 
 	var params []interface{}
 	paramIndex := 1
 
-	// Convert vector to string format for PostgreSQL
 	vectorStr := formatVector(queryVector)
 	params = append(params, vectorStr)
 	vectorParamIndex := paramIndex
@@ -121,29 +128,24 @@ func (qb *QueryBuilder) VectorSearch(table, vectorColumn string, queryVector []f
 		pParamIndex := paramIndex
 		paramIndex++
 		distanceExpr = fmt.Sprintf("vector_minkowski_distance(%s, $%d::vector, $%d::double precision) AS distance", EscapeIdentifier(vectorColumn), vectorParamIndex, pParamIndex)
-	default: // l2
+	default:
 		distanceExpr = fmt.Sprintf("%s <-> $%d::vector AS distance", EscapeIdentifier(vectorColumn), vectorParamIndex)
 	}
 
-	// Build SELECT columns
 	selectColumns := []string{}
 	if len(additionalColumns) > 0 {
 		for _, col := range additionalColumns {
 			selectColumns = append(selectColumns, EscapeIdentifier(col))
 		}
-		// Always include the vector column and distance
 		selectColumns = append(selectColumns, EscapeIdentifier(vectorColumn))
 	} else {
-		// If no additional columns, select all
 		selectColumns = append(selectColumns, "*")
 	}
 	selectColumns = append(selectColumns, distanceExpr)
 
-	// Add limit parameter
 	params = append(params, limit)
 	limitParamIndex := paramIndex
 
-	// Build the query
 	selectClause := strings.Join(selectColumns, ", ")
 	query := fmt.Sprintf(
 		"SELECT %s FROM %s ORDER BY distance ASC LIMIT $%d",
@@ -155,7 +157,7 @@ func (qb *QueryBuilder) VectorSearch(table, vectorColumn string, queryVector []f
 	return query, params
 }
 
-// formatVector formats a float32 slice as a PostgreSQL vector string
+/* formatVector formats a float32 slice as a PostgreSQL vector string */
 func formatVector(vec []float32) string {
 	var parts []string
 	for _, v := range vec {
diff --git a/NeuronMCP/internal/logging/logger.go b/NeuronMCP/internal/logging/logger.go
index d83720d..6d56198 100644
--- a/NeuronMCP/internal/logging/logger.go
+++ b/NeuronMCP/internal/logging/logger.go
@@ -1,3 +1,19 @@
+/*-------------------------------------------------------------------------
+ *
+ * logger.go
+ *    Structured logging for NeuronMCP
+ *
+ * Provides structured logging functionality using zerolog with configurable
+ * levels, formats, and outputs.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/logging/logger.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package logging
 
 import (
@@ -9,15 +25,14 @@ import (
 	"github.com/rs/zerolog"
 )
 
-// Logger provides structured logging
+/* Logger provides structured logging */
 type Logger struct {
 	logger zerolog.Logger
 	level  zerolog.Level
 }
 
-// NewLogger creates a new logger
+/* NewLogger creates a new logger */
 func NewLogger(cfg *config.LoggingConfig) *Logger {
-	// Parse level
 	var level zerolog.Level
 	switch cfg.Level {
 	case "debug":
@@ -32,7 +47,6 @@ func NewLogger(cfg *config.LoggingConfig) *Logger {
 		level = zerolog.InfoLevel
 	}
 
-	// Determine output
 	var output io.Writer
 	if cfg.Output != nil {
 		switch *cfg.Output {
@@ -41,7 +55,6 @@ func NewLogger(cfg *config.LoggingConfig) *Logger {
 		case "stderr":
 			output = os.Stderr
 		default:
-			// Try to open as file
 			if file, err := os.OpenFile(*cfg.Output, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644); err == nil {
 				output = file
 			} else {
@@ -52,7 +65,6 @@ func NewLogger(cfg *config.LoggingConfig) *Logger {
 		output = os.Stderr
 	}
 
-	// Configure format
 	if cfg.Format == "json" {
 		zerolog.TimeFieldFormat = zerolog.TimeFormatUnix
 	} else {
@@ -67,22 +79,22 @@ func NewLogger(cfg *config.LoggingConfig) *Logger {
 	}
 }
 
-// Debug logs a debug message
+/* Debug logs a debug message */
 func (l *Logger) Debug(message string, metadata map[string]interface{}) {
 	l.log(zerolog.DebugLevel, message, metadata)
 }
 
-// Info logs an info message
+/* Info logs an info message */
 func (l *Logger) Info(message string, metadata map[string]interface{}) {
 	l.log(zerolog.InfoLevel, message, metadata)
 }
 
-// Warn logs a warning message
+/* Warn logs a warning message */
 func (l *Logger) Warn(message string, metadata map[string]interface{}) {
 	l.log(zerolog.WarnLevel, message, metadata)
 }
 
-// Error logs an error message
+/* Error logs an error message */
 func (l *Logger) Error(message string, err error, metadata map[string]interface{}) {
 	event := l.logger.Error()
 	if err != nil {
@@ -106,7 +118,7 @@ func (l *Logger) log(level zerolog.Level, message string, metadata map[string]in
 	event.Msg(message)
 }
 
-// Child creates a child logger with additional metadata
+/* Child creates a child logger with additional metadata */
 func (l *Logger) Child(metadata map[string]interface{}) *Logger {
 	childLogger := l.logger.With().Fields(metadata).Logger()
 	return &Logger{
diff --git a/NeuronMCP/internal/middleware/builtin/error.go b/NeuronMCP/internal/middleware/builtin/error.go
index 35dcbd3..9080b84 100644
--- a/NeuronMCP/internal/middleware/builtin/error.go
+++ b/NeuronMCP/internal/middleware/builtin/error.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * error.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/middleware/builtin/error.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package builtin
 
 import (
@@ -7,13 +20,13 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/middleware"
 )
 
-// ErrorHandlingMiddleware handles errors
+/* ErrorHandlingMiddleware handles errors */
 type ErrorHandlingMiddleware struct {
 	logger          *logging.Logger
 	enableErrorStack bool
 }
 
-// NewErrorHandlingMiddleware creates a new error handling middleware
+/* NewErrorHandlingMiddleware creates a new error handling middleware */
 func NewErrorHandlingMiddleware(logger *logging.Logger, enableStack bool) *ErrorHandlingMiddleware {
 	return &ErrorHandlingMiddleware{
 		logger:            logger,
@@ -21,29 +34,29 @@ func NewErrorHandlingMiddleware(logger *logging.Logger, enableStack bool) *Error
 	}
 }
 
-// Name returns the middleware name
+/* Name returns the middleware name */
 func (m *ErrorHandlingMiddleware) Name() string {
 	return "error-handling"
 }
 
-// Order returns the execution order
+/* Order returns the execution order */
 func (m *ErrorHandlingMiddleware) Order() int {
 	return 100
 }
 
-// Enabled returns whether the middleware is enabled
+/* Enabled returns whether the middleware is enabled */
 func (m *ErrorHandlingMiddleware) Enabled() bool {
 	return true
 }
 
-// Execute executes the middleware
+/* Execute executes the middleware */
 func (m *ErrorHandlingMiddleware) Execute(ctx context.Context, req *middleware.MCPRequest, next middleware.Handler) (*middleware.MCPResponse, error) {
 	resp, err := next(ctx)
 	if err != nil {
 		errorMsg := err.Error()
 		var stack string
 		if m.enableErrorStack {
-			// In Go, we'd need to get stack trace differently
+    /* In Go, we'd need to get stack trace differently */
 			stack = ""
 		}
 
diff --git a/NeuronMCP/internal/middleware/builtin/logging.go b/NeuronMCP/internal/middleware/builtin/logging.go
index 3796681..a9e67ac 100644
--- a/NeuronMCP/internal/middleware/builtin/logging.go
+++ b/NeuronMCP/internal/middleware/builtin/logging.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * logging.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/middleware/builtin/logging.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package builtin
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/middleware"
 )
 
-// LoggingMiddleware logs requests and responses
+/* LoggingMiddleware logs requests and responses */
 type LoggingMiddleware struct {
 	logger              *logging.Logger
 	enableRequestLogging  bool
 	enableResponseLogging bool
 }
 
-// NewLoggingMiddleware creates a new logging middleware
+/* NewLoggingMiddleware creates a new logging middleware */
 func NewLoggingMiddleware(logger *logging.Logger, enableRequest, enableResponse bool) *LoggingMiddleware {
 	return &LoggingMiddleware{
 		logger:                logger,
@@ -24,22 +37,22 @@ func NewLoggingMiddleware(logger *logging.Logger, enableRequest, enableResponse
 	}
 }
 
-// Name returns the middleware name
+/* Name returns the middleware name */
 func (m *LoggingMiddleware) Name() string {
 	return "logging"
 }
 
-// Order returns the execution order
+/* Order returns the execution order */
 func (m *LoggingMiddleware) Order() int {
 	return 2
 }
 
-// Enabled returns whether the middleware is enabled
+/* Enabled returns whether the middleware is enabled */
 func (m *LoggingMiddleware) Enabled() bool {
 	return true
 }
 
-// Execute executes the middleware
+/* Execute executes the middleware */
 func (m *LoggingMiddleware) Execute(ctx context.Context, req *middleware.MCPRequest, next middleware.Handler) (*middleware.MCPResponse, error) {
 	start := time.Now()
 
diff --git a/NeuronMCP/internal/middleware/builtin/timeout.go b/NeuronMCP/internal/middleware/builtin/timeout.go
index b31c44e..e54eea2 100644
--- a/NeuronMCP/internal/middleware/builtin/timeout.go
+++ b/NeuronMCP/internal/middleware/builtin/timeout.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * timeout.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/middleware/builtin/timeout.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package builtin
 
 import (
@@ -9,13 +22,13 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/middleware"
 )
 
-// TimeoutMiddleware adds timeout to requests
+/* TimeoutMiddleware adds timeout to requests */
 type TimeoutMiddleware struct {
 	timeout time.Duration
 	logger  *logging.Logger
 }
 
-// NewTimeoutMiddleware creates a new timeout middleware
+/* NewTimeoutMiddleware creates a new timeout middleware */
 func NewTimeoutMiddleware(timeout time.Duration, logger *logging.Logger) *TimeoutMiddleware {
 	return &TimeoutMiddleware{
 		timeout: timeout,
@@ -23,22 +36,22 @@ func NewTimeoutMiddleware(timeout time.Duration, logger *logging.Logger) *Timeou
 	}
 }
 
-// Name returns the middleware name
+/* Name returns the middleware name */
 func (m *TimeoutMiddleware) Name() string {
 	return "timeout"
 }
 
-// Order returns the execution order
+/* Order returns the execution order */
 func (m *TimeoutMiddleware) Order() int {
 	return 3
 }
 
-// Enabled returns whether the middleware is enabled
+/* Enabled returns whether the middleware is enabled */
 func (m *TimeoutMiddleware) Enabled() bool {
 	return true
 }
 
-// Execute executes the middleware
+/* Execute executes the middleware */
 func (m *TimeoutMiddleware) Execute(ctx context.Context, req *middleware.MCPRequest, next middleware.Handler) (*middleware.MCPResponse, error) {
 	ctx, cancel := context.WithTimeout(ctx, m.timeout)
 	defer cancel()
diff --git a/NeuronMCP/internal/middleware/builtin/validation.go b/NeuronMCP/internal/middleware/builtin/validation.go
index 35e5d67..4054d0a 100644
--- a/NeuronMCP/internal/middleware/builtin/validation.go
+++ b/NeuronMCP/internal/middleware/builtin/validation.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * validation.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/middleware/builtin/validation.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package builtin
 
 import (
@@ -6,30 +19,30 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/middleware"
 )
 
-// ValidationMiddleware validates requests
+/* ValidationMiddleware validates requests */
 type ValidationMiddleware struct{}
 
-// NewValidationMiddleware creates a new validation middleware
+/* NewValidationMiddleware creates a new validation middleware */
 func NewValidationMiddleware() *ValidationMiddleware {
 	return &ValidationMiddleware{}
 }
 
-// Name returns the middleware name
+/* Name returns the middleware name */
 func (m *ValidationMiddleware) Name() string {
 	return "validation"
 }
 
-// Order returns the execution order
+/* Order returns the execution order */
 func (m *ValidationMiddleware) Order() int {
 	return 1
 }
 
-// Enabled returns whether the middleware is enabled
+/* Enabled returns whether the middleware is enabled */
 func (m *ValidationMiddleware) Enabled() bool {
 	return true
 }
 
-// Execute executes the middleware
+/* Execute executes the middleware */
 func (m *ValidationMiddleware) Execute(ctx context.Context, req *middleware.MCPRequest, next middleware.Handler) (*middleware.MCPResponse, error) {
 	if req.Method == "" {
 		return &middleware.MCPResponse{
@@ -41,7 +54,7 @@ func (m *ValidationMiddleware) Execute(ctx context.Context, req *middleware.MCPR
 	}
 
 	if req.Params != nil {
-		// Params should be a map, which is already validated by type
+   /* Params should be a map, which is already validated by type */
 	}
 
 	return next(ctx)
diff --git a/NeuronMCP/internal/middleware/chain.go b/NeuronMCP/internal/middleware/chain.go
index 7b3da7c..e355a46 100644
--- a/NeuronMCP/internal/middleware/chain.go
+++ b/NeuronMCP/internal/middleware/chain.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * chain.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/middleware/chain.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package middleware
 
 import (
@@ -5,14 +18,14 @@ import (
 	"sort"
 )
 
-// Chain executes middleware in order
+/* Chain executes middleware in order */
 type Chain struct {
 	middlewares []Middleware
 }
 
-// NewChain creates a new middleware chain
+/* NewChain creates a new middleware chain */
 func NewChain(middlewares []Middleware) *Chain {
-	// Sort by order
+  /* Sort by order */
 	sorted := make([]Middleware, len(middlewares))
 	copy(sorted, middlewares)
 	sort.Slice(sorted, func(i, j int) bool {
@@ -22,9 +35,9 @@ func NewChain(middlewares []Middleware) *Chain {
 	return &Chain{middlewares: sorted}
 }
 
-// Execute executes the middleware chain
+/* Execute executes the middleware chain */
 func (c *Chain) Execute(ctx context.Context, req *MCPRequest, finalHandler Handler) (*MCPResponse, error) {
-	// Filter enabled middlewares
+  /* Filter enabled middlewares */
 	enabled := make([]Middleware, 0)
 	for _, mw := range c.middlewares {
 		if mw.Enabled() {
@@ -32,7 +45,7 @@ func (c *Chain) Execute(ctx context.Context, req *MCPRequest, finalHandler Handl
 		}
 	}
 
-	// Build chain
+  /* Build chain */
 	index := 0
 	var next Handler
 	next = func(ctx context.Context) (*MCPResponse, error) {
diff --git a/NeuronMCP/internal/middleware/manager.go b/NeuronMCP/internal/middleware/manager.go
index 05bf18d..5462e1e 100644
--- a/NeuronMCP/internal/middleware/manager.go
+++ b/NeuronMCP/internal/middleware/manager.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * manager.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/middleware/manager.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package middleware
 
 import (
@@ -7,14 +20,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// Manager manages middleware registration and execution
+/* Manager manages middleware registration and execution */
 type Manager struct {
 	middlewares []Middleware
 	mu          sync.RWMutex
 	logger      *logging.Logger
 }
 
-// NewManager creates a new middleware manager
+/* NewManager creates a new middleware manager */
 func NewManager(logger *logging.Logger) *Manager {
 	return &Manager{
 		middlewares: make([]Middleware, 0),
@@ -22,7 +35,7 @@ func NewManager(logger *logging.Logger) *Manager {
 	}
 }
 
-// Register registers a middleware
+/* Register registers a middleware */
 func (m *Manager) Register(middleware Middleware) {
 	m.mu.Lock()
 	defer m.mu.Unlock()
@@ -33,7 +46,7 @@ func (m *Manager) Register(middleware Middleware) {
 	})
 }
 
-// Execute executes the middleware chain
+/* Execute executes the middleware chain */
 func (m *Manager) Execute(ctx context.Context, req *MCPRequest, handler Handler) (*MCPResponse, error) {
 	m.mu.RLock()
 	chain := NewChain(m.middlewares)
@@ -41,7 +54,7 @@ func (m *Manager) Execute(ctx context.Context, req *MCPRequest, handler Handler)
 	return chain.Execute(ctx, req, handler)
 }
 
-// GetAll returns all registered middlewares
+/* GetAll returns all registered middlewares */
 func (m *Manager) GetAll() []Middleware {
 	m.mu.RLock()
 	defer m.mu.RUnlock()
diff --git a/NeuronMCP/internal/middleware/types.go b/NeuronMCP/internal/middleware/types.go
index fc28360..340ee33 100644
--- a/NeuronMCP/internal/middleware/types.go
+++ b/NeuronMCP/internal/middleware/types.go
@@ -1,31 +1,44 @@
+/*-------------------------------------------------------------------------
+ *
+ * types.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/middleware/types.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package middleware
 
 import "context"
 
-// MCPRequest represents an MCP request
+/* MCPRequest represents an MCP request */
 type MCPRequest struct {
 	Method   string                 `json:"method"`
 	Params   map[string]interface{} `json:"params,omitempty"`
 	Metadata map[string]interface{} `json:"metadata,omitempty"`
 }
 
-// MCPResponse represents an MCP response
+/* MCPResponse represents an MCP response */
 type MCPResponse struct {
 	Content  []ContentBlock         `json:"content,omitempty"`
 	IsError  bool                   `json:"isError,omitempty"`
 	Metadata map[string]interface{} `json:"metadata,omitempty"`
 }
 
-// ContentBlock represents a content block in a response
+/* ContentBlock represents a content block in a response */
 type ContentBlock struct {
 	Type string `json:"type"`
 	Text string `json:"text"`
 }
 
-// Handler is a function that handles a request
+/* Handler is a function that handles a request */
 type Handler func(ctx context.Context) (*MCPResponse, error)
 
-// Middleware is the interface that all middleware must implement
+/* Middleware is the interface that all middleware must implement */
 type Middleware interface {
 	Name() string
 	Order() int
diff --git a/NeuronMCP/internal/resources/config.go b/NeuronMCP/internal/resources/config.go
index 9e9b6a2..54d9e17 100644
--- a/NeuronMCP/internal/resources/config.go
+++ b/NeuronMCP/internal/resources/config.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * config.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/resources/config.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package resources
 
 import (
@@ -6,37 +19,37 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/database"
 )
 
-// ConfigResource provides configuration information
+/* ConfigResource provides configuration information */
 type ConfigResource struct {
 	*BaseResource
 }
 
-// NewConfigResource creates a new config resource
+/* NewConfigResource creates a new config resource */
 func NewConfigResource(db *database.Database) *ConfigResource {
 	return &ConfigResource{BaseResource: NewBaseResource(db)}
 }
 
-// URI returns the resource URI
+/* URI returns the resource URI */
 func (r *ConfigResource) URI() string {
 	return "neurondb://config"
 }
 
-// Name returns the resource name
+/* Name returns the resource name */
 func (r *ConfigResource) Name() string {
 	return "Neurondb Configuration"
 }
 
-// Description returns the resource description
+/* Description returns the resource description */
 func (r *ConfigResource) Description() string {
 	return "Current Neurondb configuration settings"
 }
 
-// MimeType returns the MIME type
+/* MimeType returns the MIME type */
 func (r *ConfigResource) MimeType() string {
 	return "application/json"
 }
 
-// GetContent returns the config content
+/* GetContent returns the config content */
 func (r *ConfigResource) GetContent(ctx context.Context) (interface{}, error) {
 	query := `
 		SELECT 
diff --git a/NeuronMCP/internal/resources/indexes.go b/NeuronMCP/internal/resources/indexes.go
index 3e5e52e..417bea9 100644
--- a/NeuronMCP/internal/resources/indexes.go
+++ b/NeuronMCP/internal/resources/indexes.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * indexes.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/resources/indexes.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package resources
 
 import (
@@ -6,37 +19,37 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/database"
 )
 
-// IndexesResource provides vector indexes information
+/* IndexesResource provides vector indexes information */
 type IndexesResource struct {
 	*BaseResource
 }
 
-// NewIndexesResource creates a new indexes resource
+/* NewIndexesResource creates a new indexes resource */
 func NewIndexesResource(db *database.Database) *IndexesResource {
 	return &IndexesResource{BaseResource: NewBaseResource(db)}
 }
 
-// URI returns the resource URI
+/* URI returns the resource URI */
 func (r *IndexesResource) URI() string {
 	return "neurondb://indexes"
 }
 
-// Name returns the resource name
+/* Name returns the resource name */
 func (r *IndexesResource) Name() string {
 	return "Vector Indexes"
 }
 
-// Description returns the resource description
+/* Description returns the resource description */
 func (r *IndexesResource) Description() string {
 	return "Status and information about vector indexes"
 }
 
-// MimeType returns the MIME type
+/* MimeType returns the MIME type */
 func (r *IndexesResource) MimeType() string {
 	return "application/json"
 }
 
-// GetContent returns the indexes content
+/* GetContent returns the indexes content */
 func (r *IndexesResource) GetContent(ctx context.Context) (interface{}, error) {
 	query := `
 		SELECT 
diff --git a/NeuronMCP/internal/resources/models.go b/NeuronMCP/internal/resources/models.go
index babc71c..2bede17 100644
--- a/NeuronMCP/internal/resources/models.go
+++ b/NeuronMCP/internal/resources/models.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * models.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/resources/models.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package resources
 
 import (
@@ -6,37 +19,37 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/database"
 )
 
-// ModelsResource provides ML models catalog
+/* ModelsResource provides ML models catalog */
 type ModelsResource struct {
 	*BaseResource
 }
 
-// NewModelsResource creates a new models resource
+/* NewModelsResource creates a new models resource */
 func NewModelsResource(db *database.Database) *ModelsResource {
 	return &ModelsResource{BaseResource: NewBaseResource(db)}
 }
 
-// URI returns the resource URI
+/* URI returns the resource URI */
 func (r *ModelsResource) URI() string {
 	return "neurondb://models"
 }
 
-// Name returns the resource name
+/* Name returns the resource name */
 func (r *ModelsResource) Name() string {
 	return "ML Models"
 }
 
-// Description returns the resource description
+/* Description returns the resource description */
 func (r *ModelsResource) Description() string {
 	return "Catalog of trained ML models"
 }
 
-// MimeType returns the MIME type
+/* MimeType returns the MIME type */
 func (r *ModelsResource) MimeType() string {
 	return "application/json"
 }
 
-// GetContent returns the models content
+/* GetContent returns the models content */
 func (r *ModelsResource) GetContent(ctx context.Context) (interface{}, error) {
 	query := `
 		SELECT 
diff --git a/NeuronMCP/internal/resources/resource.go b/NeuronMCP/internal/resources/resource.go
index c418224..0fdac54 100644
--- a/NeuronMCP/internal/resources/resource.go
+++ b/NeuronMCP/internal/resources/resource.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * resource.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/resources/resource.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package resources
 
 import (
@@ -8,7 +21,7 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/database"
 )
 
-// Resource is the interface that all resources must implement
+/* Resource is the interface that all resources must implement */
 type Resource interface {
 	URI() string
 	Name() string
@@ -17,17 +30,17 @@ type Resource interface {
 	GetContent(ctx context.Context) (interface{}, error)
 }
 
-// BaseResource provides common functionality for resources
+/* BaseResource provides common functionality for resources */
 type BaseResource struct {
 	db *database.Database
 }
 
-// NewBaseResource creates a new base resource
+/* NewBaseResource creates a new base resource */
 func NewBaseResource(db *database.Database) *BaseResource {
 	return &BaseResource{db: db}
 }
 
-// executeQuery executes a query and returns results
+/* executeQuery executes a query and returns results */
 func (r *BaseResource) executeQuery(ctx context.Context, query string, params []interface{}) ([]map[string]interface{}, error) {
 	rows, err := r.db.Query(ctx, query, params...)
 	if err != nil {
@@ -38,7 +51,7 @@ func (r *BaseResource) executeQuery(ctx context.Context, query string, params []
 	return scanRowsToMaps(rows)
 }
 
-// scanRowsToMaps scans all rows into maps
+/* scanRowsToMaps scans all rows into maps */
 func scanRowsToMaps(rows pgx.Rows) ([]map[string]interface{}, error) {
 	var results []map[string]interface{}
 
@@ -53,7 +66,7 @@ func scanRowsToMaps(rows pgx.Rows) ([]map[string]interface{}, error) {
 	return results, rows.Err()
 }
 
-// scanRowToMap scans a single row into a map
+/* scanRowToMap scans a single row into a map */
 func scanRowToMap(rows pgx.Rows) (map[string]interface{}, error) {
 	fieldDescriptions := rows.FieldDescriptions()
 	values := make([]interface{}, len(fieldDescriptions))
@@ -75,20 +88,20 @@ func scanRowToMap(rows pgx.Rows) (map[string]interface{}, error) {
 	return result, nil
 }
 
-// Manager manages all resources
+/* Manager manages all resources */
 type Manager struct {
 	resources map[string]Resource
 	db        *database.Database
 }
 
-// NewManager creates a new resource manager
+/* NewManager creates a new resource manager */
 func NewManager(db *database.Database) *Manager {
 	m := &Manager{
 		resources: make(map[string]Resource),
 		db:        db,
 	}
 
-	// Register built-in resources
+  /* Register built-in resources */
 	m.Register(NewSchemaResource(db))
 	m.Register(NewModelsResource(db))
 	m.Register(NewIndexesResource(db))
@@ -100,12 +113,12 @@ func NewManager(db *database.Database) *Manager {
 	return m
 }
 
-// Register registers a resource
+/* Register registers a resource */
 func (m *Manager) Register(resource Resource) {
 	m.resources[resource.URI()] = resource
 }
 
-// HandleResource handles a resource request
+/* HandleResource handles a resource request */
 func (m *Manager) HandleResource(ctx context.Context, uri string) (*ReadResourceResponse, error) {
 	resource, exists := m.resources[uri]
 	if !exists {
@@ -133,7 +146,7 @@ func (m *Manager) HandleResource(ctx context.Context, uri string) (*ReadResource
 	}, nil
 }
 
-// ListResources returns all available resources
+/* ListResources returns all available resources */
 func (m *Manager) ListResources() []ResourceDefinition {
 	definitions := make([]ResourceDefinition, 0, len(m.resources))
 	for _, resource := range m.resources {
@@ -147,7 +160,7 @@ func (m *Manager) ListResources() []ResourceDefinition {
 	return definitions
 }
 
-// ResourceDefinition represents a resource definition
+/* ResourceDefinition represents a resource definition */
 type ResourceDefinition struct {
 	URI         string `json:"uri"`
 	Name        string `json:"name"`
@@ -155,19 +168,19 @@ type ResourceDefinition struct {
 	MimeType    string `json:"mimeType"`
 }
 
-// ReadResourceResponse represents a resource read response
+/* ReadResourceResponse represents a resource read response */
 type ReadResourceResponse struct {
 	Contents []ResourceContent `json:"contents"`
 }
 
-// ResourceContent represents resource content
+/* ResourceContent represents resource content */
 type ResourceContent struct {
 	URI      string `json:"uri"`
 	MimeType string `json:"mimeType"`
 	Text     string `json:"text"`
 }
 
-// ResourceNotFoundError is returned when a resource is not found
+/* ResourceNotFoundError is returned when a resource is not found */
 type ResourceNotFoundError struct {
 	URI string
 }
diff --git a/NeuronMCP/internal/resources/schema.go b/NeuronMCP/internal/resources/schema.go
index 720d04f..34fe3b1 100644
--- a/NeuronMCP/internal/resources/schema.go
+++ b/NeuronMCP/internal/resources/schema.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * schema.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/resources/schema.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package resources
 
 import (
@@ -6,37 +19,37 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/database"
 )
 
-// SchemaResource provides database schema information
+/* SchemaResource provides database schema information */
 type SchemaResource struct {
 	*BaseResource
 }
 
-// NewSchemaResource creates a new schema resource
+/* NewSchemaResource creates a new schema resource */
 func NewSchemaResource(db *database.Database) *SchemaResource {
 	return &SchemaResource{BaseResource: NewBaseResource(db)}
 }
 
-// URI returns the resource URI
+/* URI returns the resource URI */
 func (r *SchemaResource) URI() string {
 	return "neurondb://schema"
 }
 
-// Name returns the resource name
+/* Name returns the resource name */
 func (r *SchemaResource) Name() string {
 	return "Database Schema"
 }
 
-// Description returns the resource description
+/* Description returns the resource description */
 func (r *SchemaResource) Description() string {
 	return "NeurondB database schema information"
 }
 
-// MimeType returns the MIME type
+/* MimeType returns the MIME type */
 func (r *SchemaResource) MimeType() string {
 	return "application/json"
 }
 
-// GetContent returns the schema content
+/* GetContent returns the schema content */
 func (r *SchemaResource) GetContent(ctx context.Context) (interface{}, error) {
 	query := `
 		SELECT 
diff --git a/NeuronMCP/internal/resources/stats.go b/NeuronMCP/internal/resources/stats.go
index 9fb1dfe..6d5ecb4 100644
--- a/NeuronMCP/internal/resources/stats.go
+++ b/NeuronMCP/internal/resources/stats.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * stats.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/resources/stats.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package resources
 
 import (
@@ -6,73 +19,73 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/database"
 )
 
-// VectorStatsResource provides vector statistics
+/* VectorStatsResource provides vector statistics */
 type VectorStatsResource struct {
 	*BaseResource
 }
 
-// NewVectorStatsResource creates a new vector stats resource
+/* NewVectorStatsResource creates a new vector stats resource */
 func NewVectorStatsResource(db *database.Database) *VectorStatsResource {
 	return &VectorStatsResource{BaseResource: NewBaseResource(db)}
 }
 
-// URI returns the resource URI
+/* URI returns the resource URI */
 func (r *VectorStatsResource) URI() string {
 	return "neurondb://vector_stats"
 }
 
-// Name returns the resource name
+/* Name returns the resource name */
 func (r *VectorStatsResource) Name() string {
 	return "Vector Statistics"
 }
 
-// Description returns the resource description
+/* Description returns the resource description */
 func (r *VectorStatsResource) Description() string {
 	return "Aggregate vector statistics"
 }
 
-// MimeType returns the MIME type
+/* MimeType returns the MIME type */
 func (r *VectorStatsResource) MimeType() string {
 	return "application/json"
 }
 
-// GetContent returns the vector stats content
+/* GetContent returns the vector stats content */
 func (r *VectorStatsResource) GetContent(ctx context.Context) (interface{}, error) {
 	query := `SELECT * FROM neurondb.vector_stats`
 	return r.executeQuery(ctx, query, nil)
 }
 
-// IndexHealthResource provides index health information
+/* IndexHealthResource provides index health information */
 type IndexHealthResource struct {
 	*BaseResource
 }
 
-// NewIndexHealthResource creates a new index health resource
+/* NewIndexHealthResource creates a new index health resource */
 func NewIndexHealthResource(db *database.Database) *IndexHealthResource {
 	return &IndexHealthResource{BaseResource: NewBaseResource(db)}
 }
 
-// URI returns the resource URI
+/* URI returns the resource URI */
 func (r *IndexHealthResource) URI() string {
 	return "neurondb://index_health"
 }
 
-// Name returns the resource name
+/* Name returns the resource name */
 func (r *IndexHealthResource) Name() string {
 	return "Index Health"
 }
 
-// Description returns the resource description
+/* Description returns the resource description */
 func (r *IndexHealthResource) Description() string {
 	return "Index health dashboard"
 }
 
-// MimeType returns the MIME type
+/* MimeType returns the MIME type */
 func (r *IndexHealthResource) MimeType() string {
 	return "application/json"
 }
 
-// GetContent returns the index health content
+/* GetContent returns the index health content */
 func (r *IndexHealthResource) GetContent(ctx context.Context) (interface{}, error) {
 	query := `SELECT * FROM neurondb.index_health`
 	return r.executeQuery(ctx, query, nil)
diff --git a/NeuronMCP/internal/resources/workers.go b/NeuronMCP/internal/resources/workers.go
index d5d7739..97bc066 100644
--- a/NeuronMCP/internal/resources/workers.go
+++ b/NeuronMCP/internal/resources/workers.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * workers.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/resources/workers.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package resources
 
 import (
@@ -6,37 +19,37 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/database"
 )
 
-// WorkersResource provides worker status information
+/* WorkersResource provides worker status information */
 type WorkersResource struct {
 	*BaseResource
 }
 
-// NewWorkersResource creates a new workers resource
+/* NewWorkersResource creates a new workers resource */
 func NewWorkersResource(db *database.Database) *WorkersResource {
 	return &WorkersResource{BaseResource: NewBaseResource(db)}
 }
 
-// URI returns the resource URI
+/* URI returns the resource URI */
 func (r *WorkersResource) URI() string {
 	return "neurondb://workers"
 }
 
-// Name returns the resource name
+/* Name returns the resource name */
 func (r *WorkersResource) Name() string {
 	return "Background Workers Status"
 }
 
-// Description returns the resource description
+/* Description returns the resource description */
 func (r *WorkersResource) Description() string {
 	return "Status of background workers"
 }
 
-// MimeType returns the MIME type
+/* MimeType returns the MIME type */
 func (r *WorkersResource) MimeType() string {
 	return "application/json"
 }
 
-// GetContent returns the workers content
+/* GetContent returns the workers content */
 func (r *WorkersResource) GetContent(ctx context.Context) (interface{}, error) {
 	query := `SELECT * FROM neurondb.neurondb_workers`
 	return r.executeQuery(ctx, query, nil)
diff --git a/NeuronMCP/internal/server/filter.go b/NeuronMCP/internal/server/filter.go
index f85182c..7f8d673 100644
--- a/NeuronMCP/internal/server/filter.go
+++ b/NeuronMCP/internal/server/filter.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * filter.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/server/filter.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package server
 
 import (
@@ -5,7 +18,7 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/tools"
 )
 
-// filterToolsByFeatures filters tools based on feature flags
+/* filterToolsByFeatures filters tools based on feature flags */
 func (s *Server) filterToolsByFeatures(definitions []tools.ToolDefinition) []tools.ToolDefinition {
 	features := s.config.GetFeaturesConfig()
 	filtered := make([]tools.ToolDefinition, 0, len(definitions))
@@ -19,43 +32,43 @@ func (s *Server) filterToolsByFeatures(definitions []tools.ToolDefinition) []too
 	return filtered
 }
 
-// shouldIncludeTool determines if a tool should be included based on feature flags
+/* shouldIncludeTool determines if a tool should be included based on feature flags */
 func shouldIncludeTool(toolName string, features *config.FeaturesConfig) bool {
-	// Vector tools
+  /* Vector tools */
 	if isVectorTool(toolName) {
 		return features.Vector != nil && features.Vector.Enabled
 	}
 	
-	// ML tools
+  /* ML tools */
 	if isMLTool(toolName) {
 		return features.ML != nil && features.ML.Enabled
 	}
 	
-	// Analytics tools
+  /* Analytics tools */
 	if isAnalyticsTool(toolName) {
 		return features.Analytics != nil && features.Analytics.Enabled
 	}
 	
-	// RAG tools
+  /* RAG tools */
 	if isRAGTool(toolName) {
 		return features.RAG != nil && features.RAG.Enabled
 	}
 	
-	// Project tools
+  /* Project tools */
 	if isProjectTool(toolName) {
 		return features.Projects != nil && features.Projects.Enabled
 	}
 	
-	// GPU tools
+  /* GPU tools */
 	if isGPUTool(toolName) {
 		return features.GPU != nil && features.GPU.Enabled
 	}
 	
-	// Default: include if no specific feature flag
+  /* Default: include if no specific feature flag */
 	return true
 }
 
-// Tool category checkers
+/* Tool category checkers */
 func isVectorTool(name string) bool {
 	vectorPrefixes := []string{"vector_", "embed_", "generate_embedding", "batch_embedding", "create_hnsw_index", "drop_index"}
 	for _, prefix := range vectorPrefixes {
diff --git a/NeuronMCP/internal/server/handlers.go b/NeuronMCP/internal/server/handlers.go
index 253903c..8367ece 100644
--- a/NeuronMCP/internal/server/handlers.go
+++ b/NeuronMCP/internal/server/handlers.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * handlers.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/server/handlers.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package server
 
 import (
@@ -10,7 +23,7 @@ import (
 	"github.com/neurondb/NeuronMCP/pkg/mcp"
 )
 
-// min returns the minimum of two integers
+/* min returns the minimum of two integers */
 func min(a, b int) int {
 	if a < b {
 		return a
@@ -18,16 +31,16 @@ func min(a, b int) int {
 	return b
 }
 
-// setupToolHandlers sets up tool-related MCP handlers
+/* setupToolHandlers sets up tool-related MCP handlers */
 func (s *Server) setupToolHandlers() {
-	// List tools handler
+  /* List tools handler */
 	s.mcpServer.SetHandler("tools/list", s.handleListTools)
 
-	// Call tool handler
+  /* Call tool handler */
 	s.mcpServer.SetHandler("tools/call", s.handleCallTool)
 }
 
-// handleListTools handles the tools/list request
+/* handleListTools handles the tools/list request */
 func (s *Server) handleListTools(ctx context.Context, params json.RawMessage) (interface{}, error) {
 	definitions := s.toolRegistry.GetAllDefinitions()
 	filtered := s.filterToolsByFeatures(definitions)
@@ -44,7 +57,7 @@ func (s *Server) handleListTools(ctx context.Context, params json.RawMessage) (i
 	return mcp.ListToolsResponse{Tools: mcpTools}, nil
 }
 
-// handleCallTool handles the tools/call request
+/* handleCallTool handles the tools/call request */
 func (s *Server) handleCallTool(ctx context.Context, params json.RawMessage) (interface{}, error) {
 	var req mcp.CallToolRequest
 	if err := json.Unmarshal(params, &req); err != nil {
@@ -68,7 +81,7 @@ func (s *Server) handleCallTool(ctx context.Context, params json.RawMessage) (in
 	})
 }
 
-// executeTool executes a tool and returns the response
+/* executeTool executes a tool and returns the response */
 func (s *Server) executeTool(ctx context.Context, toolName string, arguments map[string]interface{}) (*middleware.MCPResponse, error) {
 	if toolName == "" {
 		return &middleware.MCPResponse{
@@ -94,7 +107,7 @@ func (s *Server) executeTool(ctx context.Context, toolName string, arguments map
 		}, nil
 	}
 
-	// Log tool execution start
+  /* Log tool execution start */
 	s.logger.Info("Executing tool", map[string]interface{}{
 		"tool_name": toolName,
 		"arguments_count": len(arguments),
@@ -113,7 +126,7 @@ func (s *Server) executeTool(ctx context.Context, toolName string, arguments map
 	return s.formatToolResult(result)
 }
 
-// formatToolResult formats a tool result as an MCP response
+/* formatToolResult formats a tool result as an MCP response */
 func (s *Server) formatToolResult(result *tools.ToolResult) (*middleware.MCPResponse, error) {
 	if !result.Success {
 		return s.formatToolError(result), nil
@@ -128,7 +141,7 @@ func (s *Server) formatToolResult(result *tools.ToolResult) (*middleware.MCPResp
 	}, nil
 }
 
-// formatToolError formats a tool error as an MCP response
+/* formatToolError formats a tool error as an MCP response */
 func (s *Server) formatToolError(result *tools.ToolResult) *middleware.MCPResponse {
 	errorText := "Unknown error"
 	errorMetadata := make(map[string]interface{})
diff --git a/NeuronMCP/internal/server/handlers_test.go b/NeuronMCP/internal/server/handlers_test.go
index da80aa9..206c7d1 100644
--- a/NeuronMCP/internal/server/handlers_test.go
+++ b/NeuronMCP/internal/server/handlers_test.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * handlers_test.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/server/handlers_test.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package server
 
 import (
@@ -34,7 +47,7 @@ func TestServerSetup(t *testing.T) {
 		t.Fatal("Failed to create tool registry")
 	}
 
-	// This should not panic or crash
+  /* This should not panic or crash */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
@@ -44,13 +57,13 @@ func TestServerSetup(t *testing.T) {
 		tools.RegisterAllTools(toolRegistry, db, logger)
 	}()
 
-	// Verify tools are registered
+  /* Verify tools are registered */
 	definitions := toolRegistry.GetAllDefinitions()
 	if len(definitions) == 0 {
 		t.Fatal("No tools registered - this indicates a real problem")
 	}
 
-	// Check that we have expected tools
+  /* Check that we have expected tools */
 	toolNames := make(map[string]bool)
 	for _, def := range definitions {
 		if def.Name == "" {
@@ -89,7 +102,7 @@ func TestToolRegistry(t *testing.T) {
 		t.Fatal("Failed to create tool registry")
 	}
 
-	// This should not panic
+  /* This should not panic */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
@@ -99,25 +112,25 @@ func TestToolRegistry(t *testing.T) {
 		tools.RegisterAllTools(toolRegistry, db, logger)
 	}()
 
-	// Test that nonexistent tool returns nil
+  /* Test that nonexistent tool returns nil */
 	tool := toolRegistry.GetTool("nonexistent_tool")
 	if tool != nil {
 		t.Error("GetTool() should return nil for nonexistent tool")
 	}
 
-	// Test that existing tool is found
+  /* Test that existing tool is found */
 	tool = toolRegistry.GetTool("vector_search")
 	if tool == nil {
 		t.Fatal("GetTool() should return tool for 'vector_search'")
 	}
 
-	// Test with empty string - should not crash
+  /* Test with empty string - should not crash */
 	tool = toolRegistry.GetTool("")
 	if tool != nil {
 		t.Error("GetTool() should return nil for empty string")
 	}
 
-	// Test with nil-like behavior - should not crash
+  /* Test with nil-like behavior - should not crash */
 	tool = toolRegistry.GetTool("\x00")
 	if tool != nil {
 		t.Error("GetTool() should return nil for invalid tool name")
@@ -144,7 +157,7 @@ func TestHandleListTools(t *testing.T) {
 
 	ctx := context.Background()
 
-	// Test with valid empty params
+  /* Test with valid empty params */
 	params := json.RawMessage("{}")
 	result, err := srv.handleListTools(ctx, params)
 	if err != nil {
@@ -160,16 +173,16 @@ func TestHandleListTools(t *testing.T) {
 		t.Error("handleListTools() returned no tools")
 	}
 
-	// Test with invalid JSON - should return error, not crash
+  /* Test with invalid JSON - should return error, not crash */
 	invalidParams := json.RawMessage("{invalid json}")
 	result, err = srv.handleListTools(ctx, invalidParams)
-	// Note: handleListTools doesn't parse params, so it might not error
-	// But it should not crash
+  /* Note: handleListTools doesn't parse params, so it might not error */
+  /* But it should not crash */
 	if err != nil {
 		t.Logf("handleListTools() with invalid JSON returned error (expected): %v", err)
 	}
 
-	// Test with nil params - should not crash
+  /* Test with nil params - should not crash */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
@@ -195,39 +208,39 @@ func TestHandleCallTool_ErrorConditions(t *testing.T) {
 
 	ctx := context.Background()
 
-	// Test with invalid JSON - should return error
+  /* Test with invalid JSON - should return error */
 	invalidParams := json.RawMessage("{invalid json}")
 	_, err = srv.handleCallTool(ctx, invalidParams)
 	if err == nil {
 		t.Error("handleCallTool() should return error for invalid JSON")
 	}
 
-	// Test with missing tool name - should return error
+  /* Test with missing tool name - should return error */
 	missingNameParams := json.RawMessage(`{"arguments": {}}`)
 	_, err = srv.handleCallTool(ctx, missingNameParams)
 	if err == nil {
 		t.Error("handleCallTool() should return error for missing tool name")
 	}
 
-	// Test with empty tool name - should return error
+  /* Test with empty tool name - should return error */
 	emptyNameParams := json.RawMessage(`{"name": "", "arguments": {}}`)
 	_, err = srv.handleCallTool(ctx, emptyNameParams)
 	if err == nil {
 		t.Error("handleCallTool() should return error for empty tool name")
 	}
 
-	// Test with nonexistent tool - should return error response, not crash
+  /* Test with nonexistent tool - should return error response, not crash */
 	nonexistentParams := json.RawMessage(`{"name": "nonexistent_tool_xyz", "arguments": {}}`)
 	result, err := srv.handleCallTool(ctx, nonexistentParams)
 	if err != nil {
 		t.Fatalf("handleCallTool() should not return error for nonexistent tool, but return error response: %v", err)
 	}
-	// Should return an error response, not nil
+  /* Should return an error response, not nil */
 	if result == nil {
 		t.Error("handleCallTool() should return error response for nonexistent tool, not nil")
 	}
 
-	// Test with nil params - should not crash
+  /* Test with nil params - should not crash */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
@@ -237,10 +250,10 @@ func TestHandleCallTool_ErrorConditions(t *testing.T) {
 		_, _ = srv.handleCallTool(ctx, nil)
 	}()
 
-	// Test with malformed arguments - should handle gracefully
+  /* Test with malformed arguments - should handle gracefully */
 	malformedParams := json.RawMessage(`{"name": "vector_search", "arguments": "not an object"}`)
 	_, err = srv.handleCallTool(ctx, malformedParams)
-	// May or may not error depending on JSON parsing, but should not crash
+  /* May or may not error depending on JSON parsing, but should not crash */
 	if err != nil {
 		t.Logf("handleCallTool() with malformed arguments returned error: %v", err)
 	}
@@ -261,7 +274,7 @@ func TestExecuteTool_ErrorConditions(t *testing.T) {
 
 	ctx := context.Background()
 
-	// Test with empty tool name - should return error response
+  /* Test with empty tool name - should return error response */
 	resp, err := srv.executeTool(ctx, "", map[string]interface{}{})
 	if err != nil {
 		t.Fatalf("executeTool() should not return error for empty name, but error response: %v", err)
@@ -273,7 +286,7 @@ func TestExecuteTool_ErrorConditions(t *testing.T) {
 		t.Error("executeTool() should return error response for empty name")
 	}
 
-	// Test with nonexistent tool - should return error response
+  /* Test with nonexistent tool - should return error response */
 	resp, err = srv.executeTool(ctx, "nonexistent_tool_abc", map[string]interface{}{})
 	if err != nil {
 		t.Fatalf("executeTool() should not return error for nonexistent tool: %v", err)
@@ -285,7 +298,7 @@ func TestExecuteTool_ErrorConditions(t *testing.T) {
 		t.Error("executeTool() should return error response for nonexistent tool")
 	}
 
-	// Test with nil arguments - should not crash
+  /* Test with nil arguments - should not crash */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
@@ -295,7 +308,7 @@ func TestExecuteTool_ErrorConditions(t *testing.T) {
 		_, _ = srv.executeTool(ctx, "vector_search", nil)
 	}()
 
-	// Test with invalid tool name containing null bytes - should not crash
+  /* Test with invalid tool name containing null bytes - should not crash */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
diff --git a/NeuronMCP/internal/server/middleware_setup.go b/NeuronMCP/internal/server/middleware_setup.go
index 6fb77c6..f13693e 100644
--- a/NeuronMCP/internal/server/middleware_setup.go
+++ b/NeuronMCP/internal/server/middleware_setup.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * middleware_setup.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/server/middleware_setup.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package server
 
 import (
@@ -7,27 +20,27 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/middleware/builtin"
 )
 
-// setupBuiltInMiddleware registers all built-in middleware
+/* setupBuiltInMiddleware registers all built-in middleware */
 func setupBuiltInMiddleware(mgr *middleware.Manager, cfgMgr *config.ConfigManager, logger *logging.Logger) {
 	loggingCfg := cfgMgr.GetLoggingConfig()
 	serverCfg := cfgMgr.GetServerSettings()
 
-	// Validation middleware (order: 1)
+  /* Validation middleware (order: 1) */
 	mgr.Register(builtin.NewValidationMiddleware())
 
-	// Logging middleware (order: 2)
+  /* Logging middleware (order: 2) */
 	mgr.Register(builtin.NewLoggingMiddleware(
 		logger,
 		loggingCfg.EnableRequestLogging != nil && *loggingCfg.EnableRequestLogging,
 		loggingCfg.EnableResponseLogging != nil && *loggingCfg.EnableResponseLogging,
 	))
 
-	// Timeout middleware (order: 3) - only if timeout is configured
+  /* Timeout middleware (order: 3) - only if timeout is configured */
 	if serverCfg.Timeout != nil {
 		mgr.Register(builtin.NewTimeoutMiddleware(serverCfg.GetTimeout(), logger))
 	}
 
-	// Error handling middleware (order: 100) - always last
+  /* Error handling middleware (order: 100) - always last */
 	mgr.Register(builtin.NewErrorHandlingMiddleware(
 		logger,
 		loggingCfg.EnableErrorStack != nil && *loggingCfg.EnableErrorStack,
diff --git a/NeuronMCP/internal/server/resource_handlers.go b/NeuronMCP/internal/server/resource_handlers.go
index 0f00237..893d1af 100644
--- a/NeuronMCP/internal/server/resource_handlers.go
+++ b/NeuronMCP/internal/server/resource_handlers.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * resource_handlers.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/server/resource_handlers.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package server
 
 import (
@@ -8,16 +21,16 @@ import (
 	"github.com/neurondb/NeuronMCP/pkg/mcp"
 )
 
-// setupResourceHandlers sets up resource-related MCP handlers
+/* setupResourceHandlers sets up resource-related MCP handlers */
 func (s *Server) setupResourceHandlers() {
-	// List resources handler
+  /* List resources handler */
 	s.mcpServer.SetHandler("resources/list", s.handleListResources)
 
-	// Read resource handler
+  /* Read resource handler */
 	s.mcpServer.SetHandler("resources/read", s.handleReadResource)
 }
 
-// handleListResources handles the resources/list request
+/* handleListResources handles the resources/list request */
 func (s *Server) handleListResources(ctx context.Context, params json.RawMessage) (interface{}, error) {
 	definitions := s.resources.ListResources()
 	
@@ -34,7 +47,7 @@ func (s *Server) handleListResources(ctx context.Context, params json.RawMessage
 	return mcp.ListResourcesResponse{Resources: mcpDefs}, nil
 }
 
-// handleReadResource handles the resources/read request
+/* handleReadResource handles the resources/read request */
 func (s *Server) handleReadResource(ctx context.Context, params json.RawMessage) (interface{}, error) {
 	var req mcp.ReadResourceRequest
 	if err := json.Unmarshal(params, &req); err != nil {
diff --git a/NeuronMCP/internal/server/server.go b/NeuronMCP/internal/server/server.go
index 4780a3c..25c77d3 100644
--- a/NeuronMCP/internal/server/server.go
+++ b/NeuronMCP/internal/server/server.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * server.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/server/server.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package server
 
 import (
@@ -13,7 +26,7 @@ import (
 	"github.com/neurondb/NeuronMCP/pkg/mcp"
 )
 
-// Server is the main MCP server
+/* Server is the main MCP server */
 type Server struct {
 	mcpServer    *mcp.Server
 	db           *database.Database
@@ -24,7 +37,7 @@ type Server struct {
 	resources    *resources.Manager
 }
 
-// NewServer creates a new server
+/* NewServer creates a new server */
 func NewServer() (*Server, error) {
 	cfgMgr := config.NewConfigManager()
 	_, err := cfgMgr.Load("")
@@ -35,7 +48,7 @@ func NewServer() (*Server, error) {
 	logger := logging.NewLogger(cfgMgr.GetLoggingConfig())
 
 	db := database.NewDatabase()
-	// Log database config for debugging
+  /* Log database config for debugging */
 	dbCfg := cfgMgr.GetDatabaseConfig()
 	logger.Info("Database configuration", map[string]interface{}{
 		"host":     dbCfg.GetHost(),
@@ -45,8 +58,8 @@ func NewServer() (*Server, error) {
 		"has_password": dbCfg.Password != nil && *dbCfg.Password != "",
 	})
 	
-	// Try to connect, but don't fail server startup if it fails
-	// The server can start and tools will fail gracefully with proper error messages
+  /* Try to connect, but don't fail server startup if it fails */
+  /* The server can start and tools will fail gracefully with proper error messages */
 	if err := db.Connect(dbCfg); err != nil {
 		logger.Warn("Failed to connect to database at startup", map[string]interface{}{
 			"error": err.Error(),
@@ -56,7 +69,7 @@ func NewServer() (*Server, error) {
 			"user":     dbCfg.GetUser(),
 			"note":  "Server will start but tools may fail. Database connection will be retried on first use.",
 		})
-		// Continue anyway - tools will handle connection errors gracefully
+   /* Continue anyway - tools will handle connection errors gracefully */
 	} else {
 		logger.Info("Connected to database", map[string]interface{}{
 			"host":     dbCfg.GetHost(),
@@ -95,17 +108,17 @@ func (s *Server) setupHandlers() {
 	s.setupToolHandlers()
 	s.setupResourceHandlers()
 	
-	// Set capabilities
+  /* Set capabilities */
 	s.mcpServer.SetCapabilities(mcp.ServerCapabilities{
 		Tools:     make(map[string]interface{}),
 		Resources: make(map[string]interface{}),
 	})
 }
 
-// Start starts the server
+/* Start starts the server */
 func (s *Server) Start(ctx context.Context) error {
 	s.logger.Info("Starting Neurondb MCP server", nil)
-	// Run the MCP server - this will block until context is cancelled or EOF
+  /* Run the MCP server - this will block until context is cancelled or EOF */
 	err := s.mcpServer.Run(ctx)
 	if err != nil && err != context.Canceled {
 		s.logger.Warn("MCP server stopped", map[string]interface{}{
@@ -115,7 +128,7 @@ func (s *Server) Start(ctx context.Context) error {
 	return err
 }
 
-// Stop stops the server
+/* Stop stops the server */
 func (s *Server) Stop() error {
 	s.logger.Info("Stopping Neurondb MCP server", nil)
 	s.db.Close()
diff --git a/NeuronMCP/internal/tools/analytics.go b/NeuronMCP/internal/tools/analytics.go
index 8802b89..d2909c5 100644
--- a/NeuronMCP/internal/tools/analytics.go
+++ b/NeuronMCP/internal/tools/analytics.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * analytics.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/analytics.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// ClusterDataTool performs clustering on data
+/* ClusterDataTool performs clustering on data */
 type ClusterDataTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewClusterDataTool creates a new cluster data tool
+/* NewClusterDataTool creates a new cluster data tool */
 func NewClusterDataTool(db *database.Database, logger *logging.Logger) *ClusterDataTool {
 	return &ClusterDataTool{
 		BaseTool: NewBaseTool(
@@ -67,7 +80,7 @@ func NewClusterDataTool(db *database.Database, logger *logging.Logger) *ClusterD
 	}
 }
 
-// Execute executes the clustering
+/* Execute executes the clustering */
 func (t *ClusterDataTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -194,14 +207,14 @@ func (t *ClusterDataTool) Execute(ctx context.Context, params map[string]interfa
 	}), nil
 }
 
-// DetectOutliersTool detects outliers in data
+/* DetectOutliersTool detects outliers in data */
 type DetectOutliersTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewDetectOutliersTool creates a new detect outliers tool
+/* NewDetectOutliersTool creates a new detect outliers tool */
 func NewDetectOutliersTool(db *database.Database, logger *logging.Logger) *DetectOutliersTool {
 	return &DetectOutliersTool{
 		BaseTool: NewBaseTool(
@@ -232,7 +245,7 @@ func NewDetectOutliersTool(db *database.Database, logger *logging.Logger) *Detec
 	}
 }
 
-// Execute executes the outlier detection
+/* Execute executes the outlier detection */
 func (t *DetectOutliersTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -274,7 +287,7 @@ func (t *DetectOutliersTool) Execute(ctx context.Context, params map[string]inte
 		}), nil
 	}
 
-	// Use NeuronDB's outlier detection function: detect_outliers_zscore(table, column, threshold, method)
+  /* Use NeuronDB's outlier detection function: detect_outliers_zscore(table, column, threshold, method) */
 	query := `SELECT detect_outliers_zscore($1, $2, $3, 'zscore') AS outliers`
 	result, err := t.executor.ExecuteQueryOne(ctx, query, []interface{}{table, vectorColumn, threshold})
 	if err != nil {
@@ -294,14 +307,14 @@ func (t *DetectOutliersTool) Execute(ctx context.Context, params map[string]inte
 	}), nil
 }
 
-// ReduceDimensionalityTool reduces dimensionality using PCA
+/* ReduceDimensionalityTool reduces dimensionality using PCA */
 type ReduceDimensionalityTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewReduceDimensionalityTool creates a new reduce dimensionality tool
+/* NewReduceDimensionalityTool creates a new reduce dimensionality tool */
 func NewReduceDimensionalityTool(db *database.Database, logger *logging.Logger) *ReduceDimensionalityTool {
 	return &ReduceDimensionalityTool{
 		BaseTool: NewBaseTool(
@@ -332,7 +345,7 @@ func NewReduceDimensionalityTool(db *database.Database, logger *logging.Logger)
 	}
 }
 
-// Execute executes the dimensionality reduction
+/* Execute executes the dimensionality reduction */
 func (t *ReduceDimensionalityTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -374,10 +387,10 @@ func (t *ReduceDimensionalityTool) Execute(ctx context.Context, params map[strin
 		}), nil
 	}
 
-	// PCA in NeuronDB is typically done through training a model
-	// Use neurondb.train with 'pca' algorithm or use dimensionality reduction functions
-	// For now, we'll use a direct approach - PCA might need to be implemented differently
-	// Check if there's a compute_pca function or use train with pca algorithm
+  /* PCA in NeuronDB is typically done through training a model */
+  /* Use neurondb.train with 'pca' algorithm or use dimensionality reduction functions */
+  /* For now, we'll use a direct approach - PCA might need to be implemented differently */
+  /* Check if there's a compute_pca function or use train with pca algorithm */
 	query := `SELECT neurondb.train('default', 'pca', $1, NULL, ARRAY[$2], jsonb_build_object('n_components', $3)) AS pca_model_id`
 	result, err := t.executor.ExecuteQueryOne(ctx, query, []interface{}{table, vectorColumn, nComponents})
 	if err != nil {
diff --git a/NeuronMCP/internal/tools/analytics_additional.go b/NeuronMCP/internal/tools/analytics_additional.go
index 169f0f9..b336e5c 100644
--- a/NeuronMCP/internal/tools/analytics_additional.go
+++ b/NeuronMCP/internal/tools/analytics_additional.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * analytics_additional.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/analytics_additional.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// AnalyzeDataTool performs comprehensive data analysis
+/* AnalyzeDataTool performs comprehensive data analysis */
 type AnalyzeDataTool struct {
 	*BaseTool
 	db     *database.Database
 	logger *logging.Logger
 }
 
-// NewAnalyzeDataTool creates a new AnalyzeDataTool
+/* NewAnalyzeDataTool creates a new AnalyzeDataTool */
 func NewAnalyzeDataTool(db *database.Database, logger *logging.Logger) *AnalyzeDataTool {
 	return &AnalyzeDataTool{
 		BaseTool: NewBaseTool(
@@ -52,7 +65,7 @@ func NewAnalyzeDataTool(db *database.Database, logger *logging.Logger) *AnalyzeD
 	}
 }
 
-// Execute performs data analysis
+/* Execute performs data analysis */
 func (t *AnalyzeDataTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -74,18 +87,18 @@ func (t *AnalyzeDataTool) Execute(ctx context.Context, params map[string]interfa
 		}), nil
 	}
 
-	// Build comprehensive analysis query
+  /* Build comprehensive analysis query */
 	var query string
 	var queryParams []interface{}
 
 	if len(columns) > 0 {
-		// Analyze specific columns
+   /* Analyze specific columns */
 		colNames := make([]string, len(columns))
 		for i, col := range columns {
 			colNames[i] = col.(string)
 		}
 
-		// Build statistics query for each column
+   /* Build statistics query for each column */
 		statsQueries := []string{}
 		for _, col := range colNames {
 			statsQueries = append(statsQueries, fmt.Sprintf(`
@@ -118,7 +131,7 @@ func (t *AnalyzeDataTool) Execute(ctx context.Context, params map[string]interfa
 		query = "SELECT json_agg(row_to_json(t)) AS analysis FROM (" + unionQuery + ") t"
 		queryParams = []interface{}{}
 	} else {
-		// Analyze all columns - get column list first
+   /* Analyze all columns - get column list first */
 		colQuery := `
 			SELECT column_name, data_type 
 			FROM information_schema.columns 
@@ -140,13 +153,13 @@ func (t *AnalyzeDataTool) Execute(ctx context.Context, params map[string]interfa
 			}), nil
 		}
 
-		// Build analysis for all numeric columns
+   /* Build analysis for all numeric columns */
 		statsParts := []string{}
 		for _, colRow := range colResults {
 			colName, _ := colRow["column_name"].(string)
 			dataType, _ := colRow["data_type"].(string)
 			
-			// Only analyze numeric types
+    /* Only analyze numeric types */
 			if dataType == "real" || dataType == "double precision" || dataType == "integer" || 
 			   dataType == "bigint" || dataType == "numeric" || dataType == "smallint" {
 				statsParts = append(statsParts, fmt.Sprintf(`
diff --git a/NeuronMCP/internal/tools/automl.go b/NeuronMCP/internal/tools/automl.go
index b32b499..64e4fdc 100644
--- a/NeuronMCP/internal/tools/automl.go
+++ b/NeuronMCP/internal/tools/automl.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * automl.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/automl.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// AutoMLTool performs automated machine learning
+/* AutoMLTool performs automated machine learning */
 type AutoMLTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewAutoMLTool creates a new AutoML tool
+/* NewAutoMLTool creates a new AutoML tool */
 func NewAutoMLTool(db *database.Database, logger *logging.Logger) *AutoMLTool {
 	return &AutoMLTool{
 		BaseTool: NewBaseTool(
@@ -60,7 +73,7 @@ func NewAutoMLTool(db *database.Database, logger *logging.Logger) *AutoMLTool {
 	}
 }
 
-// Execute executes AutoML operation
+/* Execute executes AutoML operation */
 func (t *AutoMLTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -122,3 +135,4 @@ func (t *AutoMLTool) Execute(ctx context.Context, params map[string]interface{})
 
 
 
+
diff --git a/NeuronMCP/internal/tools/base.go b/NeuronMCP/internal/tools/base.go
index 956c292..ca2f135 100644
--- a/NeuronMCP/internal/tools/base.go
+++ b/NeuronMCP/internal/tools/base.go
@@ -1,3 +1,19 @@
+/*-------------------------------------------------------------------------
+ *
+ * base.go
+ *    Base tool types and utilities for NeuronMCP
+ *
+ * Provides common functionality for all tools including result types,
+ * validation, and base tool structure.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/base.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -5,7 +21,7 @@ import (
 	"reflect"
 )
 
-// ToolResult represents the result of tool execution
+/* ToolResult represents the result of tool execution */
 type ToolResult struct {
 	Success  bool                   `json:"success"`
 	Data     interface{}            `json:"data,omitempty"`
@@ -13,21 +29,21 @@ type ToolResult struct {
 	Metadata map[string]interface{} `json:"metadata,omitempty"`
 }
 
-// ToolError represents a tool execution error
+/* ToolError represents a tool execution error */
 type ToolError struct {
 	Message string      `json:"message"`
 	Code    string      `json:"code,omitempty"`
 	Details interface{} `json:"details,omitempty"`
 }
 
-// BaseTool provides common functionality for tools
+/* BaseTool provides common functionality for tools */
 type BaseTool struct {
 	name        string
 	description string
 	inputSchema map[string]interface{}
 }
 
-// NewBaseTool creates a new base tool
+/* NewBaseTool creates a new base tool */
 func NewBaseTool(name, description string, inputSchema map[string]interface{}) *BaseTool {
 	return &BaseTool{
 		name:        name,
@@ -36,26 +52,25 @@ func NewBaseTool(name, description string, inputSchema map[string]interface{}) *
 	}
 }
 
-// Name returns the tool name
+/* Name returns the tool name */
 func (b *BaseTool) Name() string {
 	return b.name
 }
 
-// Description returns the tool description
+/* Description returns the tool description */
 func (b *BaseTool) Description() string {
 	return b.description
 }
 
-// InputSchema returns the input schema
+/* InputSchema returns the input schema */
 func (b *BaseTool) InputSchema() map[string]interface{} {
 	return b.inputSchema
 }
 
-// ValidateParams validates parameters against the schema
+/* ValidateParams validates parameters against the schema */
 func (b *BaseTool) ValidateParams(params map[string]interface{}, schema map[string]interface{}) (bool, []string) {
 	var errors []string
 
-	// Check required fields
 	if required, ok := schema["required"].([]interface{}); ok {
 		for _, req := range required {
 			if reqStr, ok := req.(string); ok {
@@ -66,7 +81,6 @@ func (b *BaseTool) ValidateParams(params map[string]interface{}, schema map[stri
 		}
 	}
 
-	// Validate parameter types
 	if properties, ok := schema["properties"].(map[string]interface{}); ok {
 		for key, value := range params {
 			if propSchema, exists := properties[key]; exists {
@@ -117,7 +131,6 @@ func validateType(value interface{}, schema map[string]interface{}) string {
 		}
 	}
 
-	// Validate enum
 	if enum, ok := schema["enum"].([]interface{}); ok {
 		found := false
 		for _, e := range enum {
@@ -131,7 +144,6 @@ func validateType(value interface{}, schema map[string]interface{}) string {
 		}
 	}
 
-	// Validate number constraints
 	if schemaType == "number" || schemaType == "integer" {
 		if min, ok := schema["minimum"].(float64); ok {
 			if val, ok := value.(float64); ok && val < min {
@@ -148,7 +160,7 @@ func validateType(value interface{}, schema map[string]interface{}) string {
 	return ""
 }
 
-// Success creates a success result
+/* Success creates a success result */
 func Success(data interface{}, metadata map[string]interface{}) *ToolResult {
 	return &ToolResult{
 		Success:  true,
@@ -157,7 +169,7 @@ func Success(data interface{}, metadata map[string]interface{}) *ToolResult {
 	}
 }
 
-// Error creates an error result
+/* Error creates an error result */
 func Error(message, code string, details interface{}) *ToolResult {
 	return &ToolResult{
 		Success: false,
diff --git a/NeuronMCP/internal/tools/base_test.go b/NeuronMCP/internal/tools/base_test.go
index 8e8fe85..0f9ddb2 100644
--- a/NeuronMCP/internal/tools/base_test.go
+++ b/NeuronMCP/internal/tools/base_test.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * base_test.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/base_test.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -82,7 +95,7 @@ func TestBaseTool_ValidateParams(t *testing.T) {
 
 	for _, tt := range tests {
 		t.Run(tt.name, func(t *testing.T) {
-			// Should not panic
+    /* Should not panic */
 			func() {
 				defer func() {
 					if r := recover(); r != nil {
@@ -107,7 +120,7 @@ func TestBaseTool_ValidateParams_NilSchema(t *testing.T) {
 		t.Fatal("NewBaseTool() returned nil")
 	}
 
-	// Should not panic with nil schema
+  /* Should not panic with nil schema */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
@@ -115,7 +128,7 @@ func TestBaseTool_ValidateParams_NilSchema(t *testing.T) {
 			}
 		}()
 		valid, errors := tool.ValidateParams(map[string]interface{}{"test": "value"}, nil)
-		// With nil schema, validation might pass or fail, but should not crash
+   /* With nil schema, validation might pass or fail, but should not crash */
 		if valid {
 			t.Log("ValidateParams() returned valid=true with nil schema (may be acceptable)")
 		}
@@ -128,7 +141,7 @@ func TestBaseTool_ValidateParams_InvalidSchema(t *testing.T) {
 		"invalid": "schema",
 	})
 
-	// Should not panic with invalid schema
+  /* Should not panic with invalid schema */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
@@ -142,10 +155,10 @@ func TestBaseTool_ValidateParams_InvalidSchema(t *testing.T) {
 }
 
 func TestBaseTool_NilTool(t *testing.T) {
-	// Test that methods don't crash on nil tool
-	// Note: In Go, calling methods on nil will panic, but we want to ensure
-	// our code doesn't create nil tools. This test verifies that NewBaseTool
-	// never returns nil.
+  /* Test that methods don't crash on nil tool */
+  /* Note: In Go, calling methods on nil will panic, but we want to ensure */
+  /* our code doesn't create nil tools. This test verifies that NewBaseTool */
+  /* never returns nil. */
 	tool := NewBaseTool("test", "test", nil)
 	if tool == nil {
 		t.Fatal("NewBaseTool() should never return nil")
@@ -171,7 +184,7 @@ func TestSuccess(t *testing.T) {
 		t.Error("Success() should have data")
 	}
 
-	// Test with nil data
+  /* Test with nil data */
 	result = Success(nil, nil)
 	if result == nil {
 		t.Fatal("Success() returned nil")
@@ -180,7 +193,7 @@ func TestSuccess(t *testing.T) {
 		t.Error("Success() should return success=true even with nil data")
 	}
 
-	// Test with nil metadata
+  /* Test with nil metadata */
 	result = Success(data, nil)
 	if result == nil {
 		t.Fatal("Success() returned nil")
@@ -213,7 +226,7 @@ func TestError(t *testing.T) {
 		t.Errorf("Error() code = %v, want %v", result.Error.Code, code)
 	}
 
-	// Test with empty message - should not crash
+  /* Test with empty message - should not crash */
 	result = Error("", "", nil)
 	if result == nil {
 		t.Fatal("Error() returned nil")
@@ -225,7 +238,7 @@ func TestError(t *testing.T) {
 		t.Fatal("Error() should have error even with empty message")
 	}
 
-	// Test with nil details
+  /* Test with nil details */
 	result = Error(message, code, nil)
 	if result == nil {
 		t.Fatal("Error() returned nil")
@@ -236,7 +249,7 @@ func TestError(t *testing.T) {
 }
 
 func TestNewBaseTool(t *testing.T) {
-	// Test with valid inputs
+  /* Test with valid inputs */
 	tool := NewBaseTool("test", "description", map[string]interface{}{})
 	if tool == nil {
 		t.Fatal("NewBaseTool() returned nil")
@@ -248,13 +261,13 @@ func TestNewBaseTool(t *testing.T) {
 		t.Errorf("Description() = %v, want description", tool.Description())
 	}
 
-	// Test with empty name - should not crash
+  /* Test with empty name - should not crash */
 	tool = NewBaseTool("", "description", map[string]interface{}{})
 	if tool == nil {
 		t.Fatal("NewBaseTool() returned nil")
 	}
 
-	// Test with nil schema - should not crash
+  /* Test with nil schema - should not crash */
 	tool2 := NewBaseTool("test", "description", nil)
 	if tool2 == nil {
 		t.Fatal("NewBaseTool() returned nil")
diff --git a/NeuronMCP/internal/tools/dataset_loading.go b/NeuronMCP/internal/tools/dataset_loading.go
index 2152b92..0a16697 100644
--- a/NeuronMCP/internal/tools/dataset_loading.go
+++ b/NeuronMCP/internal/tools/dataset_loading.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * dataset_loading.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/dataset_loading.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -14,14 +27,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// DatasetLoadingTool loads HuggingFace datasets
+/* DatasetLoadingTool loads HuggingFace datasets */
 type DatasetLoadingTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewDatasetLoadingTool creates a new dataset loading tool
+/* NewDatasetLoadingTool creates a new dataset loading tool */
 func NewDatasetLoadingTool(db *database.Database, logger *logging.Logger) *DatasetLoadingTool {
 	return &DatasetLoadingTool{
 		BaseTool: NewBaseTool(
@@ -66,7 +79,7 @@ func NewDatasetLoadingTool(db *database.Database, logger *logging.Logger) *Datas
 	}
 }
 
-// Execute executes the dataset loading
+/* Execute executes the dataset loading */
 func (t *DatasetLoadingTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -90,12 +103,12 @@ func (t *DatasetLoadingTool) Execute(ctx context.Context, params map[string]inte
 		return Error("dataset_name is required and cannot be empty", "VALIDATION_ERROR", nil), nil
 	}
 
-	// For all datasets, use inline Python code (works in Docker)
-	// This avoids needing external script files
+  /* For all datasets, use inline Python code (works in Docker) */
+  /* This avoids needing external script files */
 	return t.loadGenericDataset(ctx, datasetName, split, limit)
 }
 
-// findDatasetScript finds the dataset loading Python script
+/* findDatasetScript finds the dataset loading Python script */
 func (t *DatasetLoadingTool) findDatasetScript() string {
 	possiblePaths := []string{
 		"NeuronDB/dataset/gen_dataset_enhanced.py",
@@ -104,7 +117,7 @@ func (t *DatasetLoadingTool) findDatasetScript() string {
 		"/Users/ibrarahmed/pgelephant/pge/neurondb/NeuronDB/dataset/gen_dataset_enhanced.py",
 	}
 
-	// Try relative to current working directory
+  /* Try relative to current working directory */
 	cwd, _ := os.Getwd()
 	for dir := cwd; dir != "/"; dir = filepath.Dir(dir) {
 		testPath := filepath.Join(dir, "NeuronDB", "dataset", "gen_dataset_enhanced.py")
@@ -113,7 +126,7 @@ func (t *DatasetLoadingTool) findDatasetScript() string {
 		}
 	}
 
-	// Try predefined paths
+  /* Try predefined paths */
 	for _, path := range possiblePaths {
 		if absPath, err := filepath.Abs(path); err == nil {
 			if _, err := os.Stat(absPath); err == nil {
@@ -125,7 +138,7 @@ func (t *DatasetLoadingTool) findDatasetScript() string {
 	return ""
 }
 
-// buildScriptArgs builds command line arguments for the dataset script
+/* buildScriptArgs builds command line arguments for the dataset script */
 func (t *DatasetLoadingTool) buildScriptArgs(datasetName string, limit int) []string {
 	scriptPath := t.findDatasetScript()
 	if scriptPath == "" {
@@ -150,15 +163,15 @@ func (t *DatasetLoadingTool) buildScriptArgs(datasetName string, limit int) []st
 	return args
 }
 
-// scriptExists checks if the script file exists
+/* scriptExists checks if the script file exists */
 func (t *DatasetLoadingTool) scriptExists(scriptPath string) bool {
 	_, err := os.Stat(scriptPath)
 	return err == nil
 }
 
-// loadGenericDataset loads a generic HuggingFace dataset using inline Python
+/* loadGenericDataset loads a generic HuggingFace dataset using inline Python */
 func (t *DatasetLoadingTool) loadGenericDataset(ctx context.Context, datasetName, split string, limit int) (*ToolResult, error) {
-	// Create Python code to load the dataset
+  /* Create Python code to load the dataset */
 	pythonCode := fmt.Sprintf(`
 import os
 import sys
@@ -295,7 +308,7 @@ except Exception as e:
     sys.exit(1)
 `, datasetName, split, limit)
 
-	// Set up environment
+  /* Set up environment */
 	cfgMgr := config.NewConfigManager()
 	cfgMgr.Load("")
 	dbCfg := cfgMgr.GetDatabaseConfig()
@@ -312,10 +325,10 @@ except Exception as e:
 		env = append(env, fmt.Sprintf("PGPASSWORD=%s", *pwd))
 	}
 
-	// Execute Python
+  /* Execute Python */
 	cmd := exec.CommandContext(ctx, "python3", "-c", pythonCode)
 	cmd.Env = env
-	// CombinedOutput() captures both stdout and stderr
+  /* CombinedOutput() captures both stdout and stderr */
 
 	output, err := cmd.CombinedOutput()
 	if err != nil {
@@ -334,12 +347,12 @@ except Exception as e:
 		), nil
 	}
 
-	// Parse JSON output
+  /* Parse JSON output */
 	outputStr := strings.TrimSpace(string(output))
 	rowsLoaded := 0
 	tableName := ""
 
-	// Try to extract JSON from output
+  /* Try to extract JSON from output */
 	if strings.Contains(outputStr, "{") {
 		jsonStart := strings.Index(outputStr, "{")
 		jsonEnd := strings.LastIndex(outputStr, "}") + 1
@@ -371,18 +384,18 @@ except Exception as e:
 	}), nil
 }
 
-// extractRowsLoaded extracts the number of rows loaded from script output
+/* extractRowsLoaded extracts the number of rows loaded from script output */
 func (t *DatasetLoadingTool) extractRowsLoaded(output string) int {
-	// Look for patterns like "loaded: 1000", "inserted: 500", etc.
+  /* Look for patterns like "loaded: 1000", "inserted: 500", etc. */
 	lines := strings.Split(output, "\n")
 	for _, line := range lines {
 		lower := strings.ToLower(line)
 		if strings.Contains(lower, "loaded") || strings.Contains(lower, "inserted") {
-			// Try to extract number
+    /* Try to extract number */
 			words := strings.Fields(line)
 			for i, word := range words {
 				if (strings.Contains(word, "loaded") || strings.Contains(word, "inserted")) && i+1 < len(words) {
-					// Next word might be the number
+      /* Next word might be the number */
 					var num int
 					if _, err := fmt.Sscanf(words[i+1], "%d", &num); err == nil {
 						return num
@@ -393,3 +406,4 @@ func (t *DatasetLoadingTool) extractRowsLoaded(output string) int {
 	}
 	return 0
 }
+
diff --git a/NeuronMCP/internal/tools/drift_detection.go b/NeuronMCP/internal/tools/drift_detection.go
index 79b1d0e..84e030e 100644
--- a/NeuronMCP/internal/tools/drift_detection.go
+++ b/NeuronMCP/internal/tools/drift_detection.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * drift_detection.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/drift_detection.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// DriftDetectionTool detects data drift
+/* DriftDetectionTool detects data drift */
 type DriftDetectionTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewDriftDetectionTool creates a new drift detection tool
+/* NewDriftDetectionTool creates a new drift detection tool */
 func NewDriftDetectionTool(db *database.Database, logger *logging.Logger) *DriftDetectionTool {
 	return &DriftDetectionTool{
 		BaseTool: NewBaseTool(
@@ -54,7 +67,7 @@ func NewDriftDetectionTool(db *database.Database, logger *logging.Logger) *Drift
 	}
 }
 
-// Execute executes drift detection
+/* Execute executes drift detection */
 func (t *DriftDetectionTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -72,7 +85,7 @@ func (t *DriftDetectionTool) Execute(ctx context.Context, params map[string]inte
 		return Error("table and vector_column are required", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Build query based on method
+  /* Build query based on method */
 	var query string
 	var queryParams []interface{}
 
@@ -118,3 +131,4 @@ func (t *DriftDetectionTool) Execute(ctx context.Context, params map[string]inte
 
 
 
+
diff --git a/NeuronMCP/internal/tools/embeddings_complete.go b/NeuronMCP/internal/tools/embeddings_complete.go
index 6b6bfdf..f1e28c1 100644
--- a/NeuronMCP/internal/tools/embeddings_complete.go
+++ b/NeuronMCP/internal/tools/embeddings_complete.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * embeddings_complete.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/embeddings_complete.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -9,14 +22,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// EmbedImageTool generates image embeddings
+/* EmbedImageTool generates image embeddings */
 type EmbedImageTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewEmbedImageTool creates a new image embedding tool
+/* NewEmbedImageTool creates a new image embedding tool */
 func NewEmbedImageTool(db *database.Database, logger *logging.Logger) *EmbedImageTool {
 	return &EmbedImageTool{
 		BaseTool: NewBaseTool(
@@ -43,7 +56,7 @@ func NewEmbedImageTool(db *database.Database, logger *logging.Logger) *EmbedImag
 	}
 }
 
-// Execute executes the image embedding generation
+/* Execute executes the image embedding generation */
 func (t *EmbedImageTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -66,7 +79,7 @@ func (t *EmbedImageTool) Execute(ctx context.Context, params map[string]interfac
 		}), nil
 	}
 
-	// Decode base64 image data
+  /* Decode base64 image data */
 	imageBytes, err := base64.StdEncoding.DecodeString(imageData)
 	if err != nil {
 		return Error(fmt.Sprintf("Invalid base64 image data: %v", err), "VALIDATION_ERROR", map[string]interface{}{
@@ -95,14 +108,14 @@ func (t *EmbedImageTool) Execute(ctx context.Context, params map[string]interfac
 	}), nil
 }
 
-// EmbedMultimodalTool generates multimodal embeddings
+/* EmbedMultimodalTool generates multimodal embeddings */
 type EmbedMultimodalTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewEmbedMultimodalTool creates a new multimodal embedding tool
+/* NewEmbedMultimodalTool creates a new multimodal embedding tool */
 func NewEmbedMultimodalTool(db *database.Database, logger *logging.Logger) *EmbedMultimodalTool {
 	return &EmbedMultimodalTool{
 		BaseTool: NewBaseTool(
@@ -133,7 +146,7 @@ func NewEmbedMultimodalTool(db *database.Database, logger *logging.Logger) *Embe
 	}
 }
 
-// Execute executes the multimodal embedding generation
+/* Execute executes the multimodal embedding generation */
 func (t *EmbedMultimodalTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -164,7 +177,7 @@ func (t *EmbedMultimodalTool) Execute(ctx context.Context, params map[string]int
 		}), nil
 	}
 
-	// Decode base64 image data
+  /* Decode base64 image data */
 	imageBytes, err := base64.StdEncoding.DecodeString(imageData)
 	if err != nil {
 		return Error(fmt.Sprintf("Invalid base64 image data: %v", err), "VALIDATION_ERROR", map[string]interface{}{
@@ -193,14 +206,14 @@ func (t *EmbedMultimodalTool) Execute(ctx context.Context, params map[string]int
 	}), nil
 }
 
-// EmbedCachedTool generates cached embeddings
+/* EmbedCachedTool generates cached embeddings */
 type EmbedCachedTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewEmbedCachedTool creates a new cached embedding tool
+/* NewEmbedCachedTool creates a new cached embedding tool */
 func NewEmbedCachedTool(db *database.Database, logger *logging.Logger) *EmbedCachedTool {
 	return &EmbedCachedTool{
 		BaseTool: NewBaseTool(
@@ -227,7 +240,7 @@ func NewEmbedCachedTool(db *database.Database, logger *logging.Logger) *EmbedCac
 	}
 }
 
-// Execute executes the cached embedding generation
+/* Execute executes the cached embedding generation */
 func (t *EmbedCachedTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -270,14 +283,14 @@ func (t *EmbedCachedTool) Execute(ctx context.Context, params map[string]interfa
 	}), nil
 }
 
-// ConfigureEmbeddingModelTool configures embedding model
+/* ConfigureEmbeddingModelTool configures embedding model */
 type ConfigureEmbeddingModelTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewConfigureEmbeddingModelTool creates a new embedding model configuration tool
+/* NewConfigureEmbeddingModelTool creates a new embedding model configuration tool */
 func NewConfigureEmbeddingModelTool(db *database.Database, logger *logging.Logger) *ConfigureEmbeddingModelTool {
 	return &ConfigureEmbeddingModelTool{
 		BaseTool: NewBaseTool(
@@ -303,7 +316,7 @@ func NewConfigureEmbeddingModelTool(db *database.Database, logger *logging.Logge
 	}
 }
 
-// Execute executes the embedding model configuration
+/* Execute executes the embedding model configuration */
 func (t *ConfigureEmbeddingModelTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -349,14 +362,14 @@ func (t *ConfigureEmbeddingModelTool) Execute(ctx context.Context, params map[st
 	}), nil
 }
 
-// GetEmbeddingModelConfigTool gets embedding model configuration
+/* GetEmbeddingModelConfigTool gets embedding model configuration */
 type GetEmbeddingModelConfigTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewGetEmbeddingModelConfigTool creates a new get embedding model config tool
+/* NewGetEmbeddingModelConfigTool creates a new get embedding model config tool */
 func NewGetEmbeddingModelConfigTool(db *database.Database, logger *logging.Logger) *GetEmbeddingModelConfigTool {
 	return &GetEmbeddingModelConfigTool{
 		BaseTool: NewBaseTool(
@@ -378,7 +391,7 @@ func NewGetEmbeddingModelConfigTool(db *database.Database, logger *logging.Logge
 	}
 }
 
-// Execute executes the get embedding model config
+/* Execute executes the get embedding model config */
 func (t *GetEmbeddingModelConfigTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -416,14 +429,14 @@ func (t *GetEmbeddingModelConfigTool) Execute(ctx context.Context, params map[st
 	}), nil
 }
 
-// ListEmbeddingModelConfigsTool lists all embedding model configurations
+/* ListEmbeddingModelConfigsTool lists all embedding model configurations */
 type ListEmbeddingModelConfigsTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewListEmbeddingModelConfigsTool creates a new list embedding model configs tool
+/* NewListEmbeddingModelConfigsTool creates a new list embedding model configs tool */
 func NewListEmbeddingModelConfigsTool(db *database.Database, logger *logging.Logger) *ListEmbeddingModelConfigsTool {
 	return &ListEmbeddingModelConfigsTool{
 		BaseTool: NewBaseTool(
@@ -440,7 +453,7 @@ func NewListEmbeddingModelConfigsTool(db *database.Database, logger *logging.Log
 	}
 }
 
-// Execute executes the list embedding model configs
+/* Execute executes the list embedding model configs */
 func (t *ListEmbeddingModelConfigsTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	query := "SELECT * FROM list_embedding_model_configs()"
 	results, err := t.executor.ExecuteQuery(ctx, query, nil)
@@ -459,14 +472,14 @@ func (t *ListEmbeddingModelConfigsTool) Execute(ctx context.Context, params map[
 	}), nil
 }
 
-// DeleteEmbeddingModelConfigTool deletes embedding model configuration
+/* DeleteEmbeddingModelConfigTool deletes embedding model configuration */
 type DeleteEmbeddingModelConfigTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewDeleteEmbeddingModelConfigTool creates a new delete embedding model config tool
+/* NewDeleteEmbeddingModelConfigTool creates a new delete embedding model config tool */
 func NewDeleteEmbeddingModelConfigTool(db *database.Database, logger *logging.Logger) *DeleteEmbeddingModelConfigTool {
 	return &DeleteEmbeddingModelConfigTool{
 		BaseTool: NewBaseTool(
@@ -488,7 +501,7 @@ func NewDeleteEmbeddingModelConfigTool(db *database.Database, logger *logging.Lo
 	}
 }
 
-// Execute executes the delete embedding model config
+/* Execute executes the delete embedding model config */
 func (t *DeleteEmbeddingModelConfigTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -528,3 +541,4 @@ func (t *DeleteEmbeddingModelConfigTool) Execute(ctx context.Context, params map
 
 
 
+
diff --git a/NeuronMCP/internal/tools/executor.go b/NeuronMCP/internal/tools/executor.go
index 841ce78..b181201 100644
--- a/NeuronMCP/internal/tools/executor.go
+++ b/NeuronMCP/internal/tools/executor.go
@@ -1,3 +1,19 @@
+/*-------------------------------------------------------------------------
+ *
+ * executor.go
+ *    Query executor for NeuronMCP tools
+ *
+ * Provides database query execution functionality with timeouts and
+ * error handling for all tool operations.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/executor.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -11,25 +27,22 @@ import (
 )
 
 const (
-	// Default query timeout for all database operations
-	DefaultQueryTimeout = 60 * time.Second
-	// Embedding query timeout (embeddings can take longer)
-	EmbeddingQueryTimeout = 120 * time.Second
-	// Vector search timeout
-	VectorSearchTimeout = 30 * time.Second
+	DefaultQueryTimeout    = 60 * time.Second
+	EmbeddingQueryTimeout  = 120 * time.Second
+	VectorSearchTimeout    = 30 * time.Second
 )
 
-// QueryExecutor executes database queries for tools
+/* QueryExecutor executes database queries for tools */
 type QueryExecutor struct {
 	db *database.Database
 }
 
-// NewQueryExecutor creates a new query executor
+/* NewQueryExecutor creates a new query executor */
 func NewQueryExecutor(db *database.Database) *QueryExecutor {
 	return &QueryExecutor{db: db}
 }
 
-// ExecuteVectorSearch executes a vector search query
+/* ExecuteVectorSearch executes a vector search query */
 func (e *QueryExecutor) ExecuteVectorSearch(ctx context.Context, table, vectorColumn string, queryVector []interface{}, distanceMetric string, limit int, additionalColumns []interface{}) ([]map[string]interface{}, error) {
 	if e.db == nil {
 		return nil, fmt.Errorf("query executor database instance is nil: cannot execute vector search on table '%s', column '%s'", table, vectorColumn)
@@ -51,7 +64,6 @@ func (e *QueryExecutor) ExecuteVectorSearch(ctx context.Context, table, vectorCo
 		return nil, fmt.Errorf("query vector cannot be empty: vector search on table '%s', column '%s' requires a non-empty query vector", table, vectorColumn)
 	}
 	
-	// Convert queryVector to []float32
 	vec := make([]float32, 0, len(queryVector))
 	for i, v := range queryVector {
 		if f, ok := v.(float64); ok {
@@ -63,7 +75,6 @@ func (e *QueryExecutor) ExecuteVectorSearch(ctx context.Context, table, vectorCo
 		}
 	}
 
-	// Convert additional columns to []string
 	cols := make([]string, 0, len(additionalColumns))
 	for i, col := range additionalColumns {
 		if str, ok := col.(string); ok {
@@ -76,13 +87,11 @@ func (e *QueryExecutor) ExecuteVectorSearch(ctx context.Context, table, vectorCo
 		}
 	}
 
-	// Validate distance metric
 	validMetrics := map[string]bool{"l2": true, "cosine": true, "inner_product": true, "l1": true, "hamming": true, "chebyshev": true, "minkowski": true}
 	if !validMetrics[distanceMetric] {
 		return nil, fmt.Errorf("invalid distance metric '%s' for vector search on table '%s', column '%s': valid metrics are l2, cosine, inner_product, l1, hamming, chebyshev, minkowski", distanceMetric, table, vectorColumn)
 	}
 
-	// Validate limit
 	if limit <= 0 {
 		return nil, fmt.Errorf("invalid limit %d for vector search on table '%s', column '%s': limit must be greater than 0", limit, table, vectorColumn)
 	}
@@ -93,7 +102,6 @@ func (e *QueryExecutor) ExecuteVectorSearch(ctx context.Context, table, vectorCo
 	qb := &database.QueryBuilder{}
 	query, params := qb.VectorSearch(table, vectorColumn, vec, distanceMetric, limit, cols, nil)
 
-	// Create timeout context for vector search
 	queryCtx, cancel := context.WithTimeout(ctx, VectorSearchTimeout)
 	defer cancel()
 
@@ -114,7 +122,7 @@ func (e *QueryExecutor) ExecuteVectorSearch(ctx context.Context, table, vectorCo
 	return results, nil
 }
 
-// ExecuteQuery executes a query and returns all rows
+/* ExecuteQuery executes a query and returns all rows */
 func (e *QueryExecutor) ExecuteQuery(ctx context.Context, query string, params []interface{}) ([]map[string]interface{}, error) {
 	if e.db == nil {
 		return nil, fmt.Errorf("query executor database instance is nil: cannot execute query '%s' with %d parameters", query, len(params))
@@ -128,7 +136,6 @@ func (e *QueryExecutor) ExecuteQuery(ctx context.Context, query string, params [
 		return nil, fmt.Errorf("query string is empty: cannot execute empty query")
 	}
 	
-	// Create timeout context
 	queryCtx, cancel := context.WithTimeout(ctx, DefaultQueryTimeout)
 	defer cancel()
 	
@@ -149,12 +156,12 @@ func (e *QueryExecutor) ExecuteQuery(ctx context.Context, query string, params [
 	return results, nil
 }
 
-// ExecuteQueryOne executes a query and returns a single row
+/* ExecuteQueryOne executes a query and returns a single row */
 func (e *QueryExecutor) ExecuteQueryOne(ctx context.Context, query string, params []interface{}) (map[string]interface{}, error) {
 	return e.ExecuteQueryOneWithTimeout(ctx, query, params, DefaultQueryTimeout)
 }
 
-// ExecuteQueryOneWithTimeout executes a query with a specific timeout
+/* ExecuteQueryOneWithTimeout executes a query with a specific timeout */
 func (e *QueryExecutor) ExecuteQueryOneWithTimeout(ctx context.Context, query string, params []interface{}, timeout time.Duration) (map[string]interface{}, error) {
 	if e.db == nil {
 		return nil, fmt.Errorf("query executor database instance is nil: cannot execute single-row query '%s' with %d parameters", query, len(params))
@@ -168,7 +175,6 @@ func (e *QueryExecutor) ExecuteQueryOneWithTimeout(ctx context.Context, query st
 		return nil, fmt.Errorf("query string is empty: cannot execute empty query for single row result")
 	}
 	
-	// Create timeout context
 	queryCtx, cancel := context.WithTimeout(ctx, timeout)
 	defer cancel()
 	
@@ -191,7 +197,6 @@ func (e *QueryExecutor) ExecuteQueryOneWithTimeout(ctx context.Context, query st
 		return nil, fmt.Errorf("multiple rows returned from single-row query: query='%s', parameter_count=%d, parameters=%v (expected exactly one row, got at least two)", query, len(params), params)
 	}
 
-	// Check if context was cancelled (timeout)
 	if queryCtx.Err() != nil {
 		return nil, fmt.Errorf("query timeout after %v: query='%s', parameter_count=%d, error=%w", timeout, query, len(params), queryCtx.Err())
 	}
@@ -199,7 +204,7 @@ func (e *QueryExecutor) ExecuteQueryOneWithTimeout(ctx context.Context, query st
 	return result, nil
 }
 
-// Exec executes a query without returning rows (for DDL statements)
+/* Exec executes a query without returning rows (for DDL statements) */
 func (e *QueryExecutor) Exec(ctx context.Context, query string, params []interface{}) error {
 	if e.db == nil {
 		return fmt.Errorf("query executor database instance is nil: cannot execute DDL query '%s' with %d parameters", query, len(params))
@@ -220,7 +225,7 @@ func (e *QueryExecutor) Exec(ctx context.Context, query string, params []interfa
 	return nil
 }
 
-// scanRowsToMaps scans all rows into maps
+/* scanRowsToMaps scans all rows into maps */
 func scanRowsToMaps(rows pgx.Rows) ([]map[string]interface{}, error) {
 	var results []map[string]interface{}
 	rowNum := 0
@@ -246,7 +251,7 @@ func scanRowsToMaps(rows pgx.Rows) ([]map[string]interface{}, error) {
 	return results, nil
 }
 
-// scanRowToMap scans a single row into a map
+/* scanRowToMap scans a single row into a map */
 func scanRowToMap(rows pgx.Rows) (map[string]interface{}, error) {
 	fieldDescriptions := rows.FieldDescriptions()
 	if len(fieldDescriptions) == 0 {
@@ -269,9 +274,7 @@ func scanRowToMap(rows pgx.Rows) (map[string]interface{}, error) {
 	result := make(map[string]interface{})
 	for i, desc := range fieldDescriptions {
 		val := values[i]
-		// Handle byte arrays (JSON, text, etc.)
 		if bytes, ok := val.([]byte); ok {
-			// Try to parse as JSON
 			var jsonVal interface{}
 			if err := json.Unmarshal(bytes, &jsonVal); err == nil {
 				val = jsonVal
diff --git a/NeuronMCP/internal/tools/gpu.go b/NeuronMCP/internal/tools/gpu.go
index 4af1b4c..f28d7db 100644
--- a/NeuronMCP/internal/tools/gpu.go
+++ b/NeuronMCP/internal/tools/gpu.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * gpu.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/gpu.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// GPUMonitoringTool monitors GPU information
+/* GPUMonitoringTool monitors GPU information */
 type GPUMonitoringTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewGPUMonitoringTool creates a new GPU monitoring tool
+/* NewGPUMonitoringTool creates a new GPU monitoring tool */
 func NewGPUMonitoringTool(db *database.Database, logger *logging.Logger) *GPUMonitoringTool {
 	return &GPUMonitoringTool{
 		BaseTool: NewBaseTool(
@@ -32,7 +45,7 @@ func NewGPUMonitoringTool(db *database.Database, logger *logging.Logger) *GPUMon
 	}
 }
 
-// Execute executes GPU info query
+/* Execute executes GPU info query */
 func (t *GPUMonitoringTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	query := "SELECT * FROM neurondb.gpu_info()"
 	result, err := t.executor.ExecuteQueryOne(ctx, query, nil)
@@ -48,3 +61,4 @@ func (t *GPUMonitoringTool) Execute(ctx context.Context, params map[string]inter
 
 
 
+
diff --git a/NeuronMCP/internal/tools/hybrid_search.go b/NeuronMCP/internal/tools/hybrid_search.go
index 72c4b33..7c3abec 100644
--- a/NeuronMCP/internal/tools/hybrid_search.go
+++ b/NeuronMCP/internal/tools/hybrid_search.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * hybrid_search.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/hybrid_search.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -9,14 +22,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// HybridSearchTool performs hybrid semantic + lexical search
+/* HybridSearchTool performs hybrid semantic + lexical search */
 type HybridSearchTool struct {
 	*BaseTool
 	db     *database.Database
 	logger *logging.Logger
 }
 
-// NewHybridSearchTool creates a new HybridSearchTool
+/* NewHybridSearchTool creates a new HybridSearchTool */
 func NewHybridSearchTool(db *database.Database, logger *logging.Logger) *HybridSearchTool {
 	return &HybridSearchTool{
 		BaseTool: NewBaseTool(
@@ -73,7 +86,7 @@ func NewHybridSearchTool(db *database.Database, logger *logging.Logger) *HybridS
 	}
 }
 
-// Execute performs hybrid search
+/* Execute performs hybrid search */
 func (t *HybridSearchTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -152,10 +165,10 @@ func (t *HybridSearchTool) Execute(ctx context.Context, params map[string]interf
 		}), nil
 	}
 
-	// Format query vector
+  /* Format query vector */
 	vectorStr := formatVectorFromInterface(queryVector)
 
-	// Format filters as JSON string
+  /* Format filters as JSON string */
 	filtersJSON := "{}"
 	if len(filters) > 0 {
 		filtersBytes, err := json.Marshal(filters)
@@ -164,7 +177,7 @@ func (t *HybridSearchTool) Execute(ctx context.Context, params map[string]interf
 		}
 	}
 
-	// Use NeuronDB's hybrid_search function: hybrid_search(table, query_vec, query_text, filters, vector_weight, limit)
+  /* Use NeuronDB's hybrid_search function: hybrid_search(table, query_vec, query_text, filters, vector_weight, limit) */
 	query := `SELECT hybrid_search($1, $2::vector, $3, $4::text, $5, $6) AS results`
 	executor := NewQueryExecutor(t.db)
 	result, err := executor.ExecuteQueryOne(ctx, query, []interface{}{
diff --git a/NeuronMCP/internal/tools/hybrid_search_complete.go b/NeuronMCP/internal/tools/hybrid_search_complete.go
index 9816738..50512b1 100644
--- a/NeuronMCP/internal/tools/hybrid_search_complete.go
+++ b/NeuronMCP/internal/tools/hybrid_search_complete.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * hybrid_search_complete.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/hybrid_search_complete.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -9,14 +22,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// ReciprocalRankFusionTool performs reciprocal rank fusion
+/* ReciprocalRankFusionTool performs reciprocal rank fusion */
 type ReciprocalRankFusionTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewReciprocalRankFusionTool creates a new RRF tool
+/* NewReciprocalRankFusionTool creates a new RRF tool */
 func NewReciprocalRankFusionTool(db *database.Database, logger *logging.Logger) *ReciprocalRankFusionTool {
 	return &ReciprocalRankFusionTool{
 		BaseTool: NewBaseTool(
@@ -47,7 +60,7 @@ func NewReciprocalRankFusionTool(db *database.Database, logger *logging.Logger)
 	}
 }
 
-// Execute executes RRF
+/* Execute executes RRF */
 func (t *ReciprocalRankFusionTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -67,7 +80,7 @@ func (t *ReciprocalRankFusionTool) Execute(ctx context.Context, params map[strin
 		return Error("rankings cannot be empty", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Format rankings array for PostgreSQL
+  /* Format rankings array for PostgreSQL */
 	var rankingStrs []string
 	for _, ranking := range rankings {
 		if arr, ok := ranking.([]interface{}); ok {
@@ -98,14 +111,14 @@ func (t *ReciprocalRankFusionTool) Execute(ctx context.Context, params map[strin
 	}), nil
 }
 
-// SemanticKeywordSearchTool performs semantic + keyword search
+/* SemanticKeywordSearchTool performs semantic + keyword search */
 type SemanticKeywordSearchTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewSemanticKeywordSearchTool creates a new semantic-keyword search tool
+/* NewSemanticKeywordSearchTool creates a new semantic-keyword search tool */
 func NewSemanticKeywordSearchTool(db *database.Database, logger *logging.Logger) *SemanticKeywordSearchTool {
 	return &SemanticKeywordSearchTool{
 		BaseTool: NewBaseTool(
@@ -143,7 +156,7 @@ func NewSemanticKeywordSearchTool(db *database.Database, logger *logging.Logger)
 	}
 }
 
-// Execute executes semantic-keyword search
+/* Execute executes semantic-keyword search */
 func (t *SemanticKeywordSearchTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -185,14 +198,14 @@ func (t *SemanticKeywordSearchTool) Execute(ctx context.Context, params map[stri
 	}), nil
 }
 
-// MultiVectorSearchTool performs multi-vector search
+/* MultiVectorSearchTool performs multi-vector search */
 type MultiVectorSearchTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewMultiVectorSearchTool creates a new multi-vector search tool
+/* NewMultiVectorSearchTool creates a new multi-vector search tool */
 func NewMultiVectorSearchTool(db *database.Database, logger *logging.Logger) *MultiVectorSearchTool {
 	return &MultiVectorSearchTool{
 		BaseTool: NewBaseTool(
@@ -235,7 +248,7 @@ func NewMultiVectorSearchTool(db *database.Database, logger *logging.Logger) *Mu
 	}
 }
 
-// Execute executes multi-vector search
+/* Execute executes multi-vector search */
 func (t *MultiVectorSearchTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -260,7 +273,7 @@ func (t *MultiVectorSearchTool) Execute(ctx context.Context, params map[string]i
 		return Error("table and query_vectors are required", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Format vectors array
+  /* Format vectors array */
 	var vecStrs []string
 	for _, vec := range queryVectors {
 		if arr, ok := vec.([]interface{}); ok {
@@ -289,14 +302,14 @@ func (t *MultiVectorSearchTool) Execute(ctx context.Context, params map[string]i
 	}), nil
 }
 
-// FacetedVectorSearchTool performs faceted search
+/* FacetedVectorSearchTool performs faceted search */
 type FacetedVectorSearchTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewFacetedVectorSearchTool creates a new faceted search tool
+/* NewFacetedVectorSearchTool creates a new faceted search tool */
 func NewFacetedVectorSearchTool(db *database.Database, logger *logging.Logger) *FacetedVectorSearchTool {
 	return &FacetedVectorSearchTool{
 		BaseTool: NewBaseTool(
@@ -334,7 +347,7 @@ func NewFacetedVectorSearchTool(db *database.Database, logger *logging.Logger) *
 	}
 }
 
-// Execute executes faceted search
+/* Execute executes faceted search */
 func (t *FacetedVectorSearchTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -376,14 +389,14 @@ func (t *FacetedVectorSearchTool) Execute(ctx context.Context, params map[string
 	}), nil
 }
 
-// TemporalVectorSearchTool performs temporal search
+/* TemporalVectorSearchTool performs temporal search */
 type TemporalVectorSearchTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewTemporalVectorSearchTool creates a new temporal search tool
+/* NewTemporalVectorSearchTool creates a new temporal search tool */
 func NewTemporalVectorSearchTool(db *database.Database, logger *logging.Logger) *TemporalVectorSearchTool {
 	return &TemporalVectorSearchTool{
 		BaseTool: NewBaseTool(
@@ -426,7 +439,7 @@ func NewTemporalVectorSearchTool(db *database.Database, logger *logging.Logger)
 	}
 }
 
-// Execute executes temporal search
+/* Execute executes temporal search */
 func (t *TemporalVectorSearchTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -472,14 +485,14 @@ func (t *TemporalVectorSearchTool) Execute(ctx context.Context, params map[strin
 	}), nil
 }
 
-// DiverseVectorSearchTool performs diverse search
+/* DiverseVectorSearchTool performs diverse search */
 type DiverseVectorSearchTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewDiverseVectorSearchTool creates a new diverse search tool
+/* NewDiverseVectorSearchTool creates a new diverse search tool */
 func NewDiverseVectorSearchTool(db *database.Database, logger *logging.Logger) *DiverseVectorSearchTool {
 	return &DiverseVectorSearchTool{
 		BaseTool: NewBaseTool(
@@ -520,7 +533,7 @@ func NewDiverseVectorSearchTool(db *database.Database, logger *logging.Logger) *
 	}
 }
 
-// Execute executes diverse search
+/* Execute executes diverse search */
 func (t *DiverseVectorSearchTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -567,3 +580,4 @@ func (t *DiverseVectorSearchTool) Execute(ctx context.Context, params map[string
 
 
 
+
diff --git a/NeuronMCP/internal/tools/indexing.go b/NeuronMCP/internal/tools/indexing.go
index de3aa31..f9ffa5b 100644
--- a/NeuronMCP/internal/tools/indexing.go
+++ b/NeuronMCP/internal/tools/indexing.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * indexing.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/indexing.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// CreateHNSWIndexTool creates an HNSW index
+/* CreateHNSWIndexTool creates an HNSW index */
 type CreateHNSWIndexTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewCreateHNSWIndexTool creates a new create HNSW index tool
+/* NewCreateHNSWIndexTool creates a new create HNSW index tool */
 func NewCreateHNSWIndexTool(db *database.Database, logger *logging.Logger) *CreateHNSWIndexTool {
 	return &CreateHNSWIndexTool{
 		BaseTool: NewBaseTool(
@@ -59,7 +72,7 @@ func NewCreateHNSWIndexTool(db *database.Database, logger *logging.Logger) *Crea
 	}
 }
 
-// Execute executes the HNSW index creation
+/* Execute executes the HNSW index creation */
 func (t *CreateHNSWIndexTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -132,8 +145,8 @@ func (t *CreateHNSWIndexTool) Execute(ctx context.Context, params map[string]int
 		}), nil
 	}
 
-	// Use NeuronDB's unified index creation function
-	// neurondb.create_index(table_name, vector_col, index_type, params)
+  /* Use NeuronDB's unified index creation function */
+  /* neurondb.create_index(table_name, vector_col, index_type, params) */
 	paramsJSON := fmt.Sprintf(`{"m": %d, "ef_construction": %d}`, m, efConstruction)
 	query := `SELECT neurondb.create_index($1, $2, $3, $4::jsonb) AS result`
 	result, err := t.executor.ExecuteQueryOne(ctx, query, []interface{}{
@@ -158,14 +171,14 @@ func (t *CreateHNSWIndexTool) Execute(ctx context.Context, params map[string]int
 	}), nil
 }
 
-// CreateIVFIndexTool creates an IVF index
+/* CreateIVFIndexTool creates an IVF index */
 type CreateIVFIndexTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewCreateIVFIndexTool creates a new create IVF index tool
+/* NewCreateIVFIndexTool creates a new create IVF index tool */
 func NewCreateIVFIndexTool(db *database.Database, logger *logging.Logger) *CreateIVFIndexTool {
 	return &CreateIVFIndexTool{
 		BaseTool: NewBaseTool(
@@ -201,7 +214,7 @@ func NewCreateIVFIndexTool(db *database.Database, logger *logging.Logger) *Creat
 	}
 }
 
-// Execute executes the IVF index creation
+/* Execute executes the IVF index creation */
 func (t *CreateIVFIndexTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -255,8 +268,8 @@ func (t *CreateIVFIndexTool) Execute(ctx context.Context, params map[string]inte
 		}), nil
 	}
 
-	// Use NeuronDB's unified index creation function
-	// neurondb.create_index(table_name, vector_col, index_type, params)
+  /* Use NeuronDB's unified index creation function */
+  /* neurondb.create_index(table_name, vector_col, index_type, params) */
 	paramsJSON := fmt.Sprintf(`{"num_lists": %d}`, numLists)
 	query := `SELECT neurondb.create_index($1, $2, $3, $4::jsonb) AS result`
 	result, err := t.executor.ExecuteQueryOne(ctx, query, []interface{}{
@@ -279,14 +292,14 @@ func (t *CreateIVFIndexTool) Execute(ctx context.Context, params map[string]inte
 	}), nil
 }
 
-// IndexStatusTool gets index status and statistics
+/* IndexStatusTool gets index status and statistics */
 type IndexStatusTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewIndexStatusTool creates a new index status tool
+/* NewIndexStatusTool creates a new index status tool */
 func NewIndexStatusTool(db *database.Database, logger *logging.Logger) *IndexStatusTool {
 	return &IndexStatusTool{
 		BaseTool: NewBaseTool(
@@ -308,7 +321,7 @@ func NewIndexStatusTool(db *database.Database, logger *logging.Logger) *IndexSta
 	}
 }
 
-// Execute executes the index status query
+/* Execute executes the index status query */
 func (t *IndexStatusTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -359,14 +372,14 @@ func (t *IndexStatusTool) Execute(ctx context.Context, params map[string]interfa
 	}), nil
 }
 
-// DropIndexTool drops a vector index
+/* DropIndexTool drops a vector index */
 type DropIndexTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewDropIndexTool creates a new drop index tool
+/* NewDropIndexTool creates a new drop index tool */
 func NewDropIndexTool(db *database.Database, logger *logging.Logger) *DropIndexTool {
 	return &DropIndexTool{
 		BaseTool: NewBaseTool(
@@ -388,7 +401,7 @@ func NewDropIndexTool(db *database.Database, logger *logging.Logger) *DropIndexT
 	}
 }
 
-// Execute executes the index drop
+/* Execute executes the index drop */
 func (t *DropIndexTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -407,7 +420,7 @@ func (t *DropIndexTool) Execute(ctx context.Context, params map[string]interface
 		}), nil
 	}
 
-	// Escape identifier for safety
+  /* Escape identifier for safety */
 	escapedName := database.EscapeIdentifier(indexName)
 	query := fmt.Sprintf("DROP INDEX IF EXISTS %s", escapedName)
 
diff --git a/NeuronMCP/internal/tools/indexing_additional.go b/NeuronMCP/internal/tools/indexing_additional.go
index c27d4f1..4e30713 100644
--- a/NeuronMCP/internal/tools/indexing_additional.go
+++ b/NeuronMCP/internal/tools/indexing_additional.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * indexing_additional.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/indexing_additional.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// TuneHNSWIndexTool automatically tunes HNSW index parameters
+/* TuneHNSWIndexTool automatically tunes HNSW index parameters */
 type TuneHNSWIndexTool struct {
 	*BaseTool
 	db     *database.Database
 	logger *logging.Logger
 }
 
-// NewTuneHNSWIndexTool creates a new TuneHNSWIndexTool
+/* NewTuneHNSWIndexTool creates a new TuneHNSWIndexTool */
 func NewTuneHNSWIndexTool(db *database.Database, logger *logging.Logger) *TuneHNSWIndexTool {
 	return &TuneHNSWIndexTool{
 		BaseTool: NewBaseTool(
@@ -41,7 +54,7 @@ func NewTuneHNSWIndexTool(db *database.Database, logger *logging.Logger) *TuneHN
 	}
 }
 
-// Execute tunes the HNSW index
+/* Execute tunes the HNSW index */
 func (t *TuneHNSWIndexTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -69,7 +82,7 @@ func (t *TuneHNSWIndexTool) Execute(ctx context.Context, params map[string]inter
 		}), nil
 	}
 
-	// Use NeuronDB's index_tune_hnsw function: index_tune_hnsw(table, vector_col)
+  /* Use NeuronDB's index_tune_hnsw function: index_tune_hnsw(table, vector_col) */
 	query := `SELECT index_tune_hnsw($1, $2) AS tuning_result`
 	executor := NewQueryExecutor(t.db)
 	result, err := executor.ExecuteQueryOne(ctx, query, []interface{}{table, vectorColumn})
@@ -89,14 +102,14 @@ func (t *TuneHNSWIndexTool) Execute(ctx context.Context, params map[string]inter
 	}), nil
 }
 
-// TuneIVFIndexTool automatically tunes IVF index parameters
+/* TuneIVFIndexTool automatically tunes IVF index parameters */
 type TuneIVFIndexTool struct {
 	*BaseTool
 	db     *database.Database
 	logger *logging.Logger
 }
 
-// NewTuneIVFIndexTool creates a new TuneIVFIndexTool
+/* NewTuneIVFIndexTool creates a new TuneIVFIndexTool */
 func NewTuneIVFIndexTool(db *database.Database, logger *logging.Logger) *TuneIVFIndexTool {
 	return &TuneIVFIndexTool{
 		BaseTool: NewBaseTool(
@@ -122,7 +135,7 @@ func NewTuneIVFIndexTool(db *database.Database, logger *logging.Logger) *TuneIVF
 	}
 }
 
-// Execute tunes the IVF index
+/* Execute tunes the IVF index */
 func (t *TuneIVFIndexTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -150,7 +163,7 @@ func (t *TuneIVFIndexTool) Execute(ctx context.Context, params map[string]interf
 		}), nil
 	}
 
-	// Use NeuronDB's index_tune_ivf function: index_tune_ivf(table, vector_col)
+  /* Use NeuronDB's index_tune_ivf function: index_tune_ivf(table, vector_col) */
 	query := `SELECT index_tune_ivf($1, $2) AS tuning_result`
 	executor := NewQueryExecutor(t.db)
 	result, err := executor.ExecuteQueryOne(ctx, query, []interface{}{table, vectorColumn})
@@ -173,3 +186,4 @@ func (t *TuneIVFIndexTool) Execute(ctx context.Context, params map[string]interf
 
 
 
+
diff --git a/NeuronMCP/internal/tools/interfaces.go b/NeuronMCP/internal/tools/interfaces.go
index b6c670f..9203c04 100644
--- a/NeuronMCP/internal/tools/interfaces.go
+++ b/NeuronMCP/internal/tools/interfaces.go
@@ -1,8 +1,23 @@
+/*-------------------------------------------------------------------------
+ *
+ * interfaces.go
+ *    Tool interfaces for NeuronMCP
+ *
+ * Defines the core interfaces that all tools must implement.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/interfaces.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import "context"
 
-// Tool is the interface that all tools must implement
+/* Tool is the interface that all tools must implement */
 type Tool interface {
 	Name() string
 	Description() string
@@ -10,7 +25,7 @@ type Tool interface {
 	Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error)
 }
 
-// ToolExecutor provides database query execution for tools
+/* ToolExecutor provides database query execution for tools */
 type ToolExecutor interface {
 	ExecuteQuery(ctx context.Context, query string, params []interface{}) ([]map[string]interface{}, error)
 	ExecuteQueryOne(ctx context.Context, query string, params []interface{}) (map[string]interface{}, error)
diff --git a/NeuronMCP/internal/tools/ml.go b/NeuronMCP/internal/tools/ml.go
index c487c9f..9dc6fac 100644
--- a/NeuronMCP/internal/tools/ml.go
+++ b/NeuronMCP/internal/tools/ml.go
@@ -1,3 +1,19 @@
+/*-------------------------------------------------------------------------
+ *
+ * ml.go
+ *    Machine learning tools for NeuronMCP
+ *
+ * Provides tools for training, predicting, evaluating, and managing ML models
+ * using the NeuronDB unified train function.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/ml.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -10,14 +26,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// TrainModelTool trains an ML model using the unified neurondb.train function
+/* TrainModelTool trains an ML model using the unified neurondb.train function */
 type TrainModelTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewTrainModelTool creates a new train model tool
+/* NewTrainModelTool creates a new train model tool */
 func NewTrainModelTool(db *database.Database, logger *logging.Logger) *TrainModelTool {
 	return &TrainModelTool{
 		BaseTool: NewBaseTool(
@@ -60,7 +76,7 @@ func NewTrainModelTool(db *database.Database, logger *logging.Logger) *TrainMode
 	}
 }
 
-// Execute executes the model training
+/* Execute executes the model training */
 func (t *TrainModelTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -109,7 +125,6 @@ func (t *TrainModelTool) Execute(ctx context.Context, params map[string]interfac
 		}), nil
 	}
 
-	// Build params JSON
 	paramsJSON := "{}"
 	if p, ok := params["params"].(map[string]interface{}); ok && len(p) > 0 {
 		paramsBytes, err := json.Marshal(p)
@@ -118,14 +133,12 @@ func (t *TrainModelTool) Execute(ctx context.Context, params map[string]interfac
 		}
 	}
 
-	// Use unified train function
 	project := "default"
 	if p, ok := params["project"].(string); ok && p != "" {
 		project = p
 	}
 
-	// NeuronDB train function signature: neurondb.train(project_name, algorithm, table_name, label_col, feature_columns[], params)
-	// Convert featureCol to array format
+	/* NeuronDB train function signature: neurondb.train(project_name, algorithm, table_name, label_col, feature_columns[], params) */
 	query := `SELECT neurondb.train($1, $2, $3, $4, $5::text[], $6::jsonb) AS model_id`
 	result, err := t.executor.ExecuteQueryOne(ctx, query, []interface{}{
 		project, algorithm, table, labelCol, []string{featureCol}, paramsJSON,
@@ -149,14 +162,14 @@ func (t *TrainModelTool) Execute(ctx context.Context, params map[string]interfac
 	}), nil
 }
 
-// PredictTool predicts using a trained ML model
+/* PredictTool predicts using a trained ML model */
 type PredictTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewPredictTool creates a new predict tool
+/* NewPredictTool creates a new predict tool */
 func NewPredictTool(db *database.Database, logger *logging.Logger) *PredictTool {
 	return &PredictTool{
 		BaseTool: NewBaseTool(
@@ -183,7 +196,7 @@ func NewPredictTool(db *database.Database, logger *logging.Logger) *PredictTool
 	}
 }
 
-// Execute executes the prediction
+/* Execute executes the prediction */
 func (t *PredictTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -231,7 +244,6 @@ func (t *PredictTool) Execute(ctx context.Context, params map[string]interface{}
 		}), nil
 	}
 
-	// Convert features to vector format
 	vectorStr := formatVectorFromInterface(features)
 
 	query := `SELECT neurondb.predict($1::integer, $2::vector) AS prediction`
@@ -250,14 +262,14 @@ func (t *PredictTool) Execute(ctx context.Context, params map[string]interface{}
 	}), nil
 }
 
-// EvaluateModelTool evaluates a trained ML model
+/* EvaluateModelTool evaluates a trained ML model */
 type EvaluateModelTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewEvaluateModelTool creates a new evaluate model tool
+/* NewEvaluateModelTool creates a new evaluate model tool */
 func NewEvaluateModelTool(db *database.Database, logger *logging.Logger) *EvaluateModelTool {
 	return &EvaluateModelTool{
 		BaseTool: NewBaseTool(
@@ -291,7 +303,7 @@ func NewEvaluateModelTool(db *database.Database, logger *logging.Logger) *Evalua
 	}
 }
 
-// Execute executes the model evaluation
+/* Execute executes the model evaluation */
 func (t *EvaluateModelTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -365,14 +377,14 @@ func (t *EvaluateModelTool) Execute(ctx context.Context, params map[string]inter
 	}), nil
 }
 
-// ListModelsTool lists all trained models
+/* ListModelsTool lists all trained models */
 type ListModelsTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewListModelsTool creates a new list models tool
+/* NewListModelsTool creates a new list models tool */
 func NewListModelsTool(db *database.Database, logger *logging.Logger) *ListModelsTool {
 	return &ListModelsTool{
 		BaseTool: NewBaseTool(
@@ -397,7 +409,7 @@ func NewListModelsTool(db *database.Database, logger *logging.Logger) *ListModel
 	}
 }
 
-// Execute executes the list models query
+/* Execute executes the list models query */
 func (t *ListModelsTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	query := `SELECT model_id, algorithm, training_table, created_at, updated_at FROM neurondb.ml_models WHERE 1=1`
 	queryParams := []interface{}{}
@@ -444,14 +456,14 @@ func (t *ListModelsTool) Execute(ctx context.Context, params map[string]interfac
 	}), nil
 }
 
-// GetModelInfoTool gets detailed information about a model
+/* GetModelInfoTool gets detailed information about a model */
 type GetModelInfoTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewGetModelInfoTool creates a new get model info tool
+/* NewGetModelInfoTool creates a new get model info tool */
 func NewGetModelInfoTool(db *database.Database, logger *logging.Logger) *GetModelInfoTool {
 	return &GetModelInfoTool{
 		BaseTool: NewBaseTool(
@@ -473,7 +485,7 @@ func NewGetModelInfoTool(db *database.Database, logger *logging.Logger) *GetMode
 	}
 }
 
-// Execute executes the get model info query
+/* Execute executes the get model info query */
 func (t *GetModelInfoTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -514,14 +526,14 @@ func (t *GetModelInfoTool) Execute(ctx context.Context, params map[string]interf
 	return Success(result, nil), nil
 }
 
-// DeleteModelTool deletes a trained model
+/* DeleteModelTool deletes a trained model */
 type DeleteModelTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewDeleteModelTool creates a new delete model tool
+/* NewDeleteModelTool creates a new delete model tool */
 func NewDeleteModelTool(db *database.Database, logger *logging.Logger) *DeleteModelTool {
 	return &DeleteModelTool{
 		BaseTool: NewBaseTool(
@@ -543,7 +555,7 @@ func NewDeleteModelTool(db *database.Database, logger *logging.Logger) *DeleteMo
 	}
 }
 
-// Execute executes the model deletion
+/* Execute executes the model deletion */
 func (t *DeleteModelTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -588,7 +600,7 @@ func (t *DeleteModelTool) Execute(ctx context.Context, params map[string]interfa
 	}), nil
 }
 
-// Helper function to format vector from interface slice
+/* formatVectorFromInterface formats vector from interface slice */
 func formatVectorFromInterface(vec []interface{}) string {
 	var parts []string
 	for _, v := range vec {
diff --git a/NeuronMCP/internal/tools/ml_additional.go b/NeuronMCP/internal/tools/ml_additional.go
index 4609724..d2adc26 100644
--- a/NeuronMCP/internal/tools/ml_additional.go
+++ b/NeuronMCP/internal/tools/ml_additional.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * ml_additional.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/ml_additional.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// PredictBatchTool performs batch prediction using a trained ML model
+/* PredictBatchTool performs batch prediction using a trained ML model */
 type PredictBatchTool struct {
 	*BaseTool
 	db     *database.Database
 	logger *logging.Logger
 }
 
-// NewPredictBatchTool creates a new PredictBatchTool
+/* NewPredictBatchTool creates a new PredictBatchTool */
 func NewPredictBatchTool(db *database.Database, logger *logging.Logger) *PredictBatchTool {
 	return &PredictBatchTool{
 		BaseTool: NewBaseTool(
@@ -47,7 +60,7 @@ func NewPredictBatchTool(db *database.Database, logger *logging.Logger) *Predict
 	}
 }
 
-// Execute performs batch prediction
+/* Execute performs batch prediction */
 func (t *PredictBatchTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -105,8 +118,8 @@ func (t *PredictBatchTool) Execute(ctx context.Context, params map[string]interf
 		}), nil
 	}
 
-	// Convert features_array to vector array format for PostgreSQL
-	// Format: ARRAY['[1,2,3]'::vector, '[4,5,6]'::vector]
+  /* Convert features_array to vector array format for PostgreSQL */
+  /* Format: ARRAY['[1,2,3]'::vector, '[4,5,6]'::vector] */
 	var vectorStrings []string
 	for i, features := range featuresArray {
 		featureVec, ok := features.([]interface{})
@@ -135,8 +148,8 @@ func (t *PredictBatchTool) Execute(ctx context.Context, params map[string]interf
 		vectorStrings = append(vectorStrings, vectorStr)
 	}
 
-	// Build query: SELECT neurondb.predict_batch(model_id, ARRAY[vector1, vector2, ...]::vector[])
-	// Need to build array of vectors properly
+  /* Build query: SELECT neurondb.predict_batch(model_id, ARRAY[vector1, vector2, ...]::vector[]) */
+  /* Need to build array of vectors properly */
 	if len(vectorStrings) == 0 {
 		return Error(fmt.Sprintf("No valid vectors in features_array for predict_batch tool: model_id=%d", modelIDInt), "VALIDATION_ERROR", map[string]interface{}{
 			"model_id": modelIDInt,
@@ -144,7 +157,7 @@ func (t *PredictBatchTool) Execute(ctx context.Context, params map[string]interf
 		}), nil
 	}
 
-	// Build array literal: ARRAY['[1,2,3]'::vector, '[4,5,6]'::vector]::vector[]
+  /* Build array literal: ARRAY['[1,2,3]'::vector, '[4,5,6]'::vector]::vector[] */
 	var arrayParts []string
 	for _, vecStr := range vectorStrings {
 		arrayParts = append(arrayParts, fmt.Sprintf("'%s'::vector", vecStr))
@@ -176,14 +189,14 @@ func (t *PredictBatchTool) Execute(ctx context.Context, params map[string]interf
 	}), nil
 }
 
-// ExportModelTool exports a trained ML model
+/* ExportModelTool exports a trained ML model */
 type ExportModelTool struct {
 	*BaseTool
 	db     *database.Database
 	logger *logging.Logger
 }
 
-// NewExportModelTool creates a new ExportModelTool
+/* NewExportModelTool creates a new ExportModelTool */
 func NewExportModelTool(db *database.Database, logger *logging.Logger) *ExportModelTool {
 	return &ExportModelTool{
 		BaseTool: NewBaseTool(
@@ -215,7 +228,7 @@ func NewExportModelTool(db *database.Database, logger *logging.Logger) *ExportMo
 	}
 }
 
-// Execute exports the model
+/* Execute exports the model */
 func (t *ExportModelTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -302,7 +315,7 @@ func (t *ExportModelTool) Execute(ctx context.Context, params map[string]interfa
 	}), nil
 }
 
-// Helper function to format vector array for PostgreSQL
+/* Helper function to format vector array for PostgreSQL */
 func formatVectorArray(vectorStrings []string) string {
 	if len(vectorStrings) == 0 {
 		return ""
diff --git a/NeuronMCP/internal/tools/onnx.go b/NeuronMCP/internal/tools/onnx.go
index 469439c..c3cf57a 100644
--- a/NeuronMCP/internal/tools/onnx.go
+++ b/NeuronMCP/internal/tools/onnx.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * onnx.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/onnx.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// ONNXTool manages ONNX models
+/* ONNXTool manages ONNX models */
 type ONNXTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewONNXTool creates a new ONNX tool
+/* NewONNXTool creates a new ONNX tool */
 func NewONNXTool(db *database.Database, logger *logging.Logger) *ONNXTool {
 	return &ONNXTool{
 		BaseTool: NewBaseTool(
@@ -51,7 +64,7 @@ func NewONNXTool(db *database.Database, logger *logging.Logger) *ONNXTool {
 	}
 }
 
-// Execute executes ONNX operation
+/* Execute executes ONNX operation */
 func (t *ONNXTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -113,3 +126,4 @@ func (t *ONNXTool) Execute(ctx context.Context, params map[string]interface{}) (
 
 
 
+
diff --git a/NeuronMCP/internal/tools/postgresql.go b/NeuronMCP/internal/tools/postgresql.go
index 74632ee..f5f5f1b 100644
--- a/NeuronMCP/internal/tools/postgresql.go
+++ b/NeuronMCP/internal/tools/postgresql.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * postgresql.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/postgresql.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// PostgreSQLVersionTool retrieves PostgreSQL version information
+/* PostgreSQLVersionTool retrieves PostgreSQL version information */
 type PostgreSQLVersionTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewPostgreSQLVersionTool creates a new PostgreSQL version tool
+/* NewPostgreSQLVersionTool creates a new PostgreSQL version tool */
 func NewPostgreSQLVersionTool(db *database.Database, logger *logging.Logger) *PostgreSQLVersionTool {
 	return &PostgreSQLVersionTool{
 		BaseTool: NewBaseTool(
@@ -32,9 +45,9 @@ func NewPostgreSQLVersionTool(db *database.Database, logger *logging.Logger) *Po
 	}
 }
 
-// Execute executes the PostgreSQL version query
+/* Execute executes the PostgreSQL version query */
 func (t *PostgreSQLVersionTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
-	// Query PostgreSQL version information
+  /* Query PostgreSQL version information */
 	versionQuery := `
 		SELECT 
 			version() AS version,
@@ -68,14 +81,14 @@ func (t *PostgreSQLVersionTool) Execute(ctx context.Context, params map[string]i
 	}), nil
 }
 
-// PostgreSQLStatsTool retrieves PostgreSQL statistics
+/* PostgreSQLStatsTool retrieves PostgreSQL statistics */
 type PostgreSQLStatsTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewPostgreSQLStatsTool creates a new PostgreSQL statistics tool
+/* NewPostgreSQLStatsTool creates a new PostgreSQL statistics tool */
 func NewPostgreSQLStatsTool(db *database.Database, logger *logging.Logger) *PostgreSQLStatsTool {
 	return &PostgreSQLStatsTool{
 		BaseTool: NewBaseTool(
@@ -113,7 +126,7 @@ func NewPostgreSQLStatsTool(db *database.Database, logger *logging.Logger) *Post
 	}
 }
 
-// Execute executes the PostgreSQL statistics query
+/* Execute executes the PostgreSQL statistics query */
 func (t *PostgreSQLStatsTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	includeDBStats := true
 	includeTableStats := true
@@ -135,7 +148,7 @@ func (t *PostgreSQLStatsTool) Execute(ctx context.Context, params map[string]int
 
 	stats := make(map[string]interface{})
 
-	// Database statistics
+  /* Database statistics */
 	if includeDBStats {
 		dbStatsQuery := `
 			SELECT 
@@ -155,7 +168,7 @@ func (t *PostgreSQLStatsTool) Execute(ctx context.Context, params map[string]int
 		}
 	}
 
-	// Connection statistics
+  /* Connection statistics */
 	if includeConnStats {
 		connStatsQuery := `
 			SELECT 
@@ -176,7 +189,7 @@ func (t *PostgreSQLStatsTool) Execute(ctx context.Context, params map[string]int
 		}
 	}
 
-	// Table statistics
+  /* Table statistics */
 	if includeTableStats {
 		tableStatsQuery := `
 			SELECT 
@@ -197,7 +210,7 @@ func (t *PostgreSQLStatsTool) Execute(ctx context.Context, params map[string]int
 		}
 	}
 
-	// Performance statistics
+  /* Performance statistics */
 	if includePerfStats {
 		perfStatsQuery := `
 			SELECT 
@@ -221,7 +234,7 @@ func (t *PostgreSQLStatsTool) Execute(ctx context.Context, params map[string]int
 			stats["performance"] = perfStats
 		}
 
-		// Cache hit ratio
+   /* Cache hit ratio */
 		cacheQuery := `
 			SELECT 
 				sum(heap_blks_hit)::float / NULLIF(sum(heap_blks_hit) + sum(heap_blks_read), 0) * 100 AS heap_cache_hit_ratio,
@@ -241,7 +254,7 @@ func (t *PostgreSQLStatsTool) Execute(ctx context.Context, params map[string]int
 		}
 	}
 
-	// Server information
+  /* Server information */
 	serverInfoQuery := `
 		SELECT 
 			current_setting('server_version') AS server_version,
@@ -275,14 +288,14 @@ func (t *PostgreSQLStatsTool) Execute(ctx context.Context, params map[string]int
 	}), nil
 }
 
-// PostgreSQLDatabaseListTool lists all databases
+/* PostgreSQLDatabaseListTool lists all databases */
 type PostgreSQLDatabaseListTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewPostgreSQLDatabaseListTool creates a new PostgreSQL database list tool
+/* NewPostgreSQLDatabaseListTool creates a new PostgreSQL database list tool */
 func NewPostgreSQLDatabaseListTool(db *database.Database, logger *logging.Logger) *PostgreSQLDatabaseListTool {
 	return &PostgreSQLDatabaseListTool{
 		BaseTool: NewBaseTool(
@@ -305,7 +318,7 @@ func NewPostgreSQLDatabaseListTool(db *database.Database, logger *logging.Logger
 	}
 }
 
-// Execute executes the database list query
+/* Execute executes the database list query */
 func (t *PostgreSQLDatabaseListTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	includeSystem := false
 	if val, ok := params["include_system"].(bool); ok {
diff --git a/NeuronMCP/internal/tools/postgresql_advanced.go b/NeuronMCP/internal/tools/postgresql_advanced.go
index 811f210..f23c620 100644
--- a/NeuronMCP/internal/tools/postgresql_advanced.go
+++ b/NeuronMCP/internal/tools/postgresql_advanced.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * postgresql_advanced.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/postgresql_advanced.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// PostgreSQLConnectionsTool retrieves detailed connection information
+/* PostgreSQLConnectionsTool retrieves detailed connection information */
 type PostgreSQLConnectionsTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewPostgreSQLConnectionsTool creates a new PostgreSQL connections tool
+/* NewPostgreSQLConnectionsTool creates a new PostgreSQL connections tool */
 func NewPostgreSQLConnectionsTool(db *database.Database, logger *logging.Logger) *PostgreSQLConnectionsTool {
 	return &PostgreSQLConnectionsTool{
 		BaseTool: NewBaseTool(
@@ -32,7 +45,7 @@ func NewPostgreSQLConnectionsTool(db *database.Database, logger *logging.Logger)
 	}
 }
 
-// Execute executes the connections query
+/* Execute executes the connections query */
 func (t *PostgreSQLConnectionsTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	query := `
 		SELECT 
@@ -69,14 +82,14 @@ func (t *PostgreSQLConnectionsTool) Execute(ctx context.Context, params map[stri
 	}), nil
 }
 
-// PostgreSQLLocksTool retrieves lock information
+/* PostgreSQLLocksTool retrieves lock information */
 type PostgreSQLLocksTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewPostgreSQLLocksTool creates a new PostgreSQL locks tool
+/* NewPostgreSQLLocksTool creates a new PostgreSQL locks tool */
 func NewPostgreSQLLocksTool(db *database.Database, logger *logging.Logger) *PostgreSQLLocksTool {
 	return &PostgreSQLLocksTool{
 		BaseTool: NewBaseTool(
@@ -93,7 +106,7 @@ func NewPostgreSQLLocksTool(db *database.Database, logger *logging.Logger) *Post
 	}
 }
 
-// Execute executes the locks query
+/* Execute executes the locks query */
 func (t *PostgreSQLLocksTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	query := `
 		SELECT 
@@ -133,14 +146,14 @@ func (t *PostgreSQLLocksTool) Execute(ctx context.Context, params map[string]int
 	}), nil
 }
 
-// PostgreSQLReplicationTool retrieves replication status
+/* PostgreSQLReplicationTool retrieves replication status */
 type PostgreSQLReplicationTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewPostgreSQLReplicationTool creates a new PostgreSQL replication tool
+/* NewPostgreSQLReplicationTool creates a new PostgreSQL replication tool */
 func NewPostgreSQLReplicationTool(db *database.Database, logger *logging.Logger) *PostgreSQLReplicationTool {
 	return &PostgreSQLReplicationTool{
 		BaseTool: NewBaseTool(
@@ -157,7 +170,7 @@ func NewPostgreSQLReplicationTool(db *database.Database, logger *logging.Logger)
 	}
 }
 
-// Execute executes the replication query
+/* Execute executes the replication query */
 func (t *PostgreSQLReplicationTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	query := `
 		SELECT 
@@ -192,14 +205,14 @@ func (t *PostgreSQLReplicationTool) Execute(ctx context.Context, params map[stri
 	}), nil
 }
 
-// PostgreSQLSettingsTool retrieves configuration settings
+/* PostgreSQLSettingsTool retrieves configuration settings */
 type PostgreSQLSettingsTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewPostgreSQLSettingsTool creates a new PostgreSQL settings tool
+/* NewPostgreSQLSettingsTool creates a new PostgreSQL settings tool */
 func NewPostgreSQLSettingsTool(db *database.Database, logger *logging.Logger) *PostgreSQLSettingsTool {
 	return &PostgreSQLSettingsTool{
 		BaseTool: NewBaseTool(
@@ -221,7 +234,7 @@ func NewPostgreSQLSettingsTool(db *database.Database, logger *logging.Logger) *P
 	}
 }
 
-// Execute executes the settings query
+/* Execute executes the settings query */
 func (t *PostgreSQLSettingsTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	pattern, _ := params["pattern"].(string)
 
@@ -284,14 +297,14 @@ func (t *PostgreSQLSettingsTool) Execute(ctx context.Context, params map[string]
 	}), nil
 }
 
-// PostgreSQLExtensionsTool lists installed extensions
+/* PostgreSQLExtensionsTool lists installed extensions */
 type PostgreSQLExtensionsTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewPostgreSQLExtensionsTool creates a new PostgreSQL extensions tool
+/* NewPostgreSQLExtensionsTool creates a new PostgreSQL extensions tool */
 func NewPostgreSQLExtensionsTool(db *database.Database, logger *logging.Logger) *PostgreSQLExtensionsTool {
 	return &PostgreSQLExtensionsTool{
 		BaseTool: NewBaseTool(
@@ -308,7 +321,7 @@ func NewPostgreSQLExtensionsTool(db *database.Database, logger *logging.Logger)
 	}
 }
 
-// Execute executes the extensions query
+/* Execute executes the extensions query */
 func (t *PostgreSQLExtensionsTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	query := `
 		SELECT 
@@ -341,3 +354,4 @@ func (t *PostgreSQLExtensionsTool) Execute(ctx context.Context, params map[strin
 
 
 
+
diff --git a/NeuronMCP/internal/tools/quality_metrics.go b/NeuronMCP/internal/tools/quality_metrics.go
index 0c3aa16..a815240 100644
--- a/NeuronMCP/internal/tools/quality_metrics.go
+++ b/NeuronMCP/internal/tools/quality_metrics.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * quality_metrics.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/quality_metrics.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// QualityMetricsTool computes quality metrics for search results
+/* QualityMetricsTool computes quality metrics for search results */
 type QualityMetricsTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewQualityMetricsTool creates a new quality metrics tool
+/* NewQualityMetricsTool creates a new quality metrics tool */
 func NewQualityMetricsTool(db *database.Database, logger *logging.Logger) *QualityMetricsTool {
 	return &QualityMetricsTool{
 		BaseTool: NewBaseTool(
@@ -54,7 +67,7 @@ func NewQualityMetricsTool(db *database.Database, logger *logging.Logger) *Quali
 	}
 }
 
-// Execute executes quality metrics computation
+/* Execute executes quality metrics computation */
 func (t *QualityMetricsTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -71,7 +84,7 @@ func (t *QualityMetricsTool) Execute(ctx context.Context, params map[string]inte
 		return Error("table is required", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Build query based on metric type
+  /* Build query based on metric type */
 	var query string
 	var queryParams []interface{}
 
@@ -86,7 +99,7 @@ func (t *QualityMetricsTool) Execute(ctx context.Context, params map[string]inte
 		if groundTruthCol == "" || predictedCol == "" {
 			return Error("ground_truth_col and predicted_col are required for @K metrics", "VALIDATION_ERROR", nil), nil
 		}
-		// Use appropriate NeuronDB function
+   /* Use appropriate NeuronDB function */
 		funcName := "recall_at_k"
 		if metric == "precision_at_k" {
 			funcName = "precision_at_k"
@@ -131,3 +144,4 @@ func (t *QualityMetricsTool) Execute(ctx context.Context, params map[string]inte
 
 
 
+
diff --git a/NeuronMCP/internal/tools/quantization.go b/NeuronMCP/internal/tools/quantization.go
index a1855df..542a85e 100644
--- a/NeuronMCP/internal/tools/quantization.go
+++ b/NeuronMCP/internal/tools/quantization.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * quantization.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/quantization.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -9,14 +22,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// VectorQuantizationTool performs vector quantization operations
+/* VectorQuantizationTool performs vector quantization operations */
 type VectorQuantizationTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewVectorQuantizationTool creates a new vector quantization tool
+/* NewVectorQuantizationTool creates a new vector quantization tool */
 func NewVectorQuantizationTool(db *database.Database, logger *logging.Logger) *VectorQuantizationTool {
 	return &VectorQuantizationTool{
 		BaseTool: NewBaseTool(
@@ -48,7 +61,7 @@ func NewVectorQuantizationTool(db *database.Database, logger *logging.Logger) *V
 	}
 }
 
-// Execute executes the quantization operation
+/* Execute executes the quantization operation */
 func (t *VectorQuantizationTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -281,14 +294,14 @@ func (t *VectorQuantizationTool) Execute(ctx context.Context, params map[string]
 	}), nil
 }
 
-// QuantizationAnalysisTool analyzes quantization options
+/* QuantizationAnalysisTool analyzes quantization options */
 type QuantizationAnalysisTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewQuantizationAnalysisTool creates a new quantization analysis tool
+/* NewQuantizationAnalysisTool creates a new quantization analysis tool */
 func NewQuantizationAnalysisTool(db *database.Database, logger *logging.Logger) *QuantizationAnalysisTool {
 	return &QuantizationAnalysisTool{
 		BaseTool: NewBaseTool(
@@ -330,7 +343,7 @@ func NewQuantizationAnalysisTool(db *database.Database, logger *logging.Logger)
 	}
 }
 
-// Execute executes the quantization analysis
+/* Execute executes the quantization analysis */
 func (t *QuantizationAnalysisTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -427,3 +440,4 @@ func (t *QuantizationAnalysisTool) Execute(ctx context.Context, params map[strin
 
 
 
+
diff --git a/NeuronMCP/internal/tools/rag.go b/NeuronMCP/internal/tools/rag.go
index bd2e1ad..4eb2046 100644
--- a/NeuronMCP/internal/tools/rag.go
+++ b/NeuronMCP/internal/tools/rag.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * rag.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/rag.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// ProcessDocumentTool processes a document for RAG
+/* ProcessDocumentTool processes a document for RAG */
 type ProcessDocumentTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewProcessDocumentTool creates a new process document tool
+/* NewProcessDocumentTool creates a new process document tool */
 func NewProcessDocumentTool(db *database.Database, logger *logging.Logger) *ProcessDocumentTool {
 	return &ProcessDocumentTool{
 		BaseTool: NewBaseTool(
@@ -54,7 +67,7 @@ func NewProcessDocumentTool(db *database.Database, logger *logging.Logger) *Proc
 	}
 }
 
-// Execute executes the document processing
+/* Execute executes the document processing */
 func (t *ProcessDocumentTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -116,7 +129,7 @@ func (t *ProcessDocumentTool) Execute(ctx context.Context, params map[string]int
 		}), nil
 	}
 
-	// Use NeuronDB's unified chunking function: neurondb.chunk(document_text, chunk_size, chunk_overlap, method)
+  /* Use NeuronDB's unified chunking function: neurondb.chunk(document_text, chunk_size, chunk_overlap, method) */
 	query := `SELECT json_agg(json_build_object('chunk_id', chunk_id, 'chunk_text', chunk_text, 'start_pos', start_pos, 'end_pos', end_pos)) AS chunks FROM neurondb.chunk($1, $2, $3, 'fixed')`
 	result, err := t.executor.ExecuteQueryOne(ctx, query, []interface{}{text, chunkSize, overlap})
 	if err != nil {
@@ -130,10 +143,10 @@ func (t *ProcessDocumentTool) Execute(ctx context.Context, params map[string]int
 		}), nil
 	}
 
-	// If embeddings requested, generate them
+  /* If embeddings requested, generate them */
 	if generateEmbeddings {
-		// This would typically be done in a separate step or within the chunking function
-		// For now, return the chunks
+   /* This would typically be done in a separate step or within the chunking function */
+   /* For now, return the chunks */
 	}
 
 	return Success(result, map[string]interface{}{
@@ -142,14 +155,14 @@ func (t *ProcessDocumentTool) Execute(ctx context.Context, params map[string]int
 	}), nil
 }
 
-// RetrieveContextTool retrieves context for RAG
+/* RetrieveContextTool retrieves context for RAG */
 type RetrieveContextTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewRetrieveContextTool creates a new retrieve context tool
+/* NewRetrieveContextTool creates a new retrieve context tool */
 func NewRetrieveContextTool(db *database.Database, logger *logging.Logger) *RetrieveContextTool {
 	return &RetrieveContextTool{
 		BaseTool: NewBaseTool(
@@ -186,7 +199,7 @@ func NewRetrieveContextTool(db *database.Database, logger *logging.Logger) *Retr
 	}
 }
 
-// Execute executes the context retrieval
+/* Execute executes the context retrieval */
 func (t *RetrieveContextTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -251,7 +264,7 @@ func (t *RetrieveContextTool) Execute(ctx context.Context, params map[string]int
 		}), nil
 	}
 
-	// Generate embedding for query
+  /* Generate embedding for query */
 	embedQuery := `SELECT embed_text($1) AS embedding`
 	embedResult, err := t.executor.ExecuteQueryOne(ctx, embedQuery, []interface{}{queryText})
 	if err != nil {
@@ -265,14 +278,14 @@ func (t *RetrieveContextTool) Execute(ctx context.Context, params map[string]int
 		}), nil
 	}
 
-	// Extract embedding vector (assuming it's in the result)
-	// Then perform vector search
-	// For now, use the retrieve_context function if available
+  /* Extract embedding vector (assuming it's in the result) */
+  /* Then perform vector search */
+  /* For now, use the retrieve_context function if available */
 	retrieveQuery := `SELECT neurondb_retrieve_context_c($1, $2, $3, $4) AS context`
 	result, err := t.executor.ExecuteQueryOne(ctx, retrieveQuery, []interface{}{queryText, table, vectorColumn, limit})
 	if err != nil {
-		// Fallback to manual vector search
-		// This is a simplified version - actual implementation would use the embedding
+   /* Fallback to manual vector search */
+   /* This is a simplified version - actual implementation would use the embedding */
 		t.logger.Error("Context retrieval failed", err, params)
 		return Error(fmt.Sprintf("Context retrieval execution failed: query_length=%d, table='%s', vector_column='%s', limit=%d, embedding_generated=%v, error=%v", len(queryText), table, vectorColumn, limit, embedResult != nil, err), "RAG_ERROR", map[string]interface{}{
 			"query_length":      len(queryText),
@@ -289,14 +302,14 @@ func (t *RetrieveContextTool) Execute(ctx context.Context, params map[string]int
 	}), nil
 }
 
-// GenerateResponseTool generates a response using RAG
+/* GenerateResponseTool generates a response using RAG */
 type GenerateResponseTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewGenerateResponseTool creates a new generate response tool
+/* NewGenerateResponseTool creates a new generate response tool */
 func NewGenerateResponseTool(db *database.Database, logger *logging.Logger) *GenerateResponseTool {
 	return &GenerateResponseTool{
 		BaseTool: NewBaseTool(
@@ -323,7 +336,7 @@ func NewGenerateResponseTool(db *database.Database, logger *logging.Logger) *Gen
 	}
 }
 
-// Execute executes the response generation
+/* Execute executes the response generation */
 func (t *GenerateResponseTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -381,14 +394,14 @@ func (t *GenerateResponseTool) Execute(ctx context.Context, params map[string]in
 		}
 	}
 
-	// Use NeuronDB's LLM function for response generation
-	// neurondb.llm(task, model, input_text, input_array, params, max_length)
+  /* Use NeuronDB's LLM function for response generation */
+  /* neurondb.llm(task, model, input_text, input_array, params, max_length) */
 	modelName := "default"
 	if m, ok := params["model"].(string); ok && m != "" {
 		modelName = m
 	}
 
-	// Build prompt with context
+  /* Build prompt with context */
 	prompt := fmt.Sprintf("Context:\n%s\n\nQuestion: %s\n\nAnswer:", contextStr, query)
 	
 	llmParams := fmt.Sprintf(`{"temperature": 0.7, "max_tokens": 500}`)
@@ -407,14 +420,14 @@ func (t *GenerateResponseTool) Execute(ctx context.Context, params map[string]in
 	return Success(result, nil), nil
 }
 
-// ChunkDocumentTool chunks a document into smaller pieces
+/* ChunkDocumentTool chunks a document into smaller pieces */
 type ChunkDocumentTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewChunkDocumentTool creates a new chunk document tool
+/* NewChunkDocumentTool creates a new chunk document tool */
 func NewChunkDocumentTool(db *database.Database, logger *logging.Logger) *ChunkDocumentTool {
 	return &ChunkDocumentTool{
 		BaseTool: NewBaseTool(
@@ -448,7 +461,7 @@ func NewChunkDocumentTool(db *database.Database, logger *logging.Logger) *ChunkD
 	}
 }
 
-// Execute executes the document chunking
+/* Execute executes the document chunking */
 func (t *ChunkDocumentTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -506,7 +519,7 @@ func (t *ChunkDocumentTool) Execute(ctx context.Context, params map[string]inter
 		}), nil
 	}
 
-	// Use NeuronDB's unified chunking function: neurondb.chunk(document_text, chunk_size, chunk_overlap, method)
+  /* Use NeuronDB's unified chunking function: neurondb.chunk(document_text, chunk_size, chunk_overlap, method) */
 	query := `SELECT json_agg(json_build_object('chunk_id', chunk_id, 'chunk_text', chunk_text, 'start_pos', start_pos, 'end_pos', end_pos)) AS chunks FROM neurondb.chunk($1, $2, $3, 'fixed')`
 	result, err := t.executor.ExecuteQueryOne(ctx, query, []interface{}{text, chunkSize, overlap})
 	if err != nil {
diff --git a/NeuronMCP/internal/tools/register.go b/NeuronMCP/internal/tools/register.go
index 146ace7..e0bd158 100644
--- a/NeuronMCP/internal/tools/register.go
+++ b/NeuronMCP/internal/tools/register.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * register.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/register.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -5,23 +18,23 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// RegisterAllTools registers all available tools with the registry
+/* RegisterAllTools registers all available tools with the registry */
 func RegisterAllTools(registry *ToolRegistry, db *database.Database, logger *logging.Logger) {
-	// Vector search tools
+  /* Vector search tools */
 	registry.Register(NewVectorSearchTool(db, logger))
 	registry.Register(NewVectorSearchL2Tool(db, logger))
 	registry.Register(NewVectorSearchCosineTool(db, logger))
 	registry.Register(NewVectorSearchInnerProductTool(db, logger))
 
-	// Embedding tools
+  /* Embedding tools */
 	registry.Register(NewGenerateEmbeddingTool(db, logger))
 	registry.Register(NewBatchEmbeddingTool(db, logger))
 
-	// Additional vector tools
+  /* Additional vector tools */
 	registry.Register(NewVectorSimilarityTool(db, logger))
 	registry.Register(NewCreateVectorIndexTool(db, logger))
 
-	// ML tools
+  /* ML tools */
 	registry.Register(NewTrainModelTool(db, logger))
 	registry.Register(NewPredictTool(db, logger))
 	registry.Register(NewEvaluateModelTool(db, logger))
@@ -29,18 +42,18 @@ func RegisterAllTools(registry *ToolRegistry, db *database.Database, logger *log
 	registry.Register(NewGetModelInfoTool(db, logger))
 	registry.Register(NewDeleteModelTool(db, logger))
 
-	// Analytics tools
+  /* Analytics tools */
 	registry.Register(NewClusterDataTool(db, logger))
 	registry.Register(NewDetectOutliersTool(db, logger))
 	registry.Register(NewReduceDimensionalityTool(db, logger))
 
-	// RAG tools
+  /* RAG tools */
 	registry.Register(NewProcessDocumentTool(db, logger))
 	registry.Register(NewRetrieveContextTool(db, logger))
 	registry.Register(NewGenerateResponseTool(db, logger))
 	registry.Register(NewChunkDocumentTool(db, logger))
 
-	// Indexing tools
+  /* Indexing tools */
 	registry.Register(NewCreateHNSWIndexTool(db, logger))
 	registry.Register(NewCreateIVFIndexTool(db, logger))
 	registry.Register(NewIndexStatusTool(db, logger))
@@ -48,14 +61,14 @@ func RegisterAllTools(registry *ToolRegistry, db *database.Database, logger *log
 	registry.Register(NewTuneHNSWIndexTool(db, logger))
 	registry.Register(NewTuneIVFIndexTool(db, logger))
 
-	// Additional ML tools
+  /* Additional ML tools */
 	registry.Register(NewPredictBatchTool(db, logger))
 	registry.Register(NewExportModelTool(db, logger))
 
-	// Analytics tools
+  /* Analytics tools */
 	registry.Register(NewAnalyzeDataTool(db, logger))
 
-	// Hybrid search tools
+  /* Hybrid search tools */
 	registry.Register(NewHybridSearchTool(db, logger))
 	registry.Register(NewReciprocalRankFusionTool(db, logger))
 	registry.Register(NewSemanticKeywordSearchTool(db, logger))
@@ -64,7 +77,7 @@ func RegisterAllTools(registry *ToolRegistry, db *database.Database, logger *log
 	registry.Register(NewTemporalVectorSearchTool(db, logger))
 	registry.Register(NewDiverseVectorSearchTool(db, logger))
 
-	// Reranking tools
+  /* Reranking tools */
 	registry.Register(NewRerankCrossEncoderTool(db, logger))
 	registry.Register(NewRerankLLMTool(db, logger))
 	registry.Register(NewRerankCohereTool(db, logger))
@@ -72,16 +85,16 @@ func RegisterAllTools(registry *ToolRegistry, db *database.Database, logger *log
 	registry.Register(NewRerankLTRTool(db, logger))
 	registry.Register(NewRerankEnsembleTool(db, logger))
 
-	// Advanced vector operations
+  /* Advanced vector operations */
 	registry.Register(NewVectorArithmeticTool(db, logger))
 	registry.Register(NewVectorDistanceTool(db, logger))
 	registry.Register(NewVectorSimilarityUnifiedTool(db, logger))
 
-	// Quantization tools
+  /* Quantization tools */
 	registry.Register(NewVectorQuantizationTool(db, logger))
 	registry.Register(NewQuantizationAnalysisTool(db, logger))
 
-	// Complete embedding tools
+  /* Complete embedding tools */
 	registry.Register(NewEmbedImageTool(db, logger))
 	registry.Register(NewEmbedMultimodalTool(db, logger))
 	registry.Register(NewEmbedCachedTool(db, logger))
@@ -90,30 +103,30 @@ func RegisterAllTools(registry *ToolRegistry, db *database.Database, logger *log
 	registry.Register(NewListEmbeddingModelConfigsTool(db, logger))
 	registry.Register(NewDeleteEmbeddingModelConfigTool(db, logger))
 
-	// Quality metrics, drift detection, topic discovery
+  /* Quality metrics, drift detection, topic discovery */
 	registry.Register(NewQualityMetricsTool(db, logger))
 	registry.Register(NewDriftDetectionTool(db, logger))
 	registry.Register(NewTopicDiscoveryTool(db, logger))
 
-	// Time series, AutoML, ONNX
+  /* Time series, AutoML, ONNX */
 	registry.Register(NewTimeSeriesTool(db, logger))
 	registry.Register(NewAutoMLTool(db, logger))
 	registry.Register(NewONNXTool(db, logger))
 
-	// Vector graph operations
+  /* Vector graph operations */
 	registry.Register(NewVectorGraphTool(db, logger))
 
-	// Vecmap operations
+  /* Vecmap operations */
 	registry.Register(NewVecmapOperationsTool(db, logger))
 
-	// Dataset loading
+  /* Dataset loading */
 	registry.Register(NewDatasetLoadingTool(db, logger))
 
-	// Workers and GPU
+  /* Workers and GPU */
 	registry.Register(NewWorkerManagementTool(db, logger))
 	registry.Register(NewGPUMonitoringTool(db, logger))
 
-	// PostgreSQL tools
+  /* PostgreSQL tools */
 	registry.Register(NewPostgreSQLVersionTool(db, logger))
 	registry.Register(NewPostgreSQLStatsTool(db, logger))
 	registry.Register(NewPostgreSQLDatabaseListTool(db, logger))
diff --git a/NeuronMCP/internal/tools/registry.go b/NeuronMCP/internal/tools/registry.go
index 2587496..22fd0ab 100644
--- a/NeuronMCP/internal/tools/registry.go
+++ b/NeuronMCP/internal/tools/registry.go
@@ -1,3 +1,18 @@
+/*-------------------------------------------------------------------------
+ *
+ * registry.go
+ *    Tool registry for NeuronMCP
+ *
+ * Manages tool registration, definitions, and execution for the MCP server.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/registry.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +23,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// ToolDefinition represents a tool's definition for MCP
+/* ToolDefinition represents a tool's definition for MCP */
 type ToolDefinition struct {
 	Name        string                 `json:"name"`
 	Description string                 `json:"description"`
 	InputSchema map[string]interface{} `json:"inputSchema"`
 }
 
-// ToolRegistry manages tool registration and execution
+/* ToolRegistry manages tool registration and execution */
 type ToolRegistry struct {
 	tools      map[string]Tool
 	definitions map[string]ToolDefinition
@@ -24,7 +39,7 @@ type ToolRegistry struct {
 	logger     *logging.Logger
 }
 
-// NewToolRegistry creates a new tool registry
+/* NewToolRegistry creates a new tool registry */
 func NewToolRegistry(db *database.Database, logger *logging.Logger) *ToolRegistry {
 	return &ToolRegistry{
 		tools:       make(map[string]Tool),
@@ -34,7 +49,7 @@ func NewToolRegistry(db *database.Database, logger *logging.Logger) *ToolRegistr
 	}
 }
 
-// Register registers a tool
+/* Register registers a tool */
 func (r *ToolRegistry) Register(tool Tool) {
 	r.mu.Lock()
 	defer r.mu.Unlock()
@@ -50,21 +65,21 @@ func (r *ToolRegistry) Register(tool Tool) {
 	r.logger.Debug(fmt.Sprintf("Registered tool: %s", tool.Name()), nil)
 }
 
-// RegisterAll registers multiple tools
+/* RegisterAll registers multiple tools */
 func (r *ToolRegistry) RegisterAll(tools []Tool) {
 	for _, tool := range tools {
 		r.Register(tool)
 	}
 }
 
-// GetTool retrieves a tool by name
+/* GetTool retrieves a tool by name */
 func (r *ToolRegistry) GetTool(name string) Tool {
 	r.mu.RLock()
 	defer r.mu.RUnlock()
 	return r.tools[name]
 }
 
-// GetDefinition retrieves a tool definition by name
+/* GetDefinition retrieves a tool definition by name */
 func (r *ToolRegistry) GetDefinition(name string) (ToolDefinition, bool) {
 	r.mu.RLock()
 	defer r.mu.RUnlock()
@@ -72,7 +87,7 @@ func (r *ToolRegistry) GetDefinition(name string) (ToolDefinition, bool) {
 	return def, exists
 }
 
-// GetAllDefinitions returns all tool definitions
+/* GetAllDefinitions returns all tool definitions */
 func (r *ToolRegistry) GetAllDefinitions() []ToolDefinition {
 	r.mu.RLock()
 	defer r.mu.RUnlock()
@@ -84,7 +99,7 @@ func (r *ToolRegistry) GetAllDefinitions() []ToolDefinition {
 	return definitions
 }
 
-// GetAllToolNames returns all registered tool names
+/* GetAllToolNames returns all registered tool names */
 func (r *ToolRegistry) GetAllToolNames() []string {
 	r.mu.RLock()
 	defer r.mu.RUnlock()
@@ -96,7 +111,7 @@ func (r *ToolRegistry) GetAllToolNames() []string {
 	return names
 }
 
-// HasTool checks if a tool exists
+/* HasTool checks if a tool exists */
 func (r *ToolRegistry) HasTool(name string) bool {
 	r.mu.RLock()
 	defer r.mu.RUnlock()
@@ -104,7 +119,7 @@ func (r *ToolRegistry) HasTool(name string) bool {
 	return exists
 }
 
-// Unregister removes a tool
+/* Unregister removes a tool */
 func (r *ToolRegistry) Unregister(name string) bool {
 	r.mu.Lock()
 	defer r.mu.Unlock()
@@ -119,7 +134,7 @@ func (r *ToolRegistry) Unregister(name string) bool {
 	return removed
 }
 
-// Clear removes all tools
+/* Clear removes all tools */
 func (r *ToolRegistry) Clear() {
 	r.mu.Lock()
 	defer r.mu.Unlock()
@@ -127,7 +142,7 @@ func (r *ToolRegistry) Clear() {
 	r.definitions = make(map[string]ToolDefinition)
 }
 
-// GetCount returns the number of registered tools
+/* GetCount returns the number of registered tools */
 func (r *ToolRegistry) GetCount() int {
 	r.mu.RLock()
 	defer r.mu.RUnlock()
diff --git a/NeuronMCP/internal/tools/reranking.go b/NeuronMCP/internal/tools/reranking.go
index f8ce866..4755e48 100644
--- a/NeuronMCP/internal/tools/reranking.go
+++ b/NeuronMCP/internal/tools/reranking.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * reranking.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/reranking.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -9,14 +22,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// RerankCrossEncoderTool performs cross-encoder reranking
+/* RerankCrossEncoderTool performs cross-encoder reranking */
 type RerankCrossEncoderTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewRerankCrossEncoderTool creates a new cross-encoder reranking tool
+/* NewRerankCrossEncoderTool creates a new cross-encoder reranking tool */
 func NewRerankCrossEncoderTool(db *database.Database, logger *logging.Logger) *RerankCrossEncoderTool {
 	return &RerankCrossEncoderTool{
 		BaseTool: NewBaseTool(
@@ -55,7 +68,7 @@ func NewRerankCrossEncoderTool(db *database.Database, logger *logging.Logger) *R
 	}
 }
 
-// Execute executes cross-encoder reranking
+/* Execute executes cross-encoder reranking */
 func (t *RerankCrossEncoderTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -80,7 +93,7 @@ func (t *RerankCrossEncoderTool) Execute(ctx context.Context, params map[string]
 		return Error("query and documents are required", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Format documents array
+  /* Format documents array */
 	var docStrs []string
 	for _, doc := range documents {
 		if docStr, ok := doc.(string); ok {
@@ -108,14 +121,14 @@ func (t *RerankCrossEncoderTool) Execute(ctx context.Context, params map[string]
 	}), nil
 }
 
-// RerankLLMTool performs LLM-based reranking
+/* RerankLLMTool performs LLM-based reranking */
 type RerankLLMTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewRerankLLMTool creates a new LLM reranking tool
+/* NewRerankLLMTool creates a new LLM reranking tool */
 func NewRerankLLMTool(db *database.Database, logger *logging.Logger) *RerankLLMTool {
 	return &RerankLLMTool{
 		BaseTool: NewBaseTool(
@@ -154,7 +167,7 @@ func NewRerankLLMTool(db *database.Database, logger *logging.Logger) *RerankLLMT
 	}
 }
 
-// Execute executes LLM reranking
+/* Execute executes LLM reranking */
 func (t *RerankLLMTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -179,7 +192,7 @@ func (t *RerankLLMTool) Execute(ctx context.Context, params map[string]interface
 		return Error("query and documents are required", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Format documents array
+  /* Format documents array */
 	var docStrs []string
 	for _, doc := range documents {
 		if docStr, ok := doc.(string); ok {
@@ -207,14 +220,14 @@ func (t *RerankLLMTool) Execute(ctx context.Context, params map[string]interface
 	}), nil
 }
 
-// RerankCohereTool performs Cohere reranking
+/* RerankCohereTool performs Cohere reranking */
 type RerankCohereTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewRerankCohereTool creates a new Cohere reranking tool
+/* NewRerankCohereTool creates a new Cohere reranking tool */
 func NewRerankCohereTool(db *database.Database, logger *logging.Logger) *RerankCohereTool {
 	return &RerankCohereTool{
 		BaseTool: NewBaseTool(
@@ -248,7 +261,7 @@ func NewRerankCohereTool(db *database.Database, logger *logging.Logger) *RerankC
 	}
 }
 
-// Execute executes Cohere reranking
+/* Execute executes Cohere reranking */
 func (t *RerankCohereTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -269,7 +282,7 @@ func (t *RerankCohereTool) Execute(ctx context.Context, params map[string]interf
 		return Error("query and documents are required", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Format documents array
+  /* Format documents array */
 	var docStrs []string
 	for _, doc := range documents {
 		if docStr, ok := doc.(string); ok {
@@ -297,14 +310,14 @@ func (t *RerankCohereTool) Execute(ctx context.Context, params map[string]interf
 	}), nil
 }
 
-// RerankColBERTTool performs ColBERT reranking
+/* RerankColBERTTool performs ColBERT reranking */
 type RerankColBERTTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewRerankColBERTTool creates a new ColBERT reranking tool
+/* NewRerankColBERTTool creates a new ColBERT reranking tool */
 func NewRerankColBERTTool(db *database.Database, logger *logging.Logger) *RerankColBERTTool {
 	return &RerankColBERTTool{
 		BaseTool: NewBaseTool(
@@ -336,7 +349,7 @@ func NewRerankColBERTTool(db *database.Database, logger *logging.Logger) *Rerank
 	}
 }
 
-// Execute executes ColBERT reranking
+/* Execute executes ColBERT reranking */
 func (t *RerankColBERTTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -357,7 +370,7 @@ func (t *RerankColBERTTool) Execute(ctx context.Context, params map[string]inter
 		return Error("query and documents are required", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Format documents array
+  /* Format documents array */
 	var docStrs []string
 	for _, doc := range documents {
 		if docStr, ok := doc.(string); ok {
@@ -385,14 +398,14 @@ func (t *RerankColBERTTool) Execute(ctx context.Context, params map[string]inter
 	}), nil
 }
 
-// RerankLTRTool performs learning-to-rank reranking
+/* RerankLTRTool performs learning-to-rank reranking */
 type RerankLTRTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewRerankLTRTool creates a new LTR reranking tool
+/* NewRerankLTRTool creates a new LTR reranking tool */
 func NewRerankLTRTool(db *database.Database, logger *logging.Logger) *RerankLTRTool {
 	return &RerankLTRTool{
 		BaseTool: NewBaseTool(
@@ -427,7 +440,7 @@ func NewRerankLTRTool(db *database.Database, logger *logging.Logger) *RerankLTRT
 	}
 }
 
-// Execute executes LTR reranking
+/* Execute executes LTR reranking */
 func (t *RerankLTRTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -446,7 +459,7 @@ func (t *RerankLTRTool) Execute(ctx context.Context, params map[string]interface
 		return Error("query, documents, feature_table, and model_table are required", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Format documents array
+  /* Format documents array */
 	var docStrs []string
 	for _, doc := range documents {
 		if docStr, ok := doc.(string); ok {
@@ -474,14 +487,14 @@ func (t *RerankLTRTool) Execute(ctx context.Context, params map[string]interface
 	}), nil
 }
 
-// RerankEnsembleTool performs ensemble reranking
+/* RerankEnsembleTool performs ensemble reranking */
 type RerankEnsembleTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewRerankEnsembleTool creates a new ensemble reranking tool
+/* NewRerankEnsembleTool creates a new ensemble reranking tool */
 func NewRerankEnsembleTool(db *database.Database, logger *logging.Logger) *RerankEnsembleTool {
 	return &RerankEnsembleTool{
 		BaseTool: NewBaseTool(
@@ -518,7 +531,7 @@ func NewRerankEnsembleTool(db *database.Database, logger *logging.Logger) *Reran
 	}
 }
 
-// Execute executes ensemble reranking
+/* Execute executes ensemble reranking */
 func (t *RerankEnsembleTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -541,7 +554,7 @@ func (t *RerankEnsembleTool) Execute(ctx context.Context, params map[string]inte
 		return Error("rerankers and weights arrays must have the same length", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Format arrays
+  /* Format arrays */
 	var docStrs []string
 	for _, doc := range documents {
 		if docStr, ok := doc.(string); ok {
diff --git a/NeuronMCP/internal/tools/timeseries.go b/NeuronMCP/internal/tools/timeseries.go
index 3e1e254..b7e78ad 100644
--- a/NeuronMCP/internal/tools/timeseries.go
+++ b/NeuronMCP/internal/tools/timeseries.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * timeseries.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/timeseries.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// TimeSeriesTool performs time series analysis
+/* TimeSeriesTool performs time series analysis */
 type TimeSeriesTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewTimeSeriesTool creates a new time series tool
+/* NewTimeSeriesTool creates a new time series tool */
 func NewTimeSeriesTool(db *database.Database, logger *logging.Logger) *TimeSeriesTool {
 	return &TimeSeriesTool{
 		BaseTool: NewBaseTool(
@@ -66,7 +79,7 @@ func NewTimeSeriesTool(db *database.Database, logger *logging.Logger) *TimeSerie
 	}
 }
 
-// Execute executes time series analysis
+/* Execute executes time series analysis */
 func (t *TimeSeriesTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -129,3 +142,4 @@ func (t *TimeSeriesTool) Execute(ctx context.Context, params map[string]interfac
 
 
 
+
diff --git a/NeuronMCP/internal/tools/topic_discovery.go b/NeuronMCP/internal/tools/topic_discovery.go
index 6f56d2e..0114ab3 100644
--- a/NeuronMCP/internal/tools/topic_discovery.go
+++ b/NeuronMCP/internal/tools/topic_discovery.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * topic_discovery.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/topic_discovery.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// TopicDiscoveryTool performs topic modeling and discovery
+/* TopicDiscoveryTool performs topic modeling and discovery */
 type TopicDiscoveryTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewTopicDiscoveryTool creates a new topic discovery tool
+/* NewTopicDiscoveryTool creates a new topic discovery tool */
 func NewTopicDiscoveryTool(db *database.Database, logger *logging.Logger) *TopicDiscoveryTool {
 	return &TopicDiscoveryTool{
 		BaseTool: NewBaseTool(
@@ -54,7 +67,7 @@ func NewTopicDiscoveryTool(db *database.Database, logger *logging.Logger) *Topic
 	}
 }
 
-// Execute executes topic discovery
+/* Execute executes topic discovery */
 func (t *TopicDiscoveryTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -79,7 +92,7 @@ func (t *TopicDiscoveryTool) Execute(ctx context.Context, params map[string]inte
 		return Error("table and text_column are required", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Use NeuronDB topic discovery function
+  /* Use NeuronDB topic discovery function */
 	query := "SELECT * FROM discover_topics($1::text, $2::text, $3::int, $4::text)"
 	queryParams := []interface{}{table, textColumn, numTopics, algorithm}
 
@@ -103,3 +116,4 @@ func (t *TopicDiscoveryTool) Execute(ctx context.Context, params map[string]inte
 
 
 
+
diff --git a/NeuronMCP/internal/tools/vecmap_operations.go b/NeuronMCP/internal/tools/vecmap_operations.go
index 3a7bfcb..0c4e6e4 100644
--- a/NeuronMCP/internal/tools/vecmap_operations.go
+++ b/NeuronMCP/internal/tools/vecmap_operations.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * vecmap_operations.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/vecmap_operations.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -9,14 +22,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// VecmapOperationsTool performs operations on vecmap (sparse vector) type
+/* VecmapOperationsTool performs operations on vecmap (sparse vector) type */
 type VecmapOperationsTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewVecmapOperationsTool creates a new vecmap operations tool
+/* NewVecmapOperationsTool creates a new vecmap operations tool */
 func NewVecmapOperationsTool(db *database.Database, logger *logging.Logger) *VecmapOperationsTool {
 	return &VecmapOperationsTool{
 		BaseTool: NewBaseTool(
@@ -51,7 +64,7 @@ func NewVecmapOperationsTool(db *database.Database, logger *logging.Logger) *Vec
 	}
 }
 
-// Execute executes the vecmap operation
+/* Execute executes the vecmap operation */
 func (t *VecmapOperationsTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -68,7 +81,7 @@ func (t *VecmapOperationsTool) Execute(ctx context.Context, params map[string]in
 		return Error("vecmap1 is required and cannot be empty", "VALIDATION_ERROR", nil), nil
 	}
 
-	// Decode base64 vecmap data
+  /* Decode base64 vecmap data */
 	vecmap1Bytes, err := base64.StdEncoding.DecodeString(vecmap1)
 	if err != nil {
 		return Error(fmt.Sprintf("Invalid base64 vecmap1 data: %v", err), "VALIDATION_ERROR", nil), nil
@@ -133,3 +146,4 @@ func (t *VecmapOperationsTool) Execute(ctx context.Context, params map[string]in
 
 
 
+
diff --git a/NeuronMCP/internal/tools/vector.go b/NeuronMCP/internal/tools/vector.go
index 1b6f5e3..4f043af 100644
--- a/NeuronMCP/internal/tools/vector.go
+++ b/NeuronMCP/internal/tools/vector.go
@@ -1,3 +1,19 @@
+/*-------------------------------------------------------------------------
+ *
+ * vector.go
+ *    Vector search and embedding tools for NeuronMCP
+ *
+ * Provides tools for vector similarity search with multiple distance metrics
+ * and text embedding generation.
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/vector.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +24,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// VectorSearchTool performs vector similarity search
+/* VectorSearchTool performs vector similarity search */
 type VectorSearchTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewVectorSearchTool creates a new vector search tool
+/* NewVectorSearchTool creates a new vector search tool */
 func NewVectorSearchTool(db *database.Database, logger *logging.Logger) *VectorSearchTool {
 	return &VectorSearchTool{
 		BaseTool: NewBaseTool(
@@ -64,7 +80,7 @@ func NewVectorSearchTool(db *database.Database, logger *logging.Logger) *VectorS
 	}
 }
 
-// Execute executes the vector search
+/* Execute executes the vector search */
 func (t *VectorSearchTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -136,14 +152,14 @@ func (t *VectorSearchTool) Execute(ctx context.Context, params map[string]interf
 	}), nil
 }
 
-// VectorSearchL2Tool performs L2 distance vector search
+/* VectorSearchL2Tool performs L2 distance vector search */
 type VectorSearchL2Tool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewVectorSearchL2Tool creates a new L2 vector search tool
+/* NewVectorSearchL2Tool creates a new L2 vector search tool */
 func NewVectorSearchL2Tool(db *database.Database, logger *logging.Logger) *VectorSearchL2Tool {
 	return &VectorSearchL2Tool{
 		BaseTool: NewBaseTool(
@@ -165,7 +181,7 @@ func NewVectorSearchL2Tool(db *database.Database, logger *logging.Logger) *Vecto
 	}
 }
 
-// Execute executes the L2 vector search
+/* Execute executes the L2 vector search */
 func (t *VectorSearchL2Tool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -199,14 +215,14 @@ func (t *VectorSearchL2Tool) Execute(ctx context.Context, params map[string]inte
 	}), nil
 }
 
-// VectorSearchCosineTool performs cosine distance vector search
+/* VectorSearchCosineTool performs cosine distance vector search */
 type VectorSearchCosineTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewVectorSearchCosineTool creates a new cosine vector search tool
+/* NewVectorSearchCosineTool creates a new cosine vector search tool */
 func NewVectorSearchCosineTool(db *database.Database, logger *logging.Logger) *VectorSearchCosineTool {
 	return &VectorSearchCosineTool{
 		BaseTool: NewBaseTool(
@@ -228,7 +244,7 @@ func NewVectorSearchCosineTool(db *database.Database, logger *logging.Logger) *V
 	}
 }
 
-// Execute executes the cosine vector search
+/* Execute executes the cosine vector search */
 func (t *VectorSearchCosineTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -262,14 +278,14 @@ func (t *VectorSearchCosineTool) Execute(ctx context.Context, params map[string]
 	}), nil
 }
 
-// VectorSearchInnerProductTool performs inner product distance vector search
+/* VectorSearchInnerProductTool performs inner product distance vector search */
 type VectorSearchInnerProductTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewVectorSearchInnerProductTool creates a new inner product vector search tool
+/* NewVectorSearchInnerProductTool creates a new inner product vector search tool */
 func NewVectorSearchInnerProductTool(db *database.Database, logger *logging.Logger) *VectorSearchInnerProductTool {
 	return &VectorSearchInnerProductTool{
 		BaseTool: NewBaseTool(
@@ -291,7 +307,7 @@ func NewVectorSearchInnerProductTool(db *database.Database, logger *logging.Logg
 	}
 }
 
-// Execute executes the inner product vector search
+/* Execute executes the inner product vector search */
 func (t *VectorSearchInnerProductTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -325,14 +341,14 @@ func (t *VectorSearchInnerProductTool) Execute(ctx context.Context, params map[s
 	}), nil
 }
 
-// GenerateEmbeddingTool generates text embeddings
+/* GenerateEmbeddingTool generates text embeddings */
 type GenerateEmbeddingTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewGenerateEmbeddingTool creates a new embedding generation tool
+/* NewGenerateEmbeddingTool creates a new embedding generation tool */
 func NewGenerateEmbeddingTool(db *database.Database, logger *logging.Logger) *GenerateEmbeddingTool {
 	return &GenerateEmbeddingTool{
 		BaseTool: NewBaseTool(
@@ -358,7 +374,7 @@ func NewGenerateEmbeddingTool(db *database.Database, logger *logging.Logger) *Ge
 	}
 }
 
-// Execute executes the embedding generation
+/* Execute executes the embedding generation */
 func (t *GenerateEmbeddingTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	t.logger.Info("GenerateEmbeddingTool.Execute called", map[string]interface{}{
 		"params": params,
@@ -386,12 +402,9 @@ func (t *GenerateEmbeddingTool) Execute(ctx context.Context, params map[string]i
 		modelName = "default"
 	}
 
-	// Try embed_text first (C function, most reliable), then fallback to neurondb.embed
 	var result interface{}
 	var err error
 	
-	// First try: embed_text(text, model) - direct C function
-	// Cast vector to text so pgx can scan it
 	query := "SELECT embed_text($1, $2)::text AS embedding"
 	queryParams := []interface{}{text, modelName}
 	
@@ -402,20 +415,16 @@ func (t *GenerateEmbeddingTool) Execute(ctx context.Context, params map[string]i
 		"query": query,
 	})
 	
-	// Use embedding timeout for embedding queries
 	result, err = t.executor.ExecuteQueryOneWithTimeout(ctx, query, queryParams, EmbeddingQueryTimeout)
 	if err != nil {
-		// Fallback: neurondb.embed(model, input_text, task) - PL/pgSQL wrapper
 		t.logger.Warn("embed_text failed, trying neurondb.embed fallback", map[string]interface{}{
 			"error": err.Error(),
 			"model": modelName,
 		})
 		
-		// Cast vector to text so pgx can scan it
 		query = "SELECT neurondb.embed($1, $2, 'embedding')::text AS embedding"
 		queryParams = []interface{}{modelName, text}
 		
-		// Use embedding timeout for fallback query too
 		result, err = t.executor.ExecuteQueryOneWithTimeout(ctx, query, queryParams, EmbeddingQueryTimeout)
 		if err != nil {
 			t.logger.Error("Embedding generation failed with both methods", err, params)
@@ -431,14 +440,14 @@ func (t *GenerateEmbeddingTool) Execute(ctx context.Context, params map[string]i
 	return Success(result, map[string]interface{}{"model": modelName}), nil
 }
 
-// BatchEmbeddingTool generates embeddings for multiple texts
+/* BatchEmbeddingTool generates embeddings for multiple texts */
 type BatchEmbeddingTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewBatchEmbeddingTool creates a new batch embedding tool
+/* NewBatchEmbeddingTool creates a new batch embedding tool */
 func NewBatchEmbeddingTool(db *database.Database, logger *logging.Logger) *BatchEmbeddingTool {
 	return &BatchEmbeddingTool{
 		BaseTool: NewBaseTool(
@@ -467,7 +476,7 @@ func NewBatchEmbeddingTool(db *database.Database, logger *logging.Logger) *Batch
 	}
 }
 
-// Execute executes the batch embedding
+/* Execute executes the batch embedding */
 func (t *BatchEmbeddingTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -500,7 +509,6 @@ func (t *BatchEmbeddingTool) Execute(ctx context.Context, params map[string]inte
 		modelName = "default"
 	}
 
-	// Convert []interface{} to []string for PostgreSQL text[] array
 	textStrings := make([]string, 0, textsCount)
 	for i, text := range texts {
 		if textStr, ok := text.(string); ok {
@@ -524,8 +532,6 @@ func (t *BatchEmbeddingTool) Execute(ctx context.Context, params map[string]inte
 		}
 	}
 
-	// Use NeuronDB's batch embedding function: neurondb.embed_batch(model, texts[])
-	// Cast vector[] to text[] array, then to JSON so pgx can scan it
 	query := "SELECT json_agg(embedding::text) AS embeddings FROM unnest(neurondb.embed_batch($1, $2::text[])) AS embedding"
 	queryParams := []interface{}{modelName, textStrings}
 
@@ -536,7 +542,6 @@ func (t *BatchEmbeddingTool) Execute(ctx context.Context, params map[string]inte
 		"query": query,
 	})
 
-	// Use embedding timeout for batch embeddings (can take longer)
 	result, err := t.executor.ExecuteQueryOneWithTimeout(ctx, query, queryParams, EmbeddingQueryTimeout)
 	if err != nil {
 		t.logger.Error("Batch embedding failed", err, params)
diff --git a/NeuronMCP/internal/tools/vector_additional.go b/NeuronMCP/internal/tools/vector_additional.go
index fdda477..ea771e5 100644
--- a/NeuronMCP/internal/tools/vector_additional.go
+++ b/NeuronMCP/internal/tools/vector_additional.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * vector_additional.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/vector_additional.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// VectorSimilarityTool computes similarity between two vectors
+/* VectorSimilarityTool computes similarity between two vectors */
 type VectorSimilarityTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewVectorSimilarityTool creates a new vector similarity tool
+/* NewVectorSimilarityTool creates a new vector similarity tool */
 func NewVectorSimilarityTool(db *database.Database, logger *logging.Logger) *VectorSimilarityTool {
 	return &VectorSimilarityTool{
 		BaseTool: NewBaseTool(
@@ -49,7 +62,7 @@ func NewVectorSimilarityTool(db *database.Database, logger *logging.Logger) *Vec
 	}
 }
 
-// Execute executes the vector similarity computation
+/* Execute executes the vector similarity computation */
 func (t *VectorSimilarityTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -115,7 +128,7 @@ func (t *VectorSimilarityTool) Execute(ctx context.Context, params map[string]in
 		query = `SELECT $1::vector <#> $2::vector AS similarity`
 	case "l1":
 		query = `SELECT vector_l1_distance($1::vector, $2::vector) AS similarity`
-	default: // l2
+ 	default: /* l2 */
 		query = `SELECT $1::vector <-> $2::vector AS similarity`
 	}
 
@@ -136,14 +149,14 @@ func (t *VectorSimilarityTool) Execute(ctx context.Context, params map[string]in
 	}), nil
 }
 
-// CreateVectorIndexTool creates a vector index (generic, uses HNSW by default)
+/* CreateVectorIndexTool creates a vector index (generic, uses HNSW by default) */
 type CreateVectorIndexTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewCreateVectorIndexTool creates a new create vector index tool
+/* NewCreateVectorIndexTool creates a new create vector index tool */
 func NewCreateVectorIndexTool(db *database.Database, logger *logging.Logger) *CreateVectorIndexTool {
 	return &CreateVectorIndexTool{
 		BaseTool: NewBaseTool(
@@ -194,7 +207,7 @@ func NewCreateVectorIndexTool(db *database.Database, logger *logging.Logger) *Cr
 	}
 }
 
-// Execute executes the vector index creation
+/* Execute executes the vector index creation */
 func (t *CreateVectorIndexTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -265,7 +278,7 @@ func (t *CreateVectorIndexTool) Execute(ctx context.Context, params map[string]i
 			table, vectorColumn, indexName, numLists,
 		})
 	} else {
-		// Default to HNSW
+   /* Default to HNSW */
 		m := 16
 		if mVal, ok := params["m"].(float64); ok {
 			m = int(mVal)
diff --git a/NeuronMCP/internal/tools/vector_advanced.go b/NeuronMCP/internal/tools/vector_advanced.go
index efc1eec..8bde438 100644
--- a/NeuronMCP/internal/tools/vector_advanced.go
+++ b/NeuronMCP/internal/tools/vector_advanced.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * vector_advanced.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/vector_advanced.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// VectorArithmeticTool performs vector arithmetic operations
+/* VectorArithmeticTool performs vector arithmetic operations */
 type VectorArithmeticTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewVectorArithmeticTool creates a new vector arithmetic tool
+/* NewVectorArithmeticTool creates a new vector arithmetic tool */
 func NewVectorArithmeticTool(db *database.Database, logger *logging.Logger) *VectorArithmeticTool {
 	return &VectorArithmeticTool{
 		BaseTool: NewBaseTool(
@@ -52,7 +65,7 @@ func NewVectorArithmeticTool(db *database.Database, logger *logging.Logger) *Vec
 	}
 }
 
-// Execute executes the vector arithmetic operation
+/* Execute executes the vector arithmetic operation */
 func (t *VectorArithmeticTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -140,14 +153,14 @@ func (t *VectorArithmeticTool) Execute(ctx context.Context, params map[string]in
 	}), nil
 }
 
-// VectorDistanceTool computes distance between two vectors using various metrics
+/* VectorDistanceTool computes distance between two vectors using various metrics */
 type VectorDistanceTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewVectorDistanceTool creates a new vector distance tool
+/* NewVectorDistanceTool creates a new vector distance tool */
 func NewVectorDistanceTool(db *database.Database, logger *logging.Logger) *VectorDistanceTool {
 	return &VectorDistanceTool{
 		BaseTool: NewBaseTool(
@@ -191,7 +204,7 @@ func NewVectorDistanceTool(db *database.Database, logger *logging.Logger) *Vecto
 	}
 }
 
-// Execute executes the vector distance computation
+/* Execute executes the vector distance computation */
 func (t *VectorDistanceTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -261,7 +274,7 @@ func (t *VectorDistanceTool) Execute(ctx context.Context, params map[string]inte
 		query = "SELECT vector_mahalanobis_distance($1::vector, $2::vector, $3::vector) AS distance"
 		queryParams = []interface{}{vec1Str, vec2Str, covStr}
 	default:
-		// Use unified distance function
+   /* Use unified distance function */
 		pValue := 3.0
 		if p, ok := params["p_value"].(float64); ok {
 			pValue = p
@@ -287,14 +300,14 @@ func (t *VectorDistanceTool) Execute(ctx context.Context, params map[string]inte
 	}), nil
 }
 
-// VectorSimilarityUnifiedTool computes similarity using unified function
+/* VectorSimilarityUnifiedTool computes similarity using unified function */
 type VectorSimilarityUnifiedTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewVectorSimilarityUnifiedTool creates a new unified similarity tool
+/* NewVectorSimilarityUnifiedTool creates a new unified similarity tool */
 func NewVectorSimilarityUnifiedTool(db *database.Database, logger *logging.Logger) *VectorSimilarityUnifiedTool {
 	return &VectorSimilarityUnifiedTool{
 		BaseTool: NewBaseTool(
@@ -328,7 +341,7 @@ func NewVectorSimilarityUnifiedTool(db *database.Database, logger *logging.Logge
 	}
 }
 
-// Execute executes the similarity computation
+/* Execute executes the similarity computation */
 func (t *VectorSimilarityUnifiedTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
diff --git a/NeuronMCP/internal/tools/vector_graph.go b/NeuronMCP/internal/tools/vector_graph.go
index 22baa4f..e19f6d8 100644
--- a/NeuronMCP/internal/tools/vector_graph.go
+++ b/NeuronMCP/internal/tools/vector_graph.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * vector_graph.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/vector_graph.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -8,14 +21,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// VectorGraphTool performs graph operations on vgraph type
+/* VectorGraphTool performs graph operations on vgraph type */
 type VectorGraphTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewVectorGraphTool creates a new vector graph tool
+/* NewVectorGraphTool creates a new vector graph tool */
 func NewVectorGraphTool(db *database.Database, logger *logging.Logger) *VectorGraphTool {
 	return &VectorGraphTool{
 		BaseTool: NewBaseTool(
@@ -65,7 +78,7 @@ func NewVectorGraphTool(db *database.Database, logger *logging.Logger) *VectorGr
 	}
 }
 
-// Execute executes the graph operation
+/* Execute executes the graph operation */
 func (t *VectorGraphTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -153,3 +166,4 @@ func (t *VectorGraphTool) Execute(ctx context.Context, params map[string]interfa
 
 
 
+
diff --git a/NeuronMCP/internal/tools/workers.go b/NeuronMCP/internal/tools/workers.go
index e529723..82bcfd3 100644
--- a/NeuronMCP/internal/tools/workers.go
+++ b/NeuronMCP/internal/tools/workers.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * workers.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/internal/tools/workers.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package tools
 
 import (
@@ -9,14 +22,14 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/logging"
 )
 
-// WorkerManagementTool manages background workers
+/* WorkerManagementTool manages background workers */
 type WorkerManagementTool struct {
 	*BaseTool
 	executor *QueryExecutor
 	logger   *logging.Logger
 }
 
-// NewWorkerManagementTool creates a new worker management tool
+/* NewWorkerManagementTool creates a new worker management tool */
 func NewWorkerManagementTool(db *database.Database, logger *logging.Logger) *WorkerManagementTool {
 	return &WorkerManagementTool{
 		BaseTool: NewBaseTool(
@@ -51,7 +64,7 @@ func NewWorkerManagementTool(db *database.Database, logger *logging.Logger) *Wor
 	}
 }
 
-// Execute executes worker management operation
+/* Execute executes worker management operation */
 func (t *WorkerManagementTool) Execute(ctx context.Context, params map[string]interface{}) (*ToolResult, error) {
 	valid, errors := t.ValidateParams(params, t.InputSchema())
 	if !valid {
@@ -79,7 +92,7 @@ func (t *WorkerManagementTool) Execute(ctx context.Context, params map[string]in
 		if jobType == "" {
 			return Error("job_type is required for queue_job", "VALIDATION_ERROR", nil), nil
 		}
-		// Format job params as JSON
+   /* Format job params as JSON */
 		paramsJSON := "{}"
 		if len(jobParams) > 0 {
 			paramsBytes, err := json.Marshal(jobParams)
diff --git a/NeuronMCP/pkg/mcp/protocol.go b/NeuronMCP/pkg/mcp/protocol.go
index 7a3176c..c14f65b 100644
--- a/NeuronMCP/pkg/mcp/protocol.go
+++ b/NeuronMCP/pkg/mcp/protocol.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * protocol.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/pkg/mcp/protocol.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package mcp
 
 import (
@@ -8,7 +21,7 @@ import (
 
 const ProtocolVersion = "2025-06-18"
 
-// ParseRequest parses a JSON-RPC request
+/* ParseRequest parses a JSON-RPC request */
 func ParseRequest(data []byte) (*JSONRPCRequest, error) {
 	var req JSONRPCRequest
 	if err := json.Unmarshal(data, &req); err != nil {
@@ -22,7 +35,7 @@ func ParseRequest(data []byte) (*JSONRPCRequest, error) {
 	return &req, nil
 }
 
-// CreateResponse creates a JSON-RPC response
+/* CreateResponse creates a JSON-RPC response */
 func CreateResponse(id json.RawMessage, result interface{}) *JSONRPCResponse {
 	return &JSONRPCResponse{
 		JSONRPC: "2.0",
@@ -31,7 +44,7 @@ func CreateResponse(id json.RawMessage, result interface{}) *JSONRPCResponse {
 	}
 }
 
-// CreateErrorResponse creates a JSON-RPC error response
+/* CreateErrorResponse creates a JSON-RPC error response */
 func CreateErrorResponse(id json.RawMessage, code int, message string, data interface{}) *JSONRPCResponse {
 	return &JSONRPCResponse{
 		JSONRPC: "2.0",
@@ -44,7 +57,7 @@ func CreateErrorResponse(id json.RawMessage, code int, message string, data inte
 	}
 }
 
-// Standard JSON-RPC error codes
+/* Standard JSON-RPC error codes */
 const (
 	ErrCodeParseError     = -32700
 	ErrCodeInvalidRequest = -32600
@@ -53,19 +66,19 @@ const (
 	ErrCodeInternalError  = -32603
 )
 
-// MCP-specific error codes
+/* MCP-specific error codes */
 const (
 	ErrCodeToolNotFound    = -32001
 	ErrCodeResourceNotFound = -32002
 	ErrCodeExecutionError  = -32003
 )
 
-// SerializeResponse serializes a JSON-RPC response to JSON
+/* SerializeResponse serializes a JSON-RPC response to JSON */
 func SerializeResponse(resp *JSONRPCResponse) ([]byte, error) {
 	return json.Marshal(resp)
 }
 
-// ValidateRequest validates a JSON-RPC request
+/* ValidateRequest validates a JSON-RPC request */
 func ValidateRequest(req *JSONRPCRequest) error {
 	if req.JSONRPC != "2.0" {
 		return fmt.Errorf("invalid JSON-RPC version")
@@ -73,12 +86,12 @@ func ValidateRequest(req *JSONRPCRequest) error {
 	if req.Method == "" {
 		return fmt.Errorf("method is required")
 	}
-	// Note: ID is optional for notifications, but required for requests
-	// We'll handle this in the server
+  /* Note: ID is optional for notifications, but required for requests */
+  /* We'll handle this in the server */
 	return nil
 }
 
-// IsNotification checks if a request is a notification (no ID)
+/* IsNotification checks if a request is a notification (no ID) */
 func IsNotification(req *JSONRPCRequest) bool {
 	return len(req.ID) == 0 || bytes.Equal(req.ID, []byte("null"))
 }
diff --git a/NeuronMCP/pkg/mcp/protocol_test.go b/NeuronMCP/pkg/mcp/protocol_test.go
index 5a1582a..27818c6 100644
--- a/NeuronMCP/pkg/mcp/protocol_test.go
+++ b/NeuronMCP/pkg/mcp/protocol_test.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * protocol_test.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/pkg/mcp/protocol_test.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package mcp
 
 import (
@@ -24,7 +37,7 @@ func TestParseRequest(t *testing.T) {
 		{
 			name:    "missing method",
 			data:    []byte(`{"jsonrpc":"2.0","id":1}`),
-			wantErr: false, // Method validation happens in ValidateRequest
+   			wantErr: false, /* Method validation happens in ValidateRequest */
 		},
 		{
 			name:    "notification (no id)",
@@ -39,7 +52,7 @@ func TestParseRequest(t *testing.T) {
 		{
 			name:    "empty JSON",
 			data:    []byte(`{}`),
-			wantErr: true, // Missing jsonrpc version
+   			wantErr: true, /* Missing jsonrpc version */
 		},
 		{
 			name:    "nil data",
@@ -143,7 +156,7 @@ func TestValidateRequest(t *testing.T) {
 
 	for _, tt := range tests {
 		t.Run(tt.name, func(t *testing.T) {
-			// Should not panic
+    /* Should not panic */
 			func() {
 				defer func() {
 					if r := recover(); r != nil {
@@ -194,7 +207,7 @@ func TestIsNotification(t *testing.T) {
 		{
 			name: "nil request",
 			req:  nil,
-			want: true, // nil is treated as notification
+   			want: true, /* nil is treated as notification */
 		},
 		{
 			name: "empty id",
@@ -209,7 +222,7 @@ func TestIsNotification(t *testing.T) {
 
 	for _, tt := range tests {
 		t.Run(tt.name, func(t *testing.T) {
-			// Should not panic
+    /* Should not panic */
 			func() {
 				defer func() {
 					if r := recover(); r != nil {
@@ -243,7 +256,7 @@ func TestCreateResponse(t *testing.T) {
 		t.Error("CreateResponse() should not have error")
 	}
 
-	// Test with nil result - should not crash
+  /* Test with nil result - should not crash */
 	resp = CreateResponse(id, nil)
 	if resp == nil {
 		t.Fatal("CreateResponse() returned nil")
@@ -252,7 +265,7 @@ func TestCreateResponse(t *testing.T) {
 		t.Error("CreateResponse() should allow nil result")
 	}
 
-	// Test with empty id
+  /* Test with empty id */
 	emptyID := json.RawMessage("")
 	resp = CreateResponse(emptyID, result)
 	if resp == nil {
@@ -286,7 +299,7 @@ func TestCreateErrorResponse(t *testing.T) {
 		t.Errorf("CreateErrorResponse() error message = %v, want %v", resp.Error.Message, message)
 	}
 
-	// Test with empty message - should not crash
+  /* Test with empty message - should not crash */
 	resp = CreateErrorResponse(id, code, "", nil)
 	if resp == nil {
 		t.Fatal("CreateErrorResponse() returned nil")
@@ -295,7 +308,7 @@ func TestCreateErrorResponse(t *testing.T) {
 		t.Fatal("CreateErrorResponse() should have error even with empty message")
 	}
 
-	// Test with data
+  /* Test with data */
 	data := map[string]interface{}{"field": "value"}
 	resp = CreateErrorResponse(id, code, message, data)
 	if resp == nil {
@@ -320,13 +333,13 @@ func TestSerializeResponse(t *testing.T) {
 		t.Fatal("SerializeResponse() returned empty data")
 	}
 
-	// Should be valid JSON
+  /* Should be valid JSON */
 	var parsed map[string]interface{}
 	if err := json.Unmarshal(data, &parsed); err != nil {
 		t.Fatalf("SerializeResponse() produced invalid JSON: %v", err)
 	}
 
-	// Test with nil response - should return error
+  /* Test with nil response - should return error */
 	_, err = SerializeResponse(nil)
 	if err == nil {
 		t.Error("SerializeResponse() should return error for nil response")
diff --git a/NeuronMCP/pkg/mcp/server.go b/NeuronMCP/pkg/mcp/server.go
index ac51c0a..c74ae78 100644
--- a/NeuronMCP/pkg/mcp/server.go
+++ b/NeuronMCP/pkg/mcp/server.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * server.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/pkg/mcp/server.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package mcp
 
 import (
@@ -8,10 +21,10 @@ import (
 	"strings"
 )
 
-// HandlerFunc is a function that handles an MCP request
+/* HandlerFunc is a function that handles an MCP request */
 type HandlerFunc func(ctx context.Context, params json.RawMessage) (interface{}, error)
 
-// Server is an MCP protocol server
+/* Server is an MCP protocol server */
 type Server struct {
 	transport *StdioTransport
 	handlers  map[string]HandlerFunc
@@ -19,7 +32,7 @@ type Server struct {
 	caps      ServerCapabilities
 }
 
-// NewServer creates a new MCP server
+/* NewServer creates a new MCP server */
 func NewServer(name, version string) *Server {
 	return &Server{
 		transport: NewStdioTransport(),
@@ -35,17 +48,17 @@ func NewServer(name, version string) *Server {
 	}
 }
 
-// SetHandler registers a handler for a method
+/* SetHandler registers a handler for a method */
 func (s *Server) SetHandler(method string, handler HandlerFunc) {
 	s.handlers[method] = handler
 }
 
-// SetCapabilities sets server capabilities
+/* SetCapabilities sets server capabilities */
 func (s *Server) SetCapabilities(caps ServerCapabilities) {
 	s.caps = caps
 }
 
-// HandleInitialize handles the initialize request
+/* HandleInitialize handles the initialize request */
 func (s *Server) HandleInitialize(ctx context.Context, params json.RawMessage) (interface{}, error) {
 	var req InitializeRequest
 	if err := json.Unmarshal(params, &req); err != nil {
@@ -59,9 +72,9 @@ func (s *Server) HandleInitialize(ctx context.Context, params json.RawMessage) (
 	}, nil
 }
 
-// Run starts the server and processes requests
+/* Run starts the server and processes requests */
 func (s *Server) Run(ctx context.Context) error {
-	// Register initialize handler
+  /* Register initialize handler */
 	s.SetHandler("initialize", s.HandleInitialize)
 	
 	s.transport.WriteError(fmt.Errorf("DEBUG: Server Run() started, entering main loop"))
@@ -72,40 +85,40 @@ func (s *Server) Run(ctx context.Context) error {
 		s.transport.WriteError(fmt.Errorf("DEBUG: Loop iteration started"))
 		select {
 		case <-ctx.Done():
-			// Context cancelled - exit gracefully
+    /* Context cancelled - exit gracefully */
 			return ctx.Err()
 		default:
-			// Read next message - this will block until a message arrives or EOF
+    /* Read next message - this will block until a message arrives or EOF */
 			s.transport.WriteError(fmt.Errorf("DEBUG: About to call ReadMessage()"))
 			req, err := s.transport.ReadMessage()
 			s.transport.WriteError(fmt.Errorf("DEBUG: ReadMessage() returned, err=%v", err))
 			if err != nil {
-				// Check for EOF - this means stdin closed (client disconnected)
+     /* Check for EOF - this means stdin closed (client disconnected) */
 				if err == io.EOF {
-					// Client disconnected - exit gracefully
+      /* Client disconnected - exit gracefully */
 					return nil
 				}
-				// Check if error message contains EOF
+     /* Check if error message contains EOF */
 				errStr := err.Error()
 				if errStr == "EOF" || strings.Contains(errStr, "EOF") {
-					// Client disconnected - exit gracefully
+      /* Client disconnected - exit gracefully */
 					return nil
 				}
 				
-				// For any other error, log it but CONTINUE running
-				// The server MUST stay alive and wait for the next message
-				// Errors like "missing Content-Length header" can happen if there's
-				// partial input or the client is still connected but hasn't sent a complete message yet
-				// DO NOT exit on these errors - only exit on EOF
+     /* For any other error, log it but CONTINUE running */
+     /* The server MUST stay alive and wait for the next message */
+     /* Errors like "missing Content-Length header" can happen if there's */
+     /* partial input or the client is still connected but hasn't sent a complete message yet */
+     /* DO NOT exit on these errors - only exit on EOF */
 				s.transport.WriteError(fmt.Errorf("ReadMessage error (server continuing, will retry): %w", err))
 				
-				// CRITICAL: Continue the loop - server MUST stay alive
-				// Only exit on EOF (client disconnect) or context cancellation
-				// This ensures the server doesn't exit prematurely
+     /* CRITICAL: Continue the loop - server MUST stay alive */
+     /* Only exit on EOF (client disconnect) or context cancellation */
+     /* This ensures the server doesn't exit prematurely */
 				continue
 			}
 
-			// Handle initialize specially - send initialized notification
+    /* Handle initialize specially - send initialized notification */
 			if req.Method == "initialize" && !initializedSent {
 				s.transport.WriteError(fmt.Errorf("DEBUG: Received initialize request"))
 				
@@ -113,9 +126,9 @@ func (s *Server) Run(ctx context.Context) error {
 				
 				s.transport.WriteError(fmt.Errorf("DEBUG: Generated initialize response, hasError=%v", resp.Error != nil))
 				
-				// CRITICAL: ALWAYS send response for initialize request immediately
+     /* CRITICAL: ALWAYS send response for initialize request immediately */
 				if !IsNotification(req) {
-					// Send the initialize response FIRST - must happen synchronously
+      /* Send the initialize response FIRST - must happen synchronously */
 					s.transport.WriteError(fmt.Errorf("DEBUG: About to write initialize response"))
 					if err := s.transport.WriteMessage(resp); err != nil {
 						s.transport.WriteError(fmt.Errorf("CRITICAL: failed to write initialize response: %w", err))
@@ -123,9 +136,9 @@ func (s *Server) Run(ctx context.Context) error {
 						s.transport.WriteError(fmt.Errorf("DEBUG: Initialize response written successfully"))
 					}
 					
-					// If response was successful, send initialized notification
+      /* If response was successful, send initialized notification */
 					if resp.Error == nil {
-						// Send initialized notification AFTER response
+       /* Send initialized notification AFTER response */
 						s.transport.WriteError(fmt.Errorf("DEBUG: About to write initialized notification"))
 						if err := s.transport.WriteNotification("notifications/initialized", nil); err != nil {
 							s.transport.WriteError(fmt.Errorf("failed to write initialized notification: %w", err))
@@ -134,17 +147,17 @@ func (s *Server) Run(ctx context.Context) error {
 						}
 						initializedSent = true
 					} else {
-						// Even if there was an error, mark as initialized to prevent retry loops
+       /* Even if there was an error, mark as initialized to prevent retry loops */
 						initializedSent = true
 					}
 				}
 				s.transport.WriteError(fmt.Errorf("DEBUG: Finished processing initialize, continuing loop"))
-				// Continue loop to wait for next message - server stays alive
+     /* Continue loop to wait for next message - server stays alive */
 			} else {
-				// Handle other requests
+     /* Handle other requests */
 				resp := s.handleRequest(ctx, req)
 				
-				// Only send response if it's a request (has ID), not a notification
+     /* Only send response if it's a request (has ID), not a notification */
 				if !IsNotification(req) {
 					if err := s.transport.WriteMessage(resp); err != nil {
 						s.transport.WriteError(err)
@@ -157,19 +170,19 @@ func (s *Server) Run(ctx context.Context) error {
 }
 
 func (s *Server) handleRequest(ctx context.Context, req *JSONRPCRequest) *JSONRPCResponse {
-	// Validate request
+  /* Validate request */
 	if err := ValidateRequest(req); err != nil {
 		return CreateErrorResponse(req.ID, ErrCodeInvalidRequest, err.Error(), nil)
 	}
 
-	// Find handler
+  /* Find handler */
 	handler, exists := s.handlers[req.Method]
 	if !exists {
 		return CreateErrorResponse(req.ID, ErrCodeMethodNotFound,
 			fmt.Sprintf("method not found: %s", req.Method), nil)
 	}
 
-	// Execute handler
+  /* Execute handler */
 	result, err := handler(ctx, req.Params)
 	if err != nil {
 		return CreateErrorResponse(req.ID, ErrCodeInternalError, err.Error(), nil)
diff --git a/NeuronMCP/pkg/mcp/transport.go b/NeuronMCP/pkg/mcp/transport.go
index 809c3bc..cc86997 100644
--- a/NeuronMCP/pkg/mcp/transport.go
+++ b/NeuronMCP/pkg/mcp/transport.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * transport.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/pkg/mcp/transport.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package mcp
 
 import (
@@ -9,16 +22,16 @@ import (
 	"strings"
 )
 
-// StdioTransport handles MCP communication over stdio
+/* StdioTransport handles MCP communication over stdio */
 type StdioTransport struct {
 	stdin  *bufio.Reader
 	stdout *bufio.Writer
 	stderr io.Writer
 }
 
-// NewStdioTransport creates a new stdio transport
+/* NewStdioTransport creates a new stdio transport */
 func NewStdioTransport() *StdioTransport {
-	// Use a buffered writer for stdout to enable flushing
+  /* Use a buffered writer for stdout to enable flushing */
 	stdoutWriter := bufio.NewWriter(os.Stdout)
 	return &StdioTransport{
 		stdin:  bufio.NewReader(os.Stdin),
@@ -27,13 +40,13 @@ func NewStdioTransport() *StdioTransport {
 	}
 }
 
-// ReadMessage reads a JSON-RPC message from stdin
+/* ReadMessage reads a JSON-RPC message from stdin */
 func (t *StdioTransport) ReadMessage() (*JSONRPCRequest, error) {
 	t.WriteError(fmt.Errorf("DEBUG: ReadMessage() called, starting to read headers"))
-	// Read headers
+  /* Read headers */
 	var contentLength int
 	headerLines := 0
-	maxHeaders := 10 // Prevent infinite loop
+ 	maxHeaders := 10 /* Prevent infinite loop */
 	
 	for headerLines < maxHeaders {
 		t.WriteError(fmt.Errorf("DEBUG: Reading header line %d", headerLines))
@@ -41,56 +54,56 @@ func (t *StdioTransport) ReadMessage() (*JSONRPCRequest, error) {
 		t.WriteError(fmt.Errorf("DEBUG: Read header line: %q, err=%v", line, err))
 		if err != nil {
 			if err == io.EOF {
-				// If we got EOF while reading headers and haven't found Content-Length,
-				// this means the connection closed
+     /* If we got EOF while reading headers and haven't found Content-Length, */
+     /* this means the connection closed */
 				if contentLength == 0 {
 					return nil, io.EOF
 				}
-				// If we have Content-Length but got EOF, it's still EOF
+     /* If we have Content-Length but got EOF, it's still EOF */
 				return nil, io.EOF
 			}
 			return nil, fmt.Errorf("failed to read header: %w", err)
 		}
 		headerLines++
 
-		// Remove trailing newline/carriage return
+   /* Remove trailing newline/carriage return */
 		line = strings.TrimRight(line, "\r\n")
 		
-		// Check if the first line is JSON (starts with '{')
-		// If so, Claude Desktop is sending JSON directly without Content-Length headers
+   /* Check if the first line is JSON (starts with '{') */
+   /* If so, Claude Desktop is sending JSON directly without Content-Length headers */
 		if headerLines == 1 && strings.HasPrefix(strings.TrimSpace(line), "{") {
 			t.WriteError(fmt.Errorf("DEBUG: First line is JSON (no Content-Length headers), parsing directly"))
-			// Parse the JSON directly
+    /* Parse the JSON directly */
 			return ParseRequest([]byte(line))
 		}
 		
-		// Empty line indicates end of headers
+   /* Empty line indicates end of headers */
 		if line == "" {
 			break
 		}
 
-		// Parse Content-Length
+   /* Parse Content-Length */
 		lineLower := strings.ToLower(line)
 		if strings.HasPrefix(lineLower, "content-length:") {
-			// Try both capitalized and lowercase
+    /* Try both capitalized and lowercase */
 			if _, err := fmt.Sscanf(line, "Content-Length: %d", &contentLength); err != nil {
 				if _, err := fmt.Sscanf(line, "content-length: %d", &contentLength); err != nil {
 					return nil, fmt.Errorf("invalid Content-Length header: %s", line)
 				}
 			}
 		}
-		// Skip other headers (Content-Type, etc.)
+   /* Skip other headers (Content-Type, etc.) */
 	}
 
 	if contentLength <= 0 {
-		// This can happen if we read an empty line before getting Content-Length
-		// or if there's malformed input. Return error but don't treat as fatal.
+   /* This can happen if we read an empty line before getting Content-Length */
+   /* or if there's malformed input. Return error but don't treat as fatal. */
 		t.WriteError(fmt.Errorf("DEBUG: No valid Content-Length found after %d headers", headerLines))
 		return nil, fmt.Errorf("missing or invalid Content-Length header")
 	}
 
 	t.WriteError(fmt.Errorf("DEBUG: Headers parsed, contentLength=%d, reading body", contentLength))
-	// Read message body
+  /* Read message body */
 	body := make([]byte, contentLength)
 	if _, err := io.ReadFull(t.stdin, body); err != nil {
 		if err == io.EOF {
@@ -102,7 +115,7 @@ func (t *StdioTransport) ReadMessage() (*JSONRPCRequest, error) {
 	return ParseRequest(body)
 }
 
-// WriteMessage writes a JSON-RPC message to stdout
+/* WriteMessage writes a JSON-RPC message to stdout */
 func (t *StdioTransport) WriteMessage(resp *JSONRPCResponse) error {
 	data, err := SerializeResponse(resp)
 	if err != nil {
@@ -111,18 +124,18 @@ func (t *StdioTransport) WriteMessage(resp *JSONRPCResponse) error {
 
 	t.WriteError(fmt.Errorf("DEBUG: Writing response: %s", string(data)))
 
-	// Claude Desktop expects JSON directly without Content-Length headers
-	// Write JSON followed by newline
+  /* Claude Desktop expects JSON directly without Content-Length headers */
+  /* Write JSON followed by newline */
 	if _, err := t.stdout.Write(data); err != nil {
 		return fmt.Errorf("failed to write body: %w", err)
 	}
 	
-	// Add newline after JSON
+  /* Add newline after JSON */
 	if _, err := t.stdout.Write([]byte("\n")); err != nil {
 		return fmt.Errorf("failed to write newline: %w", err)
 	}
 
-	// Flush stdout to ensure message is sent immediately
+  /* Flush stdout to ensure message is sent immediately */
 	if err := t.stdout.Flush(); err != nil {
 		return fmt.Errorf("failed to flush stdout: %w", err)
 	}
@@ -132,7 +145,7 @@ func (t *StdioTransport) WriteMessage(resp *JSONRPCResponse) error {
 	return nil
 }
 
-// WriteNotification writes a JSON-RPC notification (no response expected)
+/* WriteNotification writes a JSON-RPC notification (no response expected) */
 func (t *StdioTransport) WriteNotification(method string, params interface{}) error {
 	notification := map[string]interface{}{
 		"jsonrpc": "2.0",
@@ -150,18 +163,18 @@ func (t *StdioTransport) WriteNotification(method string, params interface{}) er
 
 	t.WriteError(fmt.Errorf("DEBUG: Writing notification: %s", string(data)))
 
-	// Claude Desktop expects JSON directly without Content-Length headers
-	// Write JSON followed by newline
+  /* Claude Desktop expects JSON directly without Content-Length headers */
+  /* Write JSON followed by newline */
 	if _, err := t.stdout.Write(data); err != nil {
 		return fmt.Errorf("failed to write body: %w", err)
 	}
 	
-	// Add newline after JSON
+  /* Add newline after JSON */
 	if _, err := t.stdout.Write([]byte("\n")); err != nil {
 		return fmt.Errorf("failed to write newline: %w", err)
 	}
 
-	// Flush stdout to ensure message is sent immediately
+  /* Flush stdout to ensure message is sent immediately */
 	if err := t.stdout.Flush(); err != nil {
 		return fmt.Errorf("failed to flush stdout: %w", err)
 	}
@@ -171,10 +184,10 @@ func (t *StdioTransport) WriteNotification(method string, params interface{}) er
 	return nil
 }
 
-// WriteError writes an error to stderr (only in debug mode)
+/* WriteError writes an error to stderr (only in debug mode) */
 func (t *StdioTransport) WriteError(err error) {
-	// Only write debug errors if DEBUG environment variable is set
-	// This prevents stderr pollution in production
+  /* Only write debug errors if DEBUG environment variable is set */
+  /* This prevents stderr pollution in production */
 	if os.Getenv("NEURONDB_DEBUG") == "true" {
 		fmt.Fprintf(t.stderr, "DEBUG: %v\n", err)
 	}
diff --git a/NeuronMCP/pkg/mcp/transport_test.go b/NeuronMCP/pkg/mcp/transport_test.go
index 20c304b..9afe493 100644
--- a/NeuronMCP/pkg/mcp/transport_test.go
+++ b/NeuronMCP/pkg/mcp/transport_test.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * transport_test.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/pkg/mcp/transport_test.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package mcp
 
 import (
@@ -11,7 +24,7 @@ import (
 )
 
 func TestStdioTransport_ReadMessage(t *testing.T) {
-	// Create a test message
+  /* Create a test message */
 	message := map[string]interface{}{
 		"jsonrpc": "2.0",
 		"id":      1,
@@ -24,7 +37,7 @@ func TestStdioTransport_ReadMessage(t *testing.T) {
 	}
 	messageStr := string(messageJSON)
 
-	// Create input with Content-Length header
+  /* Create input with Content-Length header */
 	input := fmt.Sprintf("Content-Length: %d\r\nContent-Type: application/json\r\n\r\n%s", len(messageJSON), messageStr)
 
 	transport := &StdioTransport{
@@ -48,7 +61,7 @@ func TestStdioTransport_ReadMessage(t *testing.T) {
 }
 
 func TestStdioTransport_ReadMessage_InvalidContentLength(t *testing.T) {
-	// Test with invalid Content-Length header
+  /* Test with invalid Content-Length header */
 	input := "Content-Length: invalid\r\n\r\n{}"
 
 	transport := &StdioTransport{
@@ -64,7 +77,7 @@ func TestStdioTransport_ReadMessage_InvalidContentLength(t *testing.T) {
 }
 
 func TestStdioTransport_ReadMessage_MissingContentLength(t *testing.T) {
-	// Test with missing Content-Length header
+  /* Test with missing Content-Length header */
 	input := "Content-Type: application/json\r\n\r\n{}"
 
 	transport := &StdioTransport{
@@ -80,7 +93,7 @@ func TestStdioTransport_ReadMessage_MissingContentLength(t *testing.T) {
 }
 
 func TestStdioTransport_ReadMessage_InvalidJSON(t *testing.T) {
-	// Test with invalid JSON body
+  /* Test with invalid JSON body */
 	input := "Content-Length: 10\r\n\r\n{invalid}"
 
 	transport := &StdioTransport{
@@ -96,7 +109,7 @@ func TestStdioTransport_ReadMessage_InvalidJSON(t *testing.T) {
 }
 
 func TestStdioTransport_ReadMessage_ShortBody(t *testing.T) {
-	// Test with Content-Length larger than actual body
+  /* Test with Content-Length larger than actual body */
 	input := "Content-Length: 100\r\n\r\n{}"
 
 	transport := &StdioTransport{
@@ -125,7 +138,7 @@ func TestStdioTransport_ReadMessage_EOF(t *testing.T) {
 }
 
 func TestStdioTransport_ReadMessage_JSONDirect(t *testing.T) {
-	// Test reading JSON directly (without Content-Length headers)
+  /* Test reading JSON directly (without Content-Length headers) */
 	message := map[string]interface{}{
 		"jsonrpc": "2.0",
 		"id":      1,
@@ -174,7 +187,7 @@ func TestStdioTransport_WriteMessage(t *testing.T) {
 		t.Fatalf("WriteMessage() error = %v", err)
 	}
 
-	// Flush the buffer to get the output
+  /* Flush the buffer to get the output */
 	if err := transport.stdout.Flush(); err != nil {
 		t.Fatalf("Failed to flush stdout: %v", err)
 	}
@@ -184,7 +197,7 @@ func TestStdioTransport_WriteMessage(t *testing.T) {
 		t.Fatal("WriteMessage() produced no output")
 	}
 
-	// Should contain JSON
+  /* Should contain JSON */
 	if !strings.Contains(output, "jsonrpc") {
 		t.Error("WriteMessage() should include jsonrpc in output")
 	}
@@ -198,7 +211,7 @@ func TestStdioTransport_WriteMessage_NilResponse(t *testing.T) {
 		stderr: &bytes.Buffer{},
 	}
 
-	// Should not crash with nil response
+  /* Should not crash with nil response */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
@@ -225,7 +238,7 @@ func TestStdioTransport_WriteNotification(t *testing.T) {
 		t.Fatalf("WriteNotification() error = %v", err)
 	}
 
-	// Flush the buffer to get the output
+  /* Flush the buffer to get the output */
 	if err := transport.stdout.Flush(); err != nil {
 		t.Fatalf("Failed to flush stdout: %v", err)
 	}
@@ -235,7 +248,7 @@ func TestStdioTransport_WriteNotification(t *testing.T) {
 		t.Fatal("WriteNotification() produced no output")
 	}
 
-	// Should contain method
+  /* Should contain method */
 	if !strings.Contains(output, "method") {
 		t.Error("WriteNotification() should include method in JSON")
 	}
@@ -249,12 +262,12 @@ func TestStdioTransport_WriteNotification_EmptyMethod(t *testing.T) {
 		stderr: &bytes.Buffer{},
 	}
 
-	// Should not crash with empty method
+  /* Should not crash with empty method */
 	err := transport.WriteNotification("", nil)
 	if err != nil {
 		t.Logf("WriteNotification() with empty method returned error: %v", err)
 	} else {
-		// Flush if no error
+   /* Flush if no error */
 		_ = transport.stdout.Flush()
 	}
 }
@@ -267,13 +280,13 @@ func TestStdioTransport_WriteNotification_NilParams(t *testing.T) {
 		stderr: &bytes.Buffer{},
 	}
 
-	// Should not crash with nil params
+  /* Should not crash with nil params */
 	err := transport.WriteNotification("test/notification", nil)
 	if err != nil {
 		t.Fatalf("WriteNotification() error with nil params = %v", err)
 	}
 
-	// Flush the buffer to get the output
+  /* Flush the buffer to get the output */
 	if err := transport.stdout.Flush(); err != nil {
 		t.Fatalf("Failed to flush stdout: %v", err)
 	}
diff --git a/NeuronMCP/pkg/mcp/types.go b/NeuronMCP/pkg/mcp/types.go
index 964b43f..25998d3 100644
--- a/NeuronMCP/pkg/mcp/types.go
+++ b/NeuronMCP/pkg/mcp/types.go
@@ -1,8 +1,21 @@
+/*-------------------------------------------------------------------------
+ *
+ * types.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/pkg/mcp/types.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package mcp
 
 import "encoding/json"
 
-// JSON-RPC 2.0 message types
+/* JSON-RPC 2.0 message types */
 type JSONRPCRequest struct {
 	JSONRPC string          `json:"jsonrpc"`
 	ID      json.RawMessage `json:"id,omitempty"`
@@ -23,7 +36,7 @@ type JSONRPCError struct {
 	Data    interface{} `json:"data,omitempty"`
 }
 
-// MCP Request types
+/* MCP Request types */
 type ListToolsRequest struct {
 	Method string `json:"method"`
 }
@@ -41,7 +54,7 @@ type ReadResourceRequest struct {
 	URI string `json:"uri"`
 }
 
-// MCP Response types
+/* MCP Response types */
 type ToolDefinition struct {
 	Name        string                 `json:"name"`
 	Description string                 `json:"description"`
@@ -84,7 +97,7 @@ type ResourceContent struct {
 	Text     string `json:"text"`
 }
 
-// Server info
+/* Server info */
 type ServerInfo struct {
 	Name    string `json:"name"`
 	Version string `json:"version"`
diff --git a/NeuronMCP/test/integration_test.go b/NeuronMCP/test/integration_test.go
index f3a685d..bf87885 100644
--- a/NeuronMCP/test/integration_test.go
+++ b/NeuronMCP/test/integration_test.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * integration_test.go
+ *    Database operations
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/test/integration_test.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package test
 
 import (
@@ -10,12 +23,12 @@ import (
 	"github.com/neurondb/NeuronMCP/pkg/mcp"
 )
 
-// IntegrationTestSuite provides integration tests for NeuronMCP
-// Note: These tests require a running NeuronDB database
-// Set NEURONDB_HOST, NEURONDB_PORT, etc. environment variables to run
+/* IntegrationTestSuite provides integration tests for NeuronMCP */
+/* Note: These tests require a running NeuronDB database */
+/* Set NEURONDB_HOST, NEURONDB_PORT, etc. environment variables to run */
 
 func TestServerInitialization(t *testing.T) {
-	// Skip if no database configured
+  /* Skip if no database configured */
 	if testing.Short() {
 		t.Skip("Skipping integration test in short mode")
 	}
@@ -28,7 +41,7 @@ func TestServerInitialization(t *testing.T) {
 		t.Fatal("NewServer() returned nil server")
 	}
 
-	// Test that server can be stopped without crashing
+  /* Test that server can be stopped without crashing */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
@@ -38,7 +51,7 @@ func TestServerInitialization(t *testing.T) {
 		srv.Stop()
 	}()
 
-	// Test that stopping twice doesn't crash
+  /* Test that stopping twice doesn't crash */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
@@ -54,9 +67,9 @@ func TestMCPProtocolFlow(t *testing.T) {
 		t.Skip("Skipping integration test in short mode")
 	}
 
-	// This would test the full MCP protocol flow
-	// including initialize/initialized handshake
-	// In a real scenario, you'd use a mock transport
+  /* This would test the full MCP protocol flow */
+  /* including initialize/initialized handshake */
+  /* In a real scenario, you'd use a mock transport */
 
 	t.Skip("Full MCP protocol flow test requires mock transport")
 }
@@ -77,24 +90,24 @@ func TestToolExecution(t *testing.T) {
 		srv.Stop()
 	}()
 
-	// Test list tools through MCP protocol
+  /* Test list tools through MCP protocol */
 	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
 	defer cancel()
 
-	// Test that server can handle list tools request
-	// Note: We can't directly access handleListTools from test package,
-	// but we can test through the MCP server interface if available
-	// For now, we test that server initialization doesn't crash
-	// and that Stop() works correctly
+  /* Test that server can handle list tools request */
+  /* Note: We can't directly access handleListTools from test package, */
+  /* but we can test through the MCP server interface if available */
+  /* For now, we test that server initialization doesn't crash */
+  /* and that Stop() works correctly */
 
-	// Test that server can be stopped without crashing
+  /* Test that server can be stopped without crashing */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
 				t.Fatalf("Server operations panicked: %v", r)
 			}
 		}()
-		// Server should be initialized
+   /* Server should be initialized */
 		if srv == nil {
 			t.Fatal("NewServer() returned nil")
 		}
@@ -120,19 +133,19 @@ func TestResourceAccess(t *testing.T) {
 	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
 	defer cancel()
 
-	// Test that server can handle resource requests without crashing
-	// Note: We can't directly access resources from test package,
-	// but we can test that server initialization doesn't crash
-	// and that Stop() works correctly
+  /* Test that server can handle resource requests without crashing */
+  /* Note: We can't directly access resources from test package, */
+  /* but we can test that server initialization doesn't crash */
+  /* and that Stop() works correctly */
 
-	// Test that server operations don't panic
+  /* Test that server operations don't panic */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
 				t.Fatalf("Server operations panicked: %v", r)
 			}
 		}()
-		// Server should be initialized
+   /* Server should be initialized */
 		if srv == nil {
 			t.Fatal("NewServer() returned nil")
 		}
@@ -159,30 +172,30 @@ func TestServerErrorHandling(t *testing.T) {
 	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
 	defer cancel()
 
-	// Test that server handles context cancellation gracefully
+  /* Test that server handles context cancellation gracefully */
 	cancelledCtx, cancelFunc := context.WithCancel(context.Background())
 	cancelFunc()
 
-	// Test that server can be stopped multiple times without crashing
+  /* Test that server can be stopped multiple times without crashing */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
 				t.Fatalf("Server operations panicked: %v", r)
 			}
 		}()
-		// Server should handle operations gracefully
+   /* Server should handle operations gracefully */
 		_ = cancelledCtx
 		_ = ctx
 	}()
 
-	// Test that server can handle being stopped and then operations attempted
+  /* Test that server can handle being stopped and then operations attempted */
 	func() {
 		defer func() {
 			if r := recover(); r != nil {
 				t.Fatalf("Server operations panicked after stop: %v", r)
 			}
 		}()
-		// Server should be initialized
+   /* Server should be initialized */
 		if srv == nil {
 			t.Fatal("NewServer() returned nil")
 		}
diff --git a/NeuronMCP/test/test_all_tools.go b/NeuronMCP/test/test_all_tools.go
index 8a93397..29f2683 100644
--- a/NeuronMCP/test/test_all_tools.go
+++ b/NeuronMCP/test/test_all_tools.go
@@ -1,3 +1,16 @@
+/*-------------------------------------------------------------------------
+ *
+ * test_all_tools.go
+ *    Tool implementation for NeuronMCP
+ *
+ * Copyright (c) 2024-2025, neurondb, Inc. <admin@neurondb.com>
+ *
+ * IDENTIFICATION
+ *    NeuronMCP/test/test_all_tools.go
+ *
+ *-------------------------------------------------------------------------
+ */
+
 package test
 
 import (
@@ -10,7 +23,7 @@ import (
 	"github.com/neurondb/NeuronMCP/internal/tools"
 )
 
-// TestAllToolsRegistered tests that all tools are properly registered
+/* TestAllToolsRegistered tests that all tools are properly registered */
 func TestAllToolsRegistered(t *testing.T) {
 	db := database.NewDatabase()
 	output := "stderr"
@@ -23,38 +36,38 @@ func TestAllToolsRegistered(t *testing.T) {
 	registry := tools.NewToolRegistry(db, logger)
 	tools.RegisterAllTools(registry, db, logger)
 
-	// Expected tool categories and counts
+  /* Expected tool categories and counts */
 	expectedTools := []string{
-		// Vector operations
+   /* Vector operations */
 		"vector_search", "vector_search_l2", "vector_search_cosine", "vector_search_inner_product",
 		"vector_similarity", "vector_arithmetic", "vector_distance", "vector_similarity_unified",
-		// Quantization
+   /* Quantization */
 		"vector_quantize", "quantization_analyze",
-		// Embeddings
+   /* Embeddings */
 		"generate_embedding", "batch_embedding", "embed_image", "embed_multimodal", "embed_cached",
 		"configure_embedding_model", "get_embedding_model_config", "list_embedding_model_configs", "delete_embedding_model_config",
-		// Hybrid search
+   /* Hybrid search */
 		"hybrid_search", "reciprocal_rank_fusion", "semantic_keyword_search", "multi_vector_search",
 		"faceted_vector_search", "temporal_vector_search", "diverse_vector_search",
-		// Reranking
+   /* Reranking */
 		"rerank_cross_encoder", "rerank_llm", "rerank_cohere", "rerank_colbert", "rerank_ltr", "rerank_ensemble",
-		// ML
+   /* ML */
 		"train_model", "predict", "predict_batch", "evaluate_model", "list_models", "get_model_info", "delete_model", "export_model",
-		// Analytics
+   /* Analytics */
 		"analyze_data", "cluster_data", "reduce_dimensionality", "detect_outliers", "quality_metrics", "detect_drift", "topic_discovery",
-		// Time series
+   /* Time series */
 		"timeseries_analysis",
-		// AutoML
+   /* AutoML */
 		"automl",
-		// ONNX
+   /* ONNX */
 		"onnx_model",
-		// Indexing
+   /* Indexing */
 		"create_hnsw_index", "create_ivf_index", "index_status", "drop_index", "tune_hnsw_index", "tune_ivf_index",
-		// RAG
+   /* RAG */
 		"process_document", "retrieve_context", "generate_response", "chunk_document",
-		// Workers & GPU
+   /* Workers & GPU */
 		"worker_management", "gpu_info",
-		// PostgreSQL
+   /* PostgreSQL */
 		"postgresql_version", "postgresql_stats", "postgresql_databases", "postgresql_connections",
 		"postgresql_locks", "postgresql_replication", "postgresql_settings", "postgresql_extensions",
 	}
@@ -71,7 +84,7 @@ func TestAllToolsRegistered(t *testing.T) {
 	}
 }
 
-// TestToolValidation tests parameter validation
+/* TestToolValidation tests parameter validation */
 func TestToolValidation(t *testing.T) {
 	db := database.NewDatabase()
 	output := "stderr"
@@ -84,13 +97,13 @@ func TestToolValidation(t *testing.T) {
 	registry := tools.NewToolRegistry(db, logger)
 	tools.RegisterAllTools(registry, db, logger)
 
-	// Test vector_search validation
+  /* Test vector_search validation */
 	tool := registry.GetTool("vector_search")
 	if tool == nil {
 		t.Fatal("vector_search tool not found")
 	}
 
-	// Test with missing required parameter
+  /* Test with missing required parameter */
 	ctx := context.Background()
 	result, err := tool.Execute(ctx, map[string]interface{}{})
 	if err != nil {
@@ -104,12 +117,13 @@ func TestToolValidation(t *testing.T) {
 	}
 }
 
-// TestPostgreSQLTools tests PostgreSQL tools (no DB connection required for version)
+/* TestPostgreSQLTools tests PostgreSQL tools (no DB connection required for version) */
 func TestPostgreSQLTools(t *testing.T) {
-	// Note: These tests require a database connection
-	// They are integration tests and should be run with a test database
+  /* Note: These tests require a database connection */
+  /* They are integration tests and should be run with a test database */
 	t.Skip("Skipping integration tests - requires database connection")
 }
 
 
 
+
